\lstset{language=transcript}

\section{Introduction: How Cylc Works} 
\label{HowCylcWorks}

\subsection{Scheduling Forecast Suites} 
\label{SchedulingForecastSuites}

Environmental forecasting suites generate forecast products from a
potentially large group of interdependent scientific models and
associated data processing tasks. They are constrained by availability
of external driving data: typically one or more tasks will wait on real
time observations and/or model data from an external system, and these
will drive other downstream tasks, and so on. The dependency diagram for
a single forecast cycle in such a system is a {\em Directed Acyclic
Graph} as shown in Figure~\ref{fig-dep-one} (in our terminology, a {\em
forecast cycle} is comprised of all tasks with a common {\em cycle
time}, which is the nominal analysis time or start time of the forecast
models in the group). In real time operation processing will consist of
a series of distinct forecast cycles that are each initiated, after a
gap, by arrival of the new cycle's external driving data.

From a job scheduling perspective task execution order in such a system
must be carefully controlled in order to avoid dependency violations.
Ideally, each task should be queued for execution at the instant its
last prerequisite is satisfied; this is the best that can be done even
if queued tasks are not able to execute immediately because of resource
contention.

\subsection{EcoConnect} 
\label{EcoConnect}

Cylc was developed for the EcoConnect Forecasting System at NIWA
(National Institute of Water and Atmospheric Research, New Zealand).
EcoConnect takes real time atmospheric and stream flow observations, and
operational global weather forecasts from the Met Office (UK), and uses
these to drive global sea state and regional data assimilating weather
models, which in turn drive regional sea state, storm surge, and
catchment river models, plus tide prediction, and a large number of
associated data collection, quality control, preprocessing,
post-processing, product generation, and archiving tasks.\footnote{Future
plans for EcoConnect include additional deterministic regional weather
forecasts and a statistical ensemble.} The global sea state forecast
runs once daily. The regional weather forecast runs four times daily but
it supplies surface winds and pressure to several downstream models that
run only twice daily, and precipitation accumulations to catchment river
models that run on an hourly cycle assimilating real time stream flow
observations and using the most recently available regional weather
forecast.  EcoConnect runs on heterogeneous distributed hardware,
including a massively parallel supercomputer and several Linux servers. 


\subsection{Dependence Between Tasks}

\subsubsection{Intra-cycle Dependence} 
\label{IntracycleDependence}

Most inter-task dependence exist within a single forecast cycle.
Figure~\ref{fig-dep-one} shows the dependency diagram for a single
forecast cycle of a simple example suite of three forecast models ({\em
a, b,} and {\em c}) and three post processing or product generation
tasks ({\em d, e} and {\em f}). A scheduler capable of handling this
must manage, within a single forecast cycle, multiple parallel streams
of execution that branch when one task generates output for several
downstream tasks, and merge when one task takes input from several
upstream tasks. 

\begin{figure}
    \begin{center}
        \includegraphics[width=6cm]{graphics/png/orig/dep-one-cycle.png} 
    \end{center}
    \caption[A single cycle dependency graph for a simple suite]{\scriptsize
    The dependency graph for a single forecast cycle of a simple example
    suite. Tasks {\em a, b,} and {\em c} represent forecast models,
    {\em d, e} and {\em f} are post processing or product generation
    tasks, and {\em x} represents external data that the upstream
    forecast model depends on.}
    \label{fig-dep-one} 
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=8cm]{graphics/png/orig/timeline-one.png}
    \end{center}
    \caption[A single cycle job schedule for real time operation]{\scriptsize
    The optimal job schedule for two consecutive cycles of our example
    suite during real time operation, assuming that all tasks trigger 
    off upstream tasks finishing completely. The horizontal extent of
    a task bar represents its execution time, and the vertical blue
    lines show when the external driving data becomes available.}
    \label{fig-time-one}
\end{figure}

Figure~\ref{fig-time-one} shows the optimal job schedule for two
consecutive cycles of the example suite in real time operation, given
execution times represented by the horizontal extent of the task bars.
There is a time gap between cycles as the suite waits on new external
driving data.  Each task in the example suite happens to trigger off
upstream tasks {\em finishing}, rather than off any intermediate output
or event; this is merely a simplification that makes for clearer
diagrams.

\begin{figure}
    \begin{center}
        \includegraphics[width=10cm]{graphics/png/orig/dep-two-cycles-linked.png} 
    \end{center}
    \caption[What if the external driving data is available early?]{\scriptsize If
    the external driving data is available in advance, can we start
    running the next cycle early?} 
    \label{fig-dep-two-linked}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=6cm]{graphics/png/orig/timeline-one-c.png} 
    \end{center}
    \caption[Attempted overlap of consecutive single-cycle job
    schedules]{\scriptsize A naive attempt to overlap two consecutive cycles
    using the single-cycle dependency graph. The red shaded tasks will
    fail because of dependency violations (or will not be able to run
    because of upstream dependency violations).} 
    \label{fig-overlap}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=8cm]{graphics/png/orig/timeline-one-a.png} 
    \end{center}
    \caption[The only safe multi-cycle job schedule?]{\scriptsize The best
    that can be done {\em in general} when inter-cycle dependence is
    ignored.} 
    \label{fig-job-no-overlap}
\end{figure} 

Now the question arises, what happens if the external driving data for
upcoming cycles is available in advance, as it would be after a
significant delay in operations, or when running a historical case
study?  While the forecast model {\em a} appears to depend only on the
external data {\em x} at this stage of the discussion, in fact it would 
typically also depend on its own previous instance for the model {\em
background state} used in initializing the new forecast. Thus, as
alluded to in Figure~\ref{fig-dep-two-linked}, task {\em a} could in
principle start
as soon as its predecessor has finished.  Figure~\ref{fig-overlap}
shows, however, that starting a whole new cycle at this point is
dangerous - it results in dependency violations in half of the tasks in
the example suite. In fact the situation is even worse than this
- imagine that task {\em b} in the first cycle is delayed for any reason
{\em after} the second cycle has been launched? Clearly we must consider
handling inter-cycle dependence explicitly or else agree not to start
the next cycle early, as is illustrated in Figure~\ref{fig-job-no-overlap}.

\subsubsection{Inter-cycle Dependence} 
\label{IntercycleDependence}

Forecast models typically depend on their own most recent previous
forecast for background state or restart files of some kind, and
different types of tasks in different forecast cycles can also be linked
(in an atmospheric forecast analysis suite, for instance, the weather
model may also generate background states for use by the observation
processing and data-assimilation systems in the next cycle). In real
time operation this inter-cycle dependence can be ignored because it is
automatically satisfied when each cycle finishes before the next one
begins. If, on the other hand, it is explicitly accounted for, it 
complicates the dependency graph by destroying the clean boundary
between forecast cycles. Figure~\ref{fig-dep-multi} illustrates the
problem for our simple example suite assuming the minimal inter-cycle
dependence: the forecast models ($a$, $b$, and $c$) each depend on their
own previous instances.

For this reason, and perhaps because we tend to see forecasting suites
as inherently sequential (with respect to whole forecast cycles)
other metaschedulers ignore inter-cycle dependence and therefore {\em
require} a series of distinct cycles at all times. While this does not
affect normal real time operation it can be a serious impediment when
advance availability of external driving data makes it possible, in
principle, to run some tasks from upcoming cycles before the current
cycle is finished - as suggested at the end of the previous section.
This occurs
after delays (late arrival of external data, system maintenance, etc.)
and, to an even greater extent, in historical case studies, and parallel
test suites that are delayed with respect to the main operation. It is
a serious problem, in particular, for suites that have little downtime
between forecast cycles and therefore take many cycles to catch up
after a delay. Without taking account of inter-cycle dependence, the
best that can be done, in general, is to reduce the gap between cycles
to zero as shown in Figure~\ref{fig-job-no-overlap}. A limited crude
overlap of the single cycle job schedule may be possible for specific
task sets but the allowable overlap may change if new tasks are added,
and it is still dangerous: it amounts to running different parts of a
dependent system as if they were not dependent and as such it cannot be
guaranteed that some unforeseen delay in one cycle, after the 
next cycle has begun, (e.g.\ due to resource contention or task
failures) won't result in dependency violations.

\begin{figure}
    \begin{center}
        \includegraphics[width=8cm]{graphics/png/orig/dep-multi-cycle.png} 
    \end{center}
    \caption[The complete multi-cycle dependency graph]{\scriptsize The complete
    dependency graph for the example suite, assuming the least possible
    inter-cycle dependence: the forecast models ($a$, $b$, and $c$)
    depend on their own previous instances. The dashed arrows show
    connections to previous and subsequent forecast cycles.} 
    \label{fig-dep-multi}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=6cm]{graphics/png/orig/timeline-two-cycles-optimal.png} 
    \end{center}
    \caption[The optimal two-cycle job schedule]{\scriptsize The optimal two
    cycle job schedule when the next cycle's driving data is available in
    advance, possible in principle when inter-cycle dependence is
    handled explicitly.} 
    \label{fig-optimal-two}
\end{figure} 

Figure~\ref{fig-optimal-two} shows, in contrast to
Figure~\ref{fig-overlap}, the optimal two cycle job schedule obtained by
respecting all inter-cycle dependence.  This assumes no delays due to
resource contention or otherwise - i.e.\ every task runs
as soon as it is ready to run. The scheduler running
this suite must be able to adapt dynamically to external conditions 
that impact on multi-cycle scheduling in the presence of
inter-cycle dependence or else, again, risk bringing the system down
with dependency violations.

\begin{figure}
    \begin{center}
        \includegraphics[width=12cm]{graphics/png/orig/timeline-three.png} 
    \end{center}
    \caption[Comparison of job schedules after a delay]{\scriptsize Job
    schedules for the example suite after a delay of almost one whole
    forecast cycle, when inter-cycle dependence is
    taken into account (above the time axis), and when it is not
    (below the time axis). The colored lines indicate the time that
    each cycle is delayed, and normal ``caught up'' cycles
    are shaded gray.} 
    \label{fig-time-three}
\end{figure} 

\begin{figure}
    \begin{center} 
        \includegraphics[width=8cm]{graphics/png/orig/timeline-two.png}
    \end{center} 
    \caption[Optimal job schedule when all external data is
    available]{\scriptsize Job schedules for the example suite in case study
    mode, or after a long delay, when the external driving data are
    available many cycles in advance. Above the time axis is the optimal
    schedule obtained when the suite is constrained only by its true
    dependencies, as in Figure \ref{fig-dep-two-linked}, and underneath
    is the best that can be done, in general, when inter-cycle
    dependence is ignored.} 
    \label{fig-time-two}
\end{figure} 

To further illustrate the potential benefits of proper inter-cycle
dependency handling, Figure~\ref{fig-time-three} shows an operational
delay of almost one whole cycle in a suite with little downtime between
cycles. Above the time axis is the optimal schedule that is possible in
principle when inter-cycle dependence is taken into account, and below it
is the only safe schedule possible {\em in general} when it is ignored.
In the former case, even the cycle immediately after the delay
is hardly affected, and subsequent cycles are all on time, whilst in the
latter case it takes five full cycles to catch up to normal real time
operation.
%Note that simply overlapping the single cycle schedules of
%Figure~\ref{fig-time-one} from the same start point would have resulted
%in dependency violation by task {\em c}.

Similarly, Figure~\ref{fig-time-two} shows example suite job schedules
for an historical case study, or when catching up after a very long
delay; i.e.\ when the external driving data are available many cycles in
advance.  Task {\em a}, which as the most upstream forecast model is
likely to be a resource intensive atmosphere or ocean model, has no
upstream dependence on co-temporal tasks and can therefore run
continuously, regardless of how much downstream processing is yet to be
completed in its own, or any previous, forecast cycle (actually, task
{\em a} does depend on co-temporal task {\em x} which waits on the
external driving data, but that returns immediately when the external
data is available in advance, so the result stands). The other forecast
models can also cycle continuously or with short gap between, and some
post processing tasks, which have no previous-instance dependence, can
run continuously or even overlap (e.g.\ {\em e} in this case). Thus,
even for this very simple example suite, tasks from three or four
different cycles can in principle run simultaneously at any given time. 
In fact, if our tasks are able to trigger off internal outputs of 
upstream tasks, rather than waiting on full completion, successive
instances of the forecast models could overlap as well (because model
restart outputs are generally completed early in the forecast) for an
even more efficient job schedule. 

%Finally, we note again that a good job scheduler should be able to
%dynamically adapt to delays in any part of the suite due to resource
%contention, varying run times, or anything else that will inevitably
%modify the depicted job schedules. 

\subsection{The Cylc Scheduling Algorithm} 
\label{TheCylcSchedulingAlgorithm}

\begin{figure}
    \begin{center} 
        \includegraphics[width=8cm]{graphics/png/orig/task-pool.png}
    \end{center} 
    \caption[The cylc task pool]{\scriptsize How cylc sees a suite, in
    contrast to the multi-cycle dependency graph of Figure~\ref{fig-dep-multi}.
    Task colors represent different cycle times, and the small squares
    and circles represent different prerequisites and outputs. A task
    can run when its prerequisites are satisfied by the outputs 
    of other tasks in the pool.} 
    \label{fig-task-pool}
\end{figure} 

Cylc manages a pool of proxy objects that represent real tasks in the
forecasting suite. A task proxy can run the real task that it
represents when its prerequisites are satisfied, and can receive reports
of completed outputs from the real task as it runs. There is no global
cycling mechanism to advance the suite in time; instead each individual
task proxy has a private cycle time and spawns its own successor. Task
proxies are self-contained - they just know their own prerequisites and
outputs and are not aware of the wider suite context. Inter-cycle
dependencies are not treated as special, and the task pool can be
populated with tasks from many different cycle times. 
The cylc task pool is illustrated in Figure~\ref{fig-task-pool}. Now,
{\em whenever any task changes state
due to completion of an output, every task checks to see if its
own prerequisites are now satisfied.}\footnote{In fact this dependency
negotiation goes through a broker object (rather than every task
literally checking every other task) which scales as $n$ (rather than
$n^2$) where $n$ is the number of task proxies in the pool.}  Moreover,
this matching of prerequisites and outputs involves the entire task
pool, regardless of individual cycle times, so that inter- and
intra-cycle dependence is handled with ease.

Thus without using global cycling mechanisms, and treating all
inter-task dependence equally, cylc in effect gets a pool of tasks to
self-organize by negotiating their own dependencies so that optimal
scheduling, as described in the previous section, emerges naturally at
run time.

%\pagebreak
\section{Cylc Screenshots}


\begin{figure}
    \begin{center}
        \includegraphics[height=0.35\textheight]{graphics/png/orig/suiterc-jinja2.png}]
    \end{center}
    \caption[A cylc suite definition in the {\em vim} editor]{\scriptsize
    A cylc suite definition in the {\em vim} editor.}
    \label{fig-cylc-vim} 
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{graphics/png/orig/control1.png}
    \end{center}
\caption[gcylc dot and text views]{\scriptsize gcylc dot and text views.}
\label{fig-control1}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{graphics/png/orig/control2.png}
    \end{center}
\caption[gcylc graph and text views]{\scriptsize gcylc graph and text views.}
\label{fig-control2}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/png/orig/ecox-1.png}
    \end{center}
\caption[A large suite graphed by cylc]{\scriptsize A large suite graphed by cylc.}
\label{fig-ecox-1}
\end{figure} 

% dump floats
\clearpage

%\pagebreak

\section{Required Software} 
\label{Requirements}

\begin{myitemize}
    \item {\bf Cylc}, the version associated with this document is: \input{cylc-version.txt}. % generated each time by doc/process
        \newline \url{http://cylc.github.com/cylc}
    \item {\bf OS: Linux, Unix, including OS X.}
    \item {\bf The Python Language, version 2.5 or later, but 
        not 3.x yet}.
        \newline \url{http://python.org}
    \item {\bf Pyro (Python Remote Objects), version 3.10$+$, latest
        tested 3.16}; not Pyro 4.x as yet. Pyro is used by cylc
        for network communication between server processes (cylc suites)
        and client programs (running tasks, gcylc, commands).
        \newline \url{http://irmen.home.xs4all.nl/pyro3}
    \item {\bf sqlite (a server-less, zero-configuration, SQL database
        engine}). This is likely included in your Linux distribution
        already. Cylc generates an sqlite database for each suite as it
        runs, to record task events and status.
        \newline \url{http://sqlite.org}
\end{myitemize}

The following packages are technically optional as you can construct
and control cylc suites without dependency graphing, the cylc GUIs, and
template processing {\em but this is not recommended, and without 
Jinja2 you will not be able to run many of the example suites}:

\begin{myitemize}
    \item {\bf PyGTK}, a Python wrapper for the GTK+ GUI toolkit,
        required for the cylc GUI (but you can prepare and control
        cylc suites entirely from the command line if you like).
        PyGTK is included in most Linux Distributions.  
        \newline \url{http://www.pygtk.org}
    \item {\bf Graphviz} (latest tested 2.28.0) and {\bf Pygraphviz}
        (latest tested 1.1), a graph layout engine and a Python
        interface to it. These are required for suite dependency
        graphing and the gcylc graph view (but you can also run
        cylc without them).
        \newline \url{http://www.graphviz.org}
        \newline \url{http://networkx.lanl.gov/pygraphviz}
    \item {\bf Jinja2}, a template processor for Python (latest tested:
        2.6). Jinja2 adds variables, conditional and mathematical
        expressions, loop control structures, etc., to cylc suite 
        definition files. 
        \newline \url{http://jinja.pocoo.org/docs}
\end{myitemize}

If installing via a Linux package manager, you may also need a couple of
{\em devel} packages for the pygraphviz build:

\begin{myitemize}
    \item python-devel
    \item graphviz-devel
\end{myitemize}

Tagged versions of cylc-5.0 or later can be downloaded and installed
from GitHub. The user guide must be generated after download from the
\LaTeX source documents (by running \lstinline=make=). The following
\TeX packages are required:

\begin{myitemize}
    \item texlive
    \item texlive-tocloft
    \item texlive-framed
    \item texlive-preprint (for fullpage.sty, formerly in tetex)
\end{myitemize}
And for HTML versions of the User Guide:

\begin{myitemize}
    \item texlive-tex4ht
    \item ImageMagick (for image scaling)
\end{myitemize}

Finally, cylc makes heavy use of ``ordered dictionary'' data structures,
and a significant speedup in parsing large suites can be had by
installing the fast C-coded \lstinline=ordereddict= module by Anthon
van der Neut:
\begin{myitemize}
    \item {\bf ordereddict} (latest tested 0.4.5) \newline 
        \url{http://www.xs4all.nl/~anthon/Python/ordereddict}
\end{myitemize}
This module is currently included with cylc under
\lstinline=$CYLC_DIR/ext=, and is built by the top level cylc Makefile.
If you install the resulting library appropriately cylc will
automatically use it in place of a slower Python implementation of 
the ordered dictionary structure.

\subsection{Known Version Compatibility Issues}

Cylc should run ``out of the box'' on recent Linux distributions.

For distributed suites the Pyro versions installed on all suite or task
hosts must be mutually compatible. Using identical Pyro versions
guarantees compatibility but may not be strictly necessary because 
cylc uses Pyro rather minimally.

Recent versions of Pyro require Python 2.5 or greater, due to use of the 
\lstinline=with= statement introduced in 2.5.

\subsubsection{Pyro 3.9 and Earlier}

Beware of Linux distributions that come packaged with old Pyro versions.
Pyro 3.9 and earlier is not compatible with the new-style Python classes
used in cylc. It has been reported that Ubuntu 10.04 (Lucid Lynx),
released in September 2009, suffers from this problem. Surprisingly, 
so does Ubuntu 11.10 (Oneiric Ocelot), released in October 2011 - and
therefore, presumably, earlier Ubuntu releases.  Attempting to run
a suite with Pyro 3.9 or earlier installed results in the following
Python traceback:

\lstset{language=Python}
\begin{lstlisting}
Traceback (most recent call last):
 File "/home/oliverh/cylc/bin/_run", line 232, in <module>
 server = start()
 File "/home/oliverh/cylc/bin/_run", line 92, in __init__
 scheduler.__init__( self )
 File "/home/oliverh/cylc/lib/cylc/scheduler.py", line 141, in
__init__
 self.load_tasks()
 File "/home/oliverh/cylc/bin/_run", line 141, in load_tasks_cold
 itask = self.config.get_task_proxy( name, tag, 'waiting',
stopctime=None, startup=True )
 File "/home/oliverh/cylc/lib/cylc/config.py", line 1252, in
get_task_proxy
 return self.taskdefs[name].get_task_class()( ctime, state,
stopctime, startup )
 File "/home/oliverh/cylc/lib/cylc/taskdef.py", line 453, in
tclass_init
 print '-', sself.__class__.__name__, sself.__class__.__bases_
AttributeError: type object 'A' has no attribute '_taskdef__bases_'
_run --debug testsuite.1322742021 2010010106 failed: 1
\end{lstlisting}

\subsubsection{Apple Mac OSX}

It has been reported that cylc runs fine on OSX 10.6 SnowLeopard, but on 
OSX 10.7 Lion there is an issue with constructing proper FQDNs (Fully
Qualified Domain Names) that requires a change to the DNS service.
Here's how to solve the problem:

\begin{myitemize}
    \item Edit \lstinline=/System/Library/LaunchDaemons/com.apple.mDNSResponder.plist= \newline
        by adding \lstinline=<string>-AlwaysAppendSearchDomains</string>= after line 16:
\begin{lstlisting}
<key>ProgramArguments</key>
  <array>
    <string>/usr/sbin/mDNSResponder</string>
    <string>-launchd</string>
        <string>-AlwaysAppendSearchDomains</string>
  </array>
\end{lstlisting}
    \item Now unload and reload the mDNSResponder service:
\begin{lstlisting}
% sudo launchctl unload -w
/System/Library/LaunchDaemons/com.apple.mDNSResponder.plist
% sudo launchctl load -w
/System/Library/LaunchDaemons/com.apple.mDNSResponder.plist
\end{lstlisting}
\end{myitemize}


\subsection{Other Software Used Internally By Cylc}

Cylc has absorbed the following in modified form (no need to install these separately): 
\begin{myitemize}
    \item xdot, a graph viewer
        (\url{http://code.google.com/p/jrfonseca/wiki/XDot},
        LGPL license)
    \item ConfigObj and Validate 
        (\url{http://www.voidspace.org.uk/python}, %configobj.html,
        BSD license)
\end{myitemize}


\section{Installation} 
\label{Installation}

\subsection{Install The External Dependencies}

First install Pyro, graphviz, Pygraphviz, Jinja2, \TeX, and ImageMagick
using the package manager on your system if possible; otherwise download
the packages manually and follow their native installation documentation.
On a modern Linux system, this is very easy. For example, to install
cylc-5.1.0 on the Fedora 18 Linux distribution:

\begin{lstlisting}
% yum install graphviz       # (2.28)
% yum install graphviz-devel # (for pgraphviz build)
% yum install python-devel   # (ditto) 

# TeX packages, and ImageMagick, for generating the Cylc User Guide:
% yum install texlive
% yum install texlive-tex4ht
% yum install texlive-tocloft
% yum install texlive-framed
% yum install texlive-preprint
% yum install ImageMagick

# Python packages:
% easy_install pyro   # (3.16)
% easy_install Jinja2 # (2.6)
% easy_install pygraphviz
	
# (sqlite 3.7.13 already installed on the system)
\end{lstlisting}

If you do not have root access on your intended cylc host machine and
cannot get a sysadmin to do this at system level, see
Section~\ref{LocalInstall} for some tips on installing everything to 
a local user account. 

Now check that everything other than the \LaTeX packages is
installed properly:
\begin{lstlisting}
$ cylc check-sof
Checking for Python >= 2.5 ... found 2.7.3 ... ok
Checking for non-Python packages:
 + Graphviz ... ok
 + sqlite ... ok
Checking for Python packages:
 + Pyro-3 ... ok
 + Jinja2 ... ok
 + pygraphviz ... ok
 + pygtk ... ok
\end{lstlisting}
If this command reports any errors for the Python packages then either
the packages concerned are not installed or they are not in the system
python search path or your \lstinline=$PYTHONPATH= environment variable,


\subsection{Install The Cylc Release}

Cylc typically installs into a normal user account; just unpack
the release tarball in the desired location (referred to
below as  \lstinline=$CYLC_DIR=) and see the
\lstinline=$CYLC_DIR/INSTALL= file for instructions 
(see also Section~\ref{INSTALL}.

\subsection{Site And User Configuration Files}
\label{SiteAndUserConfiguration}

Cylc uses site and user configuration files to set some important
global parameters such as the range of network ports, and the editor
to use on suite definitions, 

\begin{lstlisting}
$CYLC_DIR/conf/site/site.rc  # site config file for global settings
$HOME/.cylc/user.rc          # user config file for global settings
\end{lstlisting}

These files can be auto-generated with all settings
intially commented out by running this command:
\begin{lstlisting}
% cylc get-global-config --write-site/--write-user
\end{lstlisting}

The content of these config files, in terms of legal items and
default values, is defined by the {\em ConfigObj configspec} file,
\begin{lstlisting}
$CYLC_DIR/conf/site/cfgspec 
\end{lstlisting}

The site config file should be adapted to set sensible defaults for all
users when cylc is installed.  Users can then override most settings in
their own user config file, if necessary. Some settings can not be 
overridden by users (this is determined by \lstinline=# SITE ONLY=
comments in the configspec file, which are passed through to the config
files). 

\subsection{Import The Example Suites}
\label{ImportTheExampleSuites}

Run the following command immediately after installation to copy the
cylc example suites to a given destination directory, and registering
each one for use:

\begin{lstlisting}
% cylc admin import-examples TOPDIR
\end{lstlisting}
Where \lstinline=TOPDIR= is the top level directory into which the
example suite definitions will be copied.
To view the content of the suite database run \lstinline=cylc db print= 
or open the gcylc suite chooser dialog.

\begin{lstlisting}
% cylc db print --tree -x 'Auto|Quick'
cylc-4-5-1-1344571037
  |-AutoCleanup      
  | |-FamilyFailHook  family failure hook script example
  | `-FamilyFailTask  family failure cleanup task example
  |-AutoRecover      
  | |-async           asynchronous automated failure recovery example
  | `-cycling         cycling automated failure recovery example
  `-QuickStart       
    |-a          Quick Start Example A
    |-b          Quick Start Example B
    |-c          Quick Start Example C
    `-z          Quick Start Example Z
\end{lstlisting}
Note that the dots in the cylc release version number are replaced with
hyphens because `.' is the suite name delimiter. Type
\lstinline=cylc db print --help= to see what the command options mean.
Note also that the Unix ``seconds since epoch'' string is appended to
the top level of the suite name hierarchy in order ensure uniqueness if 
the example suites are imported multiple times at the same cylc version:
\lstinline=cylc-4-5-1-1344571037=. You can re-register the example
suites to get rid of this and make the names easier to type:

\begin{lstlisting}
% cylc rereg cylc-4-5-1-1344571037 examples

% cylc db print --tree -x 'Auto|Quick'
examples         
 |-AutoRecover   
 | |-CleanupTask  No title provided
 | |-EventHook     family failure task event hook example
 | `-suicide       automated failure recovery example
 `-QuickStart    
   |-a             Quick Start Example A
   |-b             Quick Start Example B
   |-c             Quick Start Example C
   `-z             Quick Start Example Z
\end{lstlisting}

Here's the same suite database listing in flat form:
\begin{lstlisting}
% cylc db print -y 'Auto|Quick'
examples.QuickStart.c | /tmp/oliverh/examples/QuickStart/c
examples.QuickStart.z | /tmp/oliverh/examples/QuickStart/z
examples.QuickStart.a | /tmp/oliverh/examples/QuickStart/a
examples.QuickStart.b | /tmp/oliverh/examples/QuickStart/b
examples.AutoRecover.CleanupTask | /tmp/oliverh/examples/AutoRecover/CleanupTask
examples.AutoRecover.EventHook | /tmp/oliverh/examples/AutoRecover/EventHook
examples.AutoRecover.suicide | /tmp/oliverh/examples/AutoRecover/suicide
\end{lstlisting}

\subsection{Automated Database Test} 
\label{RTADT}

The command \lstinline=cylc admin test-db= gives suite name 
database functionality a work out - it copies one of the cylc example
suites, registers it under a new name and then manipulates it by
recopying the suite in various ways, and so on, before finally deleting
the test suites. This should complete without error in a few seconds.

\subsection{Automated Scheduler Tests} 
\label{RTAST}

Cylc has a battery of self-diagnosing test suites for pre-release
testing that you can also run after installation to check that
everything works properly.

The command \lstinline=cylc admin test-battery= copies, registers, and
runs, the official cylc test suites, and reports the results. Some tests
may take several minutes to complete. Pre-test requirements:

\begin{myitemize}
    \item Some test suites require passwordless self-ssh to be
        configured, i.e.\ the following command should run successfully: 
\begin{lstlisting}
ssh -oBatchmode=yes ssh localhost ls
\end{lstlisting}
    \item  Some tests require a remote task host with passwordless ssh
        configured, i.e. the following command should run successfully: 
\begin{lstlisting}
ssh -oBatchmode=yes ssh <HOSTNAME> ls
\end{lstlisting}
You must specify the hostname before running these tests:
\begin{lstlisting}
export CYLC_TEST_TASK_HOST=<HOSTNAME>
\end{lstlisting}
If you don't have access to a remote task host just use
\lstinline=localhost= as the remote hostname.
\end{myitemize}

None of the tests should fail on an official cylc release, unless
because of timeouts if the suite host is slow or overloaded. 

\subsection{Complete Non System-Level Installation}
\label{LocalInstall}

If you do not have root access to your host machine and cannot easily
get Pyro, Graphviz, Pygraphviz, and Jinja2 installed at system level,
here's how to install everything under a normal user account.

First, cylc is already designed to be installed into a user account -
just unpack the release tarball into \lstinline=$CYLC_DIR=.

Next, learn how to use \lstinline=easy_install= for local python package 
installs {\em or} download the source distributions from the URLs given
at the beginning of Section~\ref{Requirements} and follow the
package-specific advice below.

\subsubsection{Pyro}

Install Pyro under \lstinline=$HOME/external/installed= as follows:
\begin{lstlisting}
% cd $HOME/external
% tar xzf Pyro-3.14.tar.gz
% cd Pyro-3.14
% python setup.py install --prefix=$HOME/external/installed
\end{lstlisting}
Take note of the resulting Python \lstinline=site-packages= directory
under \lstinline=external/installed/=, e.g.:
\begin{lstlisting}
$HOME/external/installed/lib64/python2.6/site-packages/
\end{lstlisting}
The exact path will depend on your local Python environment. Add the
following to your login scripts:
\begin{lstlisting}
# .profile
PYTHONPATH=$HOME/external/installed/lib64/python2.6/site-packages:$PYTHONPATH
PATH=$HOME/external/installed/bin:$PATH
\end{lstlisting}

\subsubsection{Graphviz}

Install Graphviz under \lstinline=$CYLC_DIR/external/installed= as
follows:
\begin{lstlisting}
% cd $CYLC_DIR/external
% tar xzf graphviz-2.28.0.tar.gz
% cd graphviz-2.28.0
% ./configure --prefix=$CYLC_DIR/external/installed --with-qt=no
% make
% make install
\end{lstlisting}
This installs graphviz files into the bin, include, and lib
sub-directories of your local installation directory. The graphviz lib
and include locations are required when installing Pygraphviz (next).

The graphviz build reportedly may fail on systems that do not have QT
installed, hence the \lstinline@./configure --with-qt=no@ option above.

\subsubsection{Pygraphviz}
Install Pygraphviz under \lstinline=$CYLC_DIR/external/installed= as
follows:
\begin{lstlisting}
% cd $CYLC_DIR/external
% tar xzf pygraphviz-1.1.tar.gz
% cd pygraphviz-1.1
\end{lstlisting}
Now edit setup.py lines 31 and 32 to specify the graphviz lib and
include directories:
\begin{lstlisting}
library_path=os.environ['CYLC_DIR'] + '/external/installed/lib'
include_path=os.environ['CYLC_DIR'] + '/external/installed/include/graphviz'
\end{lstlisting}
Or use absolute paths instead of the \lstinline=$CYLC_DIR= environment
variable. Check that these are the correct library and include paths by
inspecting the contents of the specified directories, and adjust them if
necessary.  Finally, install pygraphviz:
\begin{lstlisting}
% export CYLC_DIR=/path/to/cylc
% python setup.py install --prefix=$CYLC_DIR/external/installed
\end{lstlisting}
This may or may not, depending on your local Python setup, install the
Pygraphviz modules into the same place as the Pyro modules, e.g.:
\begin{lstlisting}
% ls $CYLC_DIR/external/installed/lib64/python2.6/site-packages/
 pygraphviz  pygraphviz-1.1-py2.6.egg-info  Pyro  Pyro-3.14-py2.6.egg-info
\end{lstlisting}
If not, add the correct Pyraphviz installation path to your PYTHONPATH.

\subsubsection{Jinja2}

Download Jinja2 from the project web site and install it with: 
\begin{lstlisting}
python setup.py install --prefix=/path/to/install/location
\end{lstlisting}
or use the \lstinline=easy_install= command to do it all in one step.
Either way the final installed package location must be present in the
\lstinline=PYTHONPATH= variable, and you may have to arrange for this
first. You may also need to create the installed package directory 
if it doesn't exist already (the install will abort and print the
name of the missing directory in the error message).
Here's how to easy\_install Jinja2 into your private site packages
directory:

\begin{lstlisting}
% LOCALPREFIX=$CYLC_DIR/external/installed
% LOCALPACKAGES=$CYLC_DIR/external/installed/lib64/python2.6/site-packages
% export PYTHONPATH=$LOCALPACKAGES:$PYTHONPATH
% easy_install --prefix=$LOCALPREFIX Jinja2
\end{lstlisting}
Adapt the site-packages path according to your actual path, as above.

Finally, check that everything other than \LaTeX is installed properly:
\begin{lstlisting}
$ cylc check-sof
Checking for Python >= 2.5 ... found 2.7.3 ... ok
Checking for non-Python packages:
 + Graphviz ... ok
 + sqlite ... ok
Checking for Python packages:
 + Pyro-3 ... ok
 + Jinja2 ... ok
 + pygraphviz ... ok
 + pygtk ... ok
\end{lstlisting}
If this command reports an error for any Python packages then those
packages are not installed, or they are not in the system python
search path and not in your \lstinline=$PYTHONPATH= environment variable.

\subsection{What Next?}

You should now have access to all cylc functionality.  Import the
example suites if you have not done so already
(Section~\ref{ImportTheExampleSuites}) then test 
your cylc installation by running the automated suite database test
(Section~\ref{RTADT}) and the automated scheduler test
(Section~\ref{RTAST}), then go on to the {\em Quick Start Guide}
(Section~\ref{QuickStartGuide}).

\subsection{Upgrading To New Cylc Versions}

Upgrading is just a matter of unpacking the new cylc release and 
optionally re-importing the example suites for the new version. 


\section{On The Meaning Of {\em Cycle Time} In Cylc}

From using other schedulers you may be accustomed to the idea that a
forecasting suite has a ``current cycle time'', which is typically the
analysis time or nominal start time of the main forecast model(s) in the
suite, and that the whole suite advances to the next forecast cycle when
all tasks in the current cycle have finished (or even when a particular
wall clock time is reached, in real time operation). As is explained in 
the Introduction, this is not how cylc works.

Cylc suites advance by means of individual tasks with private cycle
times independently spawning successors at the next valid cycle time for
the task, not by incrementing a suite-wide forecast cycle. Each task
will be submitted when its own prerequisites are satisfied, regardless
of other tasks with other cycle times running, or not, at the time.
It may still be convenient at times, however, to refer to the ``current
cycle'', the ``previous cycle'', or the ``next cycle'' and so
forth, with reference to a particular task, or in the sense of
all tasks that ``belong to'' a particular forecast cycle.  But keep in
mind that the members of these groups may not be present simultaneously
in the running suite - i.e.\ different tasks may pass through the
``current cycle'' (etc.) at different times as the suite evolves,
particularly in delayed (catch up) operation.

%\pagebreak
\section{Quick Start Guide} 
\label{QuickStartGuide}

This section works through some basic cylc functionality using the
``QuickStart'' example suites, which you can import to your suite database 
by running the \lstinline=cylc admin import-examples= command and then
reregistering the top level suite name to ``examples'' as described in
Section~\ref{ImportTheExampleSuites}. You should end up with the
following QuickStart suites (but the directory paths on the right are up
to you):

\begin{lstlisting}
% cylc db print --tree QuickStart
 examples   
   `-QuickStart 
     |-a        Quick Start Example A | /tmp/oliverh/examples/QuickStart/a
     |-b        Quick Start Example B | /tmp/oliverh/examples/QuickStart/b
     |-c        Quick Start Example C | /tmp/oliverh/examples/QuickStart/c
     `-z        Quick Start Example Z | /tmp/oliverh/examples/QuickStart/z
\end{lstlisting}

\subsection{View The examples.QuickStart.a Suite Definition}

Cylc suites are defined by {\em suite.rc files}, discussed at length in
{\em Suite Definition} (Section~\ref{SuiteDefinition}) and the {\em
Suite.rc Reference} (Appendix~\ref{SuiteRCReference}).  To view the
examples.QuickStart.a suite definition right-click on the suite
name and choose `Edit'; or use the edit command:
\begin{lstlisting}
% cylc edit examples.QuickStart.a
\end{lstlisting}

This opens the suite definition in your editor (configured in the cylc
site or user config file - see Section~\ref{SiteAndUserConfiguration})
{\em from the suite definition directory so that you can easily open
other suite files in the editor}. You can of course do this manually,
but by using the cylc interface you don't have to remember suite
directory locations. If you do need to move to a suite definition
directory, you can do this:
\begin{lstlisting}
% cd $( cylc db get-dir examples.QuickStart.a )
\end{lstlisting}
Suites that use include-files can optionally be edited in a temporarily
inlined state - the inlined file gets split back into its constituent
include-files when you save it and exit the editor. While editing, the
inlined file becomes the official suite definition so that changes take
effect whenever you save the file.

Anyhow, you should now see the following suite.rc file in your editor: 
\lstset{language=suiterc}
\lstinputlisting{../examples/QuickStart/a/suite.rc}
\lstset{language=transcript}
Cylc comes with syntax highlighting and section folding for the {\em vim}
editor, and an emacs font-lock mode - see Section~\ref{SyntaxHighlighting}.

This defines a complete, valid, runnable suite.  Here's how to interpret
it: At 0, 6, 12, and 18 hours each day a clock-triggered task called GetData
triggers 1 hour after the wall clock reaches its (GetData's) nominal cycle
time; then a task called Model triggers when GetData finishes;
and a task called PostA triggers when Model is finished. Additionally,
Model depends on its own previous instance from 6 hours earlier; and twice
per day at 6 and 18 hours another task called PostB also triggers off Model.

All the tasks in this suite can run in parallel with their own previous
instances if the opportunity arises (i.e.\ if their prerequisites
are satisfied before the previous instance is finished). Most tasks
should be capable of this (see Section~\ref{LimitPID}) but if
necessary you can force particular tasks to run sequentially like 
this:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[special tasks]]
        sequential = GetData, PostB
\end{lstlisting}
\lstset{language=transcript}

Finally, when the suite is {\em cold-started} (started from scratch) it
is made to wait on a special {\em synchronous start-up task} called
Prep. Start-up tasks are one-off (non-spawning) tasks that are only used
at suite start-up, and any dependence on them only applies at suite
start-up. They cannot be used in conditional trigger expressions with 
normal cycling tasks, because the trigger becomes undefined in
subsequent cycles. Start-up tasks are {\em synchronous} because they
have a defined cycle time even though they are not cycling tasks. Cylc
also has {\em asynchronous one-off tasks}, which have no cycle time:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        graph = "prep"     # an asynchronous one-off task (no cycle time)
        [[[ 0,6,12,18 ]]]
            graph = "prep => foo => bar"   # followed by cycling tasks
\end{lstlisting}
\lstset{language=transcript}

The optional visualization section configures graph plotting.

\subsection[Plotting examples.QuickStart.a]{Plotting The examples.QuickStart.a Dependency Graph}

Open the {\em examples.QuickStart.a} suite gcylc and graph it using the ``Suite'' menu; 
or by command line,
\begin{lstlisting}
% cylc graph examples.QuickStart.a 2011052300 2011052318 &
\end{lstlisting}
This will pop up a zoomable, pannable, graph viewer showing
the graph of Figure~\ref{fig-QuickStartA-graph18}. 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{graphics/png/orig/QuickStartA-graph18.png}
    \end{center}
    \caption[The {\em examples.QuickStart.a} dependency
    graph]{\scriptsize The {\em QuickStart.a} dependency graph, plotted
    by cylc.}
\label{fig-QuickStartA-graph18}
\end{figure}

\subsection{Run The examples.QuickStart.a Suite}

Each cylc task defines command scripting to invoke the right external
processing when the task is ready to run. This has not been explicilty
configured in the example suite, so it defaults, for all tasks, to the
{\em dummy task} scripting inherited from the {\em root namespace} (see
Section~\ref{SuiteDefinition}):
\begin{lstlisting}
% cylc get-config -i [runtime][GetData]'command scripting' examples.QuickStart.a 
['echo Dummy command scripting; sleep 10']
\end{lstlisting}
The command arguments above reflect suite definition section nesting. 

Now open the suite in gcylc and select {\em Graph View} in the top
toolbar.  You can also open other gcylc instances for the same suite at
the same time if you like. Note that closing gcylc does not kill the suite 
that the GUI is currently connected to.

In gcylc click on Control $\rightarrow$ Run, enter an initial
cold-start cycle time (e.g.\ 2011052306), and select ``Hold (pause) on
start-up'' so that the suite will start in the held state (tasks
will not be submitted even if they are ready to run). 

{\em Do not choose an initial cycle time in the future unless you're
running in simulation mode, or nothing much will happen until that
time.}

If the initial cycle time ends in 06 or 18 
the suite controller should look like
Figure~\ref{fig-QuickStartA-ControlStart06},
or otherwise (00 or 12) like
Figure~\ref{fig-QuickStartA-ControlStart00}.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{graphics/png/orig/QuickStartA-ControlStart06.png}
    \end{center}
    \caption[Suite {\em examples.QuickStart.a} at start-up, 06 or 18 hours.]{\scriptsize
    Suite {\em examples.QuickStart:one} at start-up with an initial
    cycle time ending in 06 or 18 hours. Yellow nodes represent 
    waiting tasks in the held state.}
    \label{fig-QuickStartA-ControlStart06}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{graphics/png/orig/QuickStartA-ControlStart00.png}
    \end{center}
    \caption[Suite {\em examples.QuickStart.a} at start-up, 00 or 12
    hours]{\scriptsize Suite {\em examples.QuickStart.a} at start-up with an
    initial cycle time ending in 00 or 12 hours. Yellow nodes represent 
    waiting tasks in the held state and greyed out nodes are tasks from
    the base graph, defined in the suite.rc file, that aren't currently
    live in the suite.}
    \label{fig-QuickStartA-ControlStart00}
\end{figure} 

The reason for the difference in graph structure between the two figures
is this: cylc starts up with every task present in the waiting
state (blue) at the initial cycle time {\em or} at the first
subsequent valid cycle time for the task - and PostB does not run at 00
or 12. The greyed out tasks are from the base graph, defined in the 
suite.rc file, and aren't actually present in the suite as yet (they
are shown in the graph in order to put the live tasks in context).

Now, select Control $\rightarrow$ Release in gcylc to {\em release the
hold on the suite} and observe what happens: the GetData tasks will
rapidly go off in parallel out to a few cycles ahead (how far ahead
depends on the suite runahead limit as explained below and in {\em The
Suite Runahead Limit}, Section~\ref{RunaheadLimit}) and then the suite
will stall, as shown in Figures~\ref{fig-QuickStartA-ControlRunning}
and~\ref{fig-QuickStartA-ControlStalled}. 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{graphics/png/orig/QuickStartA-ControlRunning.png}
    \end{center}
    \caption[Suite {\em examples.QuickStart.a} running.]
    {\scriptsize Suite {\em examples.QuickStart.a} running, showing several
    consecutive instances of the clock-triggered GetData task running at
    once, out to the suite runahead limit of 12 hours.}
    \label{fig-QuickStartA-ControlRunning}
\end{figure} 
\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{graphics/png/orig/QuickStartA-ControlStalled.png}
    \end{center}
    \caption[ Suite {\em examples.QuickStart.a} stalled.]
    {\scriptsize Suite {\em examples.QuickStart.a} stalled after the
    clock-triggered GetData tasks have finished because of Model's
    previous-cycle dependence and the suite runahead limit.}
    \label{fig-QuickStartA-ControlStalled}
\end{figure} 

The Prep task runs immediately because it has no prerequisites and is
not clock-triggered. The clock-triggered GetData tasks then all go off
at once because they have no prerequisites (i.e.\ they do not have to
wait on any upstream tasks), their trigger time has long passed (the
initial cycle time was in the past), and they are not sequential tasks
(so they are able to run in parallel - try declaring GetData
sequential to see the difference).
Beyond the suite {\em runahead limit} which is set to 12 hours in
this suite - see Section~\ref{RunaheadLimit}) GetData is put into a
special `runahead' held state indicated by the darker blue graph node.
The task will be released from this state when the slower tasks have
caught up sufficiently. The runahead limit is designed to stop quick
tasks from running off too far into the future.  It is typically of
little consequence in real time operation when suites are typically
constrained by clock triggered tasks.

\subsubsection{Viewing The State Of Tasks}

If you're wondering why a particular task has not triggered yet in a
running suite you can view the current state of its prerequisites 
by right-clicking on the task and choosing `View State', or using
\lstinline=cylc show=. Do this for the first Model task, which appears 
to be stuck in the waiting state; it will pop up a small window as in   
Figure~\ref{fig-QuickStartA-ModelState}.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.4\textwidth]{graphics/png/orig/QuickStartA-ModelState.png}
    \end{center}
    \caption[Viewing current task state in gcylc]
{\scriptsize Viewing current task state after right-clicking on a task in gcylc. The
    same information is available from the \lstinline=cylc show= command.}
    \label{fig-QuickStartA-ModelState}
\end{figure} 

It is clear that the reason the task is not running, and consequently,
by virtue of the runahead limit, why the suite has stalled, is
that Model[T] is waiting on Model[T-6] which does not exist at suite
start-up. Model represents a warm-cycled forecast model that depends on a
model background state or restart file(s) generated by its own
previous run.

\subsubsection{Triggering Tasks Manually}

Right-click on the waiting Model task and choose Trigger, or use
\lstinline=cylc trigger=, to force the task to trigger and thereby get
the suite up and running. In a real suite this would not be sufficient:
the real forecast model that Model represents would fail for lack of the
real restart files that it requires as input.  We'll see how to handle
this properly shortly.

\subsubsection{Suite Shut-Down And Restart}

After watching the {\em examples.QuickStart.a} suite run for a while
choose Stop from the Control menu, or \lstinline=cylc stop=, to shut it
down.  The default stop method waits for any tasks that are currently
running to finish before shutting the suite down, so that the final
recorded suite state is perfectly consistent with what actually
happened.  

You can restart the suite from where it left off by choosing 
Control $\rightarrow$ Run and selecting the `restart' option, or using
\lstinline=cylc restart=. Note that cylc always writes a special
state dump, and logs its name, prior to actioning any intervention, and
you can also restart a suite from one of these states, rather than the 
default most recent state.


\subsection{examples.QuickStart.b - Handling Cold-Starts Properly}

Now take a look at {\em examples.QuickStart.b}, which is a minor
modification of {\em examples.QuickStart.a}. Its suite.rc file has a new
{\em cold-start} task called ColdModel,
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[special tasks]]
        cold-start = ColdModel
\end{lstlisting}
\lstset{language=transcript}
and the dependency graph (see also Figure~\ref{fig-QuickStartB-graph18})
looks like this:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[ 0,6,12,18 ]]]
            graph  = """Prep => GetData & ColdModel
                        GetData => Model => PostA
                        ColdModel | Model[T-6] => Model"""
        [[[ 6,18 ]]]
            graph = "Model => PostB"
\end{lstlisting}
\lstset{language=transcript}
In other words, Model[T] can trigger off {\em either} Model[T-6] {\em or} 
ColdModel[T]. 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{graphics/png/orig/QuickStartB-graph18.png}
    \end{center}
    \caption[The {\em examples.QuickStart.b} graph with model cold-start task.]{\scriptsize
    The {\em examples.QuickStart.b} dependency graph showing a model cold
    start task.}
\label{fig-QuickStartB-graph18}
\end{figure}

Cold-start tasks are one-off tasks used in the first cycle to satisfy
another task's intercycle-cycle dependence at suite start-up (when there
is no previous cycle to do it). For instance, a series of cold-start
tasks may be used to cold-start a warm-cycled model. Unlike {\em
start-up} tasks though, cold-start dependence is preserved in subsequent
cycles, so they must generally appear in OR'd conditional triggers in
order to avoid stalling the suite after the first cycle (as in this example). 
This means cold-start tasks can be inserted into a running suite, if
necessary, to cold-start their associated tasks in case of problems that 
prevent continued normal warm cycling. 

A cold-start task in a real suite may submit a real ``cold start
forecast'', or similar, to generate the previous-cycle input files
required by the associated model, or it may just stand in for some
external spinup process, or similar, that has to be completed before the
suite is started (in the latter case the cold-start task would be a
dummy task that just reports successful completion in order to satisfy
the initial previous-cycle dependence of the model).

Run {\em examples.QuickStart.b} to confirm that that no manual triggering is
required to get the suite started now.

\subsection{examples.QuickStart.c - Real Task Implementations}

The suite {\em examples.QuickStart.c} is the same as {\em examples.QuickStart.b}
except that it has real task implementations (scripts located
in the suite bin directory) that generate and consume files in such a
way that they have to run according to the graph of
Figure~\ref{fig-QuickStartB-graph18}. The suite gets them to run
together out of a common I/O workspace, configured via the suite.rc
file. 

By studying this suite and its tasks, and by making quick copies of 
it to modify and run, you should be able to learn a lot about how 
to build real cylc suites. Here's the complete suite definition 
\lstset{language=suiterc}
\lstinputlisting{../examples/QuickStart/c/suite.rc}
\lstset{language=transcript}
And here is the complete implementation for the PostA task
(located with the other task scripts in the suite bin directory):
\lstset{language=bash}
\lstinputlisting{../examples/QuickStart/c/bin/PostA.sh}
\lstset{language=transcript}

\subsection{Monitoring Running Suites}

\subsubsection{Suite stdout and stderr}

Cylc writes some information, including warnings and errors, to the
suite stdout stream. In debug mode (\lstinline=cylc run --debug=) this
output is directed to the terminal; otherwise it is directed to a log 
file under the suite run directory. 

\subsubsection{Suite Logs}

The suite event log records timestamped events as the suite runs; it is
stored under the suite run directory.
\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{graphics/png/orig/suite-log.png}
    \end{center}
\caption[ A cylc suite log.]{\scriptsize A cylc suite log.}
\label{fig-suite-log}
\end{figure}

Figure~\ref{fig-suite-log} shows a suite log viewed from gcylc. The 
\lstinline=cylc log= command also prints the suite event, stdout, and
stderr logs, with optionally filtering of the event log for specific
tasks. 

\subsubsection{Task stdout and stderr Logs}

\lstset{language=transcript}

The stdout and stderr streams from running tasks (if they do not detach 
from the process that does the initial job submission, or manage their
own output) are written to a suite-specific sub-directory of the {\em
suite run directory}.  The location of this directory is determined by
site/user config files, defaulting to 
\lstinline=$HOME/cylc-run/$CYLC_SUITE_REG_NAME/log/job/= (where 
\lstinline=$CYLC_SUITE_REG_NAME= is the registered suite name).

\subsubsection{Other Users's Suites}

If you are on a shared machine with other users running cylc and have read
access to their account, it is possible to use \lstinline=cylc monitor= to
look at their suite's progress without full shell access to their account.
To do this, you will need to copy their suite passphrase into
\lstinline=$HOME/.cylc/SUITE_HOST/SUITE_OWNER/SUITE_NAME/=
(see Section \ref{passphrases}) {\em and} also retrieve the port number of the
running suite (which can typically be found in
\lstinline=~SUITE_OWNER/.cylc/ports/SUITE_NAME=).  Once you have this
information, you can run
\begin{lstlisting}
% cylc monitor --owner=SUITE_OWNER --port=SUITE_PORT SUITE_NAME
\end{lstlisting}
to view the progress of their suite.

\subsection{Searching A Suite}

The cylc suite search tool reports matches in the suite.rc file
by line number, suite section, and file, even if include-files
are used (and even if they are nested), and by file and line number for
matches in the suite bin directory. The following output listing is from
a search of the {\em examples.QuickStart.c} suite. 
\begin{lstlisting}
% cylc grep OUTPUT_DIR examples.QuickStart.c
\end{lstlisting}
\lstset{language=suiterc}
\begin{lstlisting}
SUITE: examples.QuickStart.c /tmp/oliverh/QuickStart/c/suite.rc
PATTERN: OUTPUT_DIR

FILE: /tmp/oliverh/QuickStart/c/suite.rc
   SECTION: [runtime]->[[GetData]]->[[[environment]]]
      (40):             GETDATA_OUTPUT_DIR = $WORKSPACE
   SECTION: [runtime]->[[Models]]->[[[environment]]]
      (45):             MODEL_OUTPUT_DIR = $WORKSPACE
   SECTION: [runtime]->[[Post]]->[[[environment]]]
      (60):             OUTPUT_DIR = $WORKSPACE

FILE: /tmp/oliverh/QuickStart/c/bin/PostB.sh
   (7): cylc checkvars -c OUTPUT_DIR
   (21): touch $OUTPUT_DIR/precip.products

FILE: /tmp/oliverh/QuickStart/c/bin/Model.sh
   (11): cylc checkvars -c MODEL_OUTPUT_DIR MODEL_RUNNING_DIR
   (54): touch $MODEL_OUTPUT_DIR/surface-winds-${CYLC_TASK_CYCLE_TIME}.nc
   (55): touch $MODEL_OUTPUT_DIR/precipitation-${CYLC_TASK_CYCLE_TIME}.nc

FILE: /tmp/oliverh/QuickStart/c/bin/PostA.sh
   (7): cylc checkvars -c OUTPUT_DIR
   (21): touch $OUTPUT_DIR/surface-wind.products

FILE: /tmp/oliverh/QuickStart/c/bin/GetData.sh
   (6): cylc checkvars -c GETDATA_OUTPUT_DIR
   (11): touch $GETDATA_OUTPUT_DIR/obs-${CYLC_TASK_CYCLE_TIME}.nc
\end{lstlisting}
(Suite search is also available from the gcylc {\em Suite} menu).
\lstset{language=transcript}

\subsection{Comparing Suites}

\lstset{language=transcript}

The \lstinline=cylc diff= command compares suites and reports differences
by suite.rc section and item.  Note that some differences may be due to 
suite-name-specific defaults that are not explicitly configured in
either suite.

\subsection{Validating A Suite}

Suite validation checks for errors by parsing the suite definition,
comparing all items against the suite.rc specification file, and then
parsing the suite graph and attempting to instantiate all task proxy
objects. This can be done using the cylc GUIs or \lstinline=cylc validate=:
\lstset{language=transcript}
\begin{lstlisting}
% cylc validate -v foo.bar
Parsing Suite Definition
LOADING suite.rc
VALIDATING against the suite.rc specification.
PARSING clock-triggered tasks
PARSING runtime generator expressions
PARSING runtime hierarchies
PARSING SUITE GRAPH
Instantiating Task Proxies:
root              
 |-GEN            
 | |-OPS          
 | | |-aircraft    ... OK
 | | |-atovs       ... OK
 | | `-atovs_post  ... OK
 | `-VAR          
 |   |-AnPF        ... OK
 |   `-ConLS       ... OK
 |-baz            
 | |-bar1          ... OK
 | `-bar2          ... OK
 |-foo             ... OK
 `-prepobs         ... OK
Suite foo.bar is valid for cylc-4.2.0
\end{lstlisting}

For more information on suite validation see
Section~\ref{Validation}.


\subsection{Other Example Suites}

Cylc has been designed from the ground up to make prototyping and testing
new suites very easy. Simply drawing (in text) a dependency graph in a
new suite definition creates a valid suite that you can run: the tasks
will be {\em dummy tasks} that default to emitting an identifying
message, sleeping for a few seconds, and then exiting; but you can then
arrange for particular tasks to do more complex things by configuring
their runtime properties appropriately.

Cylc has example suites intended to illustrate most facets of suite
construction. These are held centrally under
\lstinline=$CYLC_DIR/examples= and can be imported to your suite
database by running 'cylc admin import-examples'. They all run
``out the box" and can be copied and modified by users to test almost
anything.  Some of them just configure a suite dependency graph, in
which case cylc will run dummy tasks according to the graph; some also
configure task runtime properties (e.g.\ command scripting and
environment variables) within the suite definition; and some have real
task implementations that generate and consume real files and which will
fail if they are not executed in the right order.  All of the example
suites are portable in the sense that all suite and task I/O directory
paths incorporate the suite name (this is the default
situation for any cylc suite in fact) so you can run multiple copies of 
the same suite at once without any interference between them. 

\lstset{language=suiterc}
\lstinputlisting{../examples/QuickStart/z/suite.rc}
\lstset{language=transcript}
(This suite is explained in the {\em Quick Start Guide},
Section~\ref{QuickStartGuide}).


\subsubsection{Choosing The Initial Cycle Time}

When running a suite in live mode that contains clock-triggered tasks do
not give an initial cycle time in the future or nothing will happen
until that time. However, you can also run any suite in {\em simulation
mode} or {\em dummy mode} in which case a future start time is fine (see
Appendix~\ref{SimulationMode}). 
%\pagebreak

\section{Suite Name Registration, DB, and Passphrases}
\label{SuiteRegistration}

Cylc commands target particular suites via names registered in a {\em
suite database}, so that you don't need to remember and continually
refer to the actual location of the suite definition on disk. A suite
name is a hierarchical name akin to a directory path but
delimited by the `.' character; this allows suites to be organised in
nested tree-like structures:
\begin{lstlisting}
% cylc db print -t nwp
nwp          
 |-oper      
 | |-region1  Local Model Region1       /oper/nwp/suite_defs/LocalModel/nested/Region1
 | `-region2  Local Model Region2       /oper/nwp/suite_defs/LocalModel/nested/Region2
 `-test      
   `-region1  Local Model TEST Region1  /home/oliverh/nwp_suites/Regional/TESTING/Region1
\end{lstlisting}

Note that name groups are entirely virtual, they do not need to be
explicitly created before use, and they automatically disappear if all
tasks are removed from them.  From the listing above, for example, to
move the suite \lstinline=nwp.oper.region2= into the
\lstinline=nwp.test= group:
\begin{lstlisting}
% cylc db rereg nwp.oper.region2 nwp.test.region2
REREGISTER nwp.oper.region2 to nwp.test.region2
% cylc db print -tx nwp
nwp          
 |-oper      
 | `-region1  Local Model Region1
 `-test      
   |-region1  Local Model TEST Region1
   `-region2  Local Model Region2
\end{lstlisting}
And to move \lstinline=nwp.test.region2= into a new group \lstinline=nwp.para=:
\begin{lstlisting}
% cylc db rereg nwp.test.region2 nwp.para.region2
REREGISTER nwp.test.region2 to nwp.para.region2
% cylc db print -tx nwp
nwp          
 |-oper      
 | `-region1  Local Model Region1
 |-test      
 | `-region1  Local Model TEST Region1
 `-para
   `-region2  Local Model Region2
\end{lstlisting}

Currently you cannot explicitly indicate a group name on the command
line by appending a dot character. Rather, in database operations such
as copy, reregister, or unregister, the identity of the source item
(group or suite) is inferred from the content of the database; and if
the source item is a group, so must the target be a group (or it will
be, in the case of an item that will be created by the operation). This
means that you cannot copy a single suite into a group that does not 
exist yet unless you specify the entire target suite name. 

\lstinline=cylc db register --help= shows a number of other examples.

\subsection{Suite Name Databases}

Each user has a database that associates registered suite names
with their respective suite definition directory locations. Hierarchical
suite names stored in the database can be displayed in a tree structure.

Note that the suite title held in the name database is parsed
from the suite definition at the time of initial registration; if you
change the title (by editing the suite.rc file) use 
\lstinline=cylc db refresh= to update the database.

The user suite database file is \lstinline=$HOME/.cylc/DB=.

\subsubsection{Database Operations}

On the command line, the  `database' (or `db') command category contains
commands to implement the aforementioned operations.

\lstset{language=usage}
\begin{lstlisting}
% cylc db help
CATEGORY: db|database - Suite name registration, copying, deletion, etc.

HELP: cylc [db|database] COMMAND help,--help
  You can abbreviate db|database and COMMAND.
  The category db|database may be omitted.

COMMANDS:
  alias ............... Register an alternative name for a suite
  copy|cp ............. Copy a suite or a group of suites
  get-directory ....... Retrieve suite definition directory paths
  print ............... Print registered suites
  refresh ............. Report invalid registrations and update suite titles
  register ............ Register a suite for use
  reregister|rename ... Change the name of a suite
  unregister .......... Unregister and optionally delete suites
\end{lstlisting}

Groups of suites (at any level in the name hierarchy) can be deleted, 
copied, imported, and exported; as well as individual suites.  To do this, 
just use suite group names as source and/or target for operations, as
appropriate.  For instance, if a group \lstinline=foo.bar= contains the 
suites \lstinline=foo.bar.baz= and \lstinline=foo.bar.qux=, you can copy
a single suite like this:
\begin{lstlisting}
% cylc copy foo.bar.baz boo $HOME/suites
\end{lstlisting}
(resulting in a new suite \lstinline=boo=); or the group like this:
\begin{lstlisting}
% cylc copy foo.bar boo $HOME/suites
\end{lstlisting}
(resulting in new suites \lstinline=boo.baz= and \lstinline=boo.qux=); or the group like this:
\begin{lstlisting}
% cylc copy foo boo $HOME/suites
\end{lstlisting}
(resulting in new suites \lstinline=boo.bar.baz= and \lstinline=boo.bar.qux=).
When suites are copied, the suite definition directories are copied into
a directory tree, under the target directory, that reflects the
suite name hierarchy. \lstinline=cylc copy --help= has some explicit examples.

The same functionality is also available by right-clicking on suites
or groups in the gcylc ``Open Registered Suite'' dialog.

\subsection{Suite Passphrases}

Any client process that connects to a running suite (this includes task
messaging and user-invoked interrogation and control commands) must
authenticate with a secure passphrase that has been loaded by the suite. 
A random passphrase is generated automatically in the suite definition
directory at registration time if one does not already exist there. For 
the default Pyro-based connection method the passphrase file must be
distributed to other accounts that host running tasks or from which
you need monitoring or control access to the running suite.

Alternatively, cylc can be configured to,
\begin{myenumerate}
\item use ssh to re-invoke task messaging commands on the suite host; or
\item use a one-way polling mechanism for tracking task progress. 
\end{myenumerate}
Neither of these methods require the suite passphrase to be installed 
on the task host. For ssh re-invocation ssh keys must be installed for
the task-to-suite direction in addition to the suite-to-task setup
already required for job submission. The automatic polling mechanism can
be used as a last resort for hosts that do not allow routing back to the
suite host for pyro or ssh. It can also be used as regular health check 
on submitted tasks under the other communications methods.

See Section~\ref{RunningSuites} for more detail on cylc client/server
communications, and how to use it.


%\pagebreak
\section{Suite Definition} 
\label{SuiteDefinition}

Cylc suites are defined in structured, validated, {\em suite.rc} files
that concisely specify the properties of, and the relationships
between, the various tasks managed by the suite. This section of the
User Guide deals with the format and content of the suite.rc file,
including task definition. Task implementation - what's required of the
real commands, scripts, or programs that do the processing that the
tasks represent - is covered in Section~\ref{TaskImplementation}; and
task job submission - how tasks are submitted to run - is in
Section~\ref{TaskJobSubmission}.

\subsection{Suite Definition Directories}

A cylc {\em suite definition directory} contains:
\begin{myitemize}
    \item {\bf A suite.rc file}: this is the suite definition.
        \begin{myitemize}
            \item And any include-files used in it (see below; may be
                kept in sub-directories).
        \end{myitemize}
    \item {\bf A \lstinline=bin/= sub-directory}.
        \begin{myitemize}
            \item {\em Optional.}
            \item For scripts and executables that implement, or are
                used by, suite tasks.
            \item Automatically added to \lstinline=$PATH= in task
                execution environments.
            \item Alternatively, tasks can call external
                commands, scripts, or programs; or they can be scripted
                entirely within the suite.rc file.
        \end{myitemize}
    \item {\bf A \lstinline=python/= sub-directory}.
        \begin{myitemize}
            \item {\em Optional.}
            \item For user-defined job-submission methods, if needed (see
                Section~\ref{TaskJobSubmission}).
            \item Alternatively, new job submission methods can be
                installed into the cylc source tree, or in any directory
                in your \lstinline=$PYTHONPATH=.
        \end{myitemize}
    \item {\bf Any other sub-directories and files} - documentation,
        control files, etc.
        \begin{myitemize}
            \item {\em Optional.}
            \item Holding everything in one place makes proper suite
                revision control possible.
            \item Portable access to files here, for running tasks, is
                provided through  
                \lstinline=$CYLC_SUITE_DEF_PATH= 
                (see Section~\ref{TaskExecutionEnvironment}).
            \item Ignored by cylc, but the entire suite definition
                directory tree is copied when you copy a
                suite using cylc commands.

        \end{myitemize}
\end{myitemize}
A typical example:
\lstset{language=transcript}
\begin{lstlisting}
/path/to/my/suite   # suite definition directory
    suite.rc           # THE SUITE DEFINITION FILE
    bin/               # scripts and executables used by tasks
        foo.sh
        bar.sh
        ...
    # (OPTIONAL) any other suite-related files, for example:
    inc/               # suite.rc include-files
        nwp-tasks.rc
        globals.rc
        ...
    doc/               # documentation
    control/           # control files
    ancil/             # ancillary files
    ...
\end{lstlisting}

\subsection{Suite.rc File Overview}
\label{SuiteRCFile}

Suite.rc files conform to the ConfigObj extended INI format
(\url{http://www.voidspace.org.uk/python/configobj.html}) 
with several modifications to allow continuation lines and include-files,
and to make it legal to redefine environment variables and scheduler 
directives (duplicate config item definitions are normally flagged as an
error).

Additionally, embedded template processor expressions may be used in the 
file, to programatically generate the final suite definition
seen by cylc.  Currently the Jinja2 template engine is supported
(\url{http://jinja.pocoo.org/docs}). In the future cylc may provide a
plug-in interface to allow use of other template engines too.
See {\em Jinja2 Suite Templates}
(Section~\ref{Jinja2}) for some examples. 

\subsubsection{Syntax}

The following list shows legal raw suite.rc syntax. Suites using the
Jinja2 template processor (Section~\ref{Jinja2}) can of course use
Jinja2 syntax as well (it must generate raw syntax on processing).

\begin{myitemize}
    \item {\bf Entries} are of the form \lstinline@item = value@, and
        item names may contain spaces.
    \item {\bf Comments} \# follow a hash character.
    \item {\bf List Values} are comma, separated.
    \item {\bf Strings} must be quoted ``if, they, contain, commas''
    \item {\bf Multiline Strings} must be ``````triple-quoted''''''.
    \item {\bf Boolean Values} are written as True or False (capitalized).
    \item {\bf White Space} is ignored; indentation can be used for clarity.
    \item {\bf Continuation Lines} follow a trailing backslash.\textbackslash
    \item {\bf [Section Headings]} are enclosed in square brackets.
    \item {\bf [[Subsection Nesting]]} is indicated by the number of
        square brackets.\footnote{Sections are closed by the next
        section heading, so items within a section must be
        defined before any subsequent subsection headings.}
    \item {\bf Duplicate Items} are illegal, except in
        \lstinline=environment= and \lstinline=directives=
        sections.\footnote{The exceptions were designed to allow 
        tasks to override environment variables defined in include-files
        that could be included in multiple tasks, to assist in factoring
        out common task configuration. However, namespace inheritance
        now provides a better way to do this in most cases.}
    \item {\bf Include-files} can be used: \lstinline=%include inc/foo.rc=.
\end{myitemize}
        

The following pseudo-listing illustrates suite.rc syntax:
\lstset{language=suiterc}
\begin{lstlisting}
# a full line comment
an item = value # a trailing comment
a boolean item = True # or False
one string item = the quick brown fox # string quotes optional ...
two string item = "the quick, brown fox" # ... unless internal commas
a multiline string item = """the quick brown fox
jumped over the lazy dog""" # triple quoted
a list item = foo, bar, baz   # comma separated
a list item with continuation = a, b, c, \
                                d, e, f
[section]
    item = value
%include inc/vars/foo.inc  # include file
    [[subsection]]
        item = value
        [[[subsubsection]]]
            item = value
[another section]
    [[another subsection]]
        # ...
    # ...
# ...
\end{lstlisting}

\subsubsection{Include-Files}

Cylc has native support for suite.rc include-files, which may help to
organize large suites. Inclusion boundaries are completely arbitrary -
you can think of include-files as chunks of the suite.rc file simply
cut-and-pasted into another file.  Include-files may be included
multiple times in the same file, and even nested.  Include-file paths
can be specified portably relative to the suite definition directory,
e.g.:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
# include the file $CYLC_SUITE_DEF_PATH/inc/foo.rc:
%include inc/foo.rc
\end{lstlisting}

\paragraph{Editing Temporarily Inlined Suites}

Cylc's native file inclusion mechanism supports optional inlined
editing: 
\begin{lstlisting}
% cylc edit --inline SUITE
\end{lstlisting}
The suite will be split back into its constituent include-files when you
exit the edit session. While editing, the inlined file becomes the
official suite definition so that changes take effect whenever you save
the file.  See \lstinline=cylc prep edit --help= for more information.

\paragraph{Include-Files via Jinja2}

Jinja2 (Section~\ref{Jinja2}) provides template inclusion functionality
although this is more akin to Python module import than simple text
inclusion, and the implications of this for suite design have not yet
been explored.

\subsubsection{Syntax Highlighting For Suite Definitions}
\label{SyntaxHighlighting}

\lstset{language=transcript}
Cylc comes with a syntax file to configure suite.rc syntax
highlighting and section folding in the {\em vim} editor, as shown in
Figure~\ref{fig-cylc-vim}. We also have an {\em emacs} font-lock mode,
and syntax files for the {\em gedit} and {\em kate} editors: 
\begin{lstlisting}
$CYLC_DIR/conf/cylc.vim     # vim
$CYLC_DIR/conf/cylc-mode.el # emacs
$CYLC_DIR/conf/cylc.lang    # gedit (and other gtksourceview programs)
$CYLC_DIR/conf/cylc.xml     # kate
\end{lstlisting}
Refer to comments at the top of each file to see how to use them.

\subsubsection{Gross File Structure}

Cylc suite.rc files consist of a suite title and description followed by
configuration items grouped under several top level section headings:

\begin{myitemize}
    \item {\bf [cylc] } - {\em non task-specific suite configuration}
    \item {\bf [scheduling] } - {\em determines when tasks are ready to run}
        \begin{myitemize}
            \item tasks with special behavior, e.g. clock-triggered tasks
            \item the dependency graph, which defines the relationships
                between tasks
        \end{myitemize}
    \item {\bf [runtime] } - {\em determines how, where, and what to
        execute when tasks are ready}
        \begin{myitemize}
            \item command scripting, environment, job submission, remote
                hosting, etc.
            \item suite-wide defaults in the {\em root} namespace
            \item a nested family hierarchy with common properties
                inherited by related tasks
        \end{myitemize}
    \item {\bf [visualization] } - configures suite graphing.
\end{myitemize}


\subsubsection{Validation}
\label{Validation}

Cylc suite.rc files are automatically validated against a specification
that defines all legal entries, values, options, and defaults 
(held in \lstinline=$CYLC_DIR/conf/suiterc/=). This detects any formatting
errors, typographic errors, illegal items and illegal values prior to 
run time. Some values are complex strings that require further parsing
by cylc to determine their correctness (this is also done during
validation). All legal entries are documented in the {\em Suite.rc
Reference} (Appendix~\ref{SuiteRCReference}).

The validator reports the line numbers of detected errors. Here's an 
example showing a subsection heading with a missing right bracket.
\begin{lstlisting}
% cylc validate foo.bar
Parsing Suite Config File
ERROR: [[special tasks]
NestingError('Cannot compute the section depth at line 19.',)
_validate foo.bar failed:  1
\end{lstlisting}

If the suite.rc file contains include-files you can use 
\lstinline=cylc view= to view an inlined copy with correct line numbers
(you can also edit suites in a temporarily inlined state with 
\lstinline=cylc edit --inline=).

Validation does not check the validity of chosen job submission methods;
this is to allow users to extend cylc with their own job submission 
methods, which are by definition unknown to the suite.rc spec.


\subsection{Scheduling - Dependency Graphs}
\label{ConfiguringScheduling}
 
\lstset{language=suiterc}
The \lstinline=[scheduling]= section of a suite.rc file defines the
relationships between tasks in a suite - the information that allows
cylc to determine when tasks are ready to run. The most important
component of this is the suite dependency graph. Cylc graph notation
makes clear textual graph representations that are very concise because
sections of the graph that repeat at different hours of the day, say,
only have to be defined once.  Here's an example with dependencies that
vary depending on cycle time:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[0,6,12,18]]] # validity (hours of the day)
            graph = """
A => B & C   # B and C trigger off A
A[T-6] => A  # Model A restart trigger
                    """
        [[[6,18]]]
            graph = "C => X"
\end{lstlisting}
\lstset{language=transcript}
Figure~\ref{fig-dep-eg-1} shows the complete suite.rc listing alongside
the suite graph.
This is actually a complete, valid, runnable suite (it will use default 
runtime properties and command scripting and you'll need to trigger task A
manually to get the suite started because A[T] depends on A[T-6] and at
start-up there is no previous cycle to satisfy that dependence - how to
handle this properly is described in 
{\em Handling Intercycle Dependencies At Start-Up}
(Section~\ref{HandlingIntercycleDependenceAtStartUp}) and in the
{\em Quick Start Guide} (Section~\ref{QuickStartGuide}).

\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = "Dependency Graph Example"
[scheduling]
    [[dependencies]]
        [[[0,6,12,18]]] # validity (hours)
            graph = """
A => B & C   # B and C trigger off A
A[T-6] => A  # Model A restart trigger
                    """
        [[[6,18]]] # hours
            graph = "C => X"
[visualization]
    [[node attributes]]
        X = "color=red"
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/png/orig/dep-eg-1.png}
    \end{center}
\end{minipage}
\caption[Example Suite]{\scriptsize Example Suite}
\label{fig-dep-eg-1}
\end{figure}

\subsubsection{Graph String Syntax}

Multiline graph strings may contain:
\begin{myitemize}
    \item {\bf blank lines}
    \item {\bf arbitrary white space}
    \item {\bf internal comments:} following the \lstinline=#= character
    \item {\bf conditional task trigger expressions} - see below.
\end{myitemize}

\subsubsection{Interpreting Graph Strings}

Suite dependency graphs can be broken down into pairs in which the left
side (which may be a single task or family, or several that are
conditionally related) defines a trigger for the task or family on the
right. For instance the ``word graph'' {\em C triggers off B which
triggers off A} can be deconstructed into pairs {\em C triggers off B}
and {\em B triggers off A}.  In this section we use only the default 
trigger type, which is to trigger off the upstream task succeeding; see
Section~\ref{TriggerTypes} for other available triggers.

In the case of cycling tasks, the triggers defined by a graph string are 
valid for cycle times matching the list of hours specified for the
graph section. For example this graph,
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[0,12]]]
            graph = "A => B"
\end{lstlisting}
\lstset{language=transcript}
implies that B triggers off A for cycle times in which the hour matches
$0$ or $12$.

To define intercycle dependencies, attach an offset indicator to the
left side of a pair:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[0,12]]]
            graph = "A[T-12] => B"
\end{lstlisting}
\lstset{language=transcript}
This means B[T] triggers off A[T-12] for cycle times T with hours
matching $0$ or $12$. Note that {\em T must be left implicit unless
there is a cycle time offset} (this helps to keep graphs clean and
concise because the majority of tasks in a typical suite will only
depend on others with the same cycle time) and that {\em cycle time
offsets can only appear on the left} (because each pair defines a
trigger for the right task at cycle time T).

Now, having explained that dependency graphs are interpreted pairwise,
you can optionally chain pairs together to ``follow a path'' through the
graph. So this,
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
    graph = """A => B  # B triggers off A
               B => C  # C triggers off B"""
\end{lstlisting}
is equivalent to this:
\begin{lstlisting}
# SUITE.RC
    graph = "A => B => C"
\end{lstlisting}

Cycle time offsets, if they appear in a chain of triggers, must be
leftmost (because, as explained previously they can't appear on the
right of any pair). So this is legal:
\begin{lstlisting}
# SUITE.RC
    graph = "A[T-6] => B => C"  # OK
\end{lstlisting}
but this isn't:
\begin{lstlisting}
# SUITE.RC
    graph = "A => B[T-6] => C"  # ERROR!
\end{lstlisting}
The trigger \lstinline@A => B[T-6]@ does not make sense in any case -
if this kind of relationship seems necessary it probably means that B
should be ``reassigned'' to the next cycle (keep in mind that cycle
time is really just a label used to define the relatonships between
tasks).

{\em Each trigger in the graph must be unique} but {\em the same task
can appear in multiple pairs or chains}. Separately defined triggers
for the same task have an AND relationship. So this:
\begin{lstlisting}
# SUITE.RC
    graph = """A => X  # X triggers off A
               B => X  # X also triggers off B"""
\end{lstlisting}
is equivalent to this:
\begin{lstlisting}
# SUITE.RC
    graph = "A & B => X"  # X triggers off A AND B
\end{lstlisting}

In summary, the branching tree structure of a dependency graph can 
be partitioned into lines (in the suite.rc graph string) of pairs
or chains, in any way you like, with liberal use of internal white space
and comments to make the graph structure as clear as possible.

\begin{lstlisting}
# SUITE.RC
# B triggers if A succeeds, then C and D trigger if B succeeds:
    graph = "A => B => C & D"
# which is equivalent to this:
    graph = """A => B => C
               B => D"""
# and to this:
    graph = """A => B => D
               B => C"""
# and to this:
    graph = """A => B
               B => C
               B => D"""
# and it can even be written like this:
    graph = """A => B # blank line follows:

               B => C # comment ...
               B => D"""
\end{lstlisting}

\paragraph{Handling Long Graph Lines}

Long chains of dependencies can be split into pairs:
\begin{lstlisting}
# SUITE.RC
    graph = "A => B => C"
# is equivalent to this:
    graph = """A => B 
               B => C"""
# BUT THIS IS AN ERROR:
    graph = """A => B => # WRONG!
               C"""      # WRONG!
\end{lstlisting}
If you have very long task names, or long conditional trigger
expressions (below) then you can use the suite.rc line continuation
marker:
\begin{lstlisting}
# SUITE.RC
    graph = "A => B \
    => C"  # OK
\end{lstlisting}
Note that a line continuation marker must be the final character on
the line; it cannot be followed by trailing spaces or a comment.

\lstset{language=transcript}

\subsubsection{Graph Types (VALIDITY)}
\label{GraphTypes}

A suite definition can contain multiple graph strings that are combined
to generate the final graph. There are different graph VALIDITY section
headings for cycling, one-off asynchronous, and repeating asynchronous
tasks. Additionally, there may be multiple graph strings under different
VALIDITY sections for cycling tasks with different dependencies at
different cycle times.

\paragraph{One-off Asynchronous Tasks}

Figure~\ref{fig-test1} shows a small suite of one-off asynchronous
tasks; these have no associated cycle time and don't spawn successors
(once they're all finished the suite just exits).  The integer $1$
attached to each graph node is just an arbitrary label, akin to the task
cycle time in cycling tasks; it increments when a repeating asynchronous
task (below) spawns.
\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = some one-off asynchronous tasks
[scheduling]
    [[dependencies]]
        graph = "foo => bar & baz => qux"
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.25\textwidth]{graphics/png/orig/test1.png}
    \end{center}
\end{minipage}
\caption[One-off Asynchronous Tasks]{\scriptsize One-off Asynchronous Tasks.}
\label{fig-test1}
\end{figure}

\paragraph{Cycling Tasks}

For cycling tasks the graph VALIDITY section heading defines a sequence of 
cycles times for which the subsequent graph section is valid. 
Figure~\ref{fig-test2} shows a small suite of cycling tasks.
\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = some cycling tasks
# (no dependence between cycles here)
[scheduling]
    [[dependencies]]
        [[[0,12]]]
            graph = "foo => bar & baz => qux"
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/png/orig/test2.png}
    \end{center}
\end{minipage}
\caption[Cycling Tasks]{\scriptsize Cycling Tasks.}
\label{fig-test2}
\end{figure}

\subparagraph{Stepped Daily, Monthly, And Yearly Cycling}

In addition to the original hours-of-the-day section headings, cylc now
has an extensible cycling mechanism and (so far) stepped daily, monthly,
and yearly cycling modules:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[Daily(20100809,2)]]]
            graph = "foo => bar"
        [[[Monthly(201008,2)]]]
            graph = "cat[T-2] => dog"
        [[[Yearly(2010,2)]]]
            graph = "apple => orange"
\end{lstlisting}
In the examples above the section headings define an infinite sequence of 
cycle times {\em anchored} on the first (date-time) argument and {\em
stepped} by the second (integer) argument. The anchoring serves to
generate the same sequence, as opposed to some off-set sequence,
regardless of the initial cycle time from which the suite is started.
The anchor date can lie outside of the suite's initial and final cycle
times.

Note that hours-of-the-day graph section headings can also be written
to explicitly reference the associated cycling module:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[HoursOfTheDay(0,6,12,18)]]] # same as [[[0,6,12,18]]]
            graph = "red => blue"
\end{lstlisting}

\subparagraph{How Multiple Graph Strings Combine}
\label{HowMultipleGraphStringsCombine}

For a cycling graph with multiple validity sections for different
hours of the day, the different sections {\em add} to generate the
complete graph. Different graph sections can overlap (i.e.\ the same
hours may appear in multiple section headings) and the same tasks may
appear in multiple sections, but individual dependencies should be
unique across the entire graph. For example, the following graph defines
a duplicate prerequisite for task C:
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[0,6,12,18]]]
            graph = "A => B => C"
        [[[6,18]]]
            graph = "B => C => X"
            # duplicate prerequisite: B => C already defined at 6, 18
\end{lstlisting}
This does not affect scheduling, but for the sake of clarity and brevity
the graph should be written like this:
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[0,6,12,18]]]
            graph = "A => B => C"
        [[[6,18]]]
            # X triggers off C only at 6 and 18 hours
            graph = "C => X"
\end{lstlisting}

\paragraph{Combined Asynchronous And Synchronous Graphs}

Cycling tasks can be made to wait on one-off asynchronous tasks, as
shown in Figure~\ref{fig-test4}. Alternatively, they can be made to wait 
on one-off synchronous start-up tasks, which have an associated cycle
time even though they are non-cycling - see Figure~\ref{fig-test5}.

\subparagraph{Synchronous Start-up vs One-off Asynchronous Tasks}

One-off synchronous start-up tasks run only when a cycling suite is
{\em cold-started} and they are often associated with subsequent one-off
{\em cold-start tasks} used to bootstrap a cycling suite into existence. 

The distinction between cold- and warm-start is only meaningful for
cycling tasks, and one-off asynchronous tasks may be best used in
constructing entirely non-cycling suites.

However, one-off asynchronous tasks can precede cycling tasks in the
same suite, as shown above. It seems likely that, if used in this way,
they will be intended as start-up tasks - so currently {\em one-off
asynchronous tasks only run in a cold-start}.

\lstset{language=suiterc}
\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = one-off async and cycling tasks
# (with dependence between cycles too)
[scheduling]
    [[dependencies]]
        graph = "prep1 => prep2"
        [[[0,12]]]
            graph = """
    prep2 => foo => bar & baz => qux
    foo[T-12] => foo
                    """
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/png/orig/test4.png}
    \end{center}
\end{minipage}
\caption[One-off Asynchronous and Cycling Tasks]{\scriptsize One-off
asynchronous and cycling tasks in the same suite.}
\label{fig-test4}
\end{figure}

\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = one-off start-up and cycling tasks
# (with dependence between cycles too)
[scheduling]
    [[special tasks]]
        start-up = prep1, prep2
    [[dependencies]]
        [[[0,12]]]
            graph = """
    prep1 => prep2 => foo => bar & baz => qux
    foo[T-12] => foo
                    """
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/png/orig/test5.png}
    \end{center}
\end{minipage}
\caption[One-off Synchronous and Cycling Tasks]{\scriptsize One-off
synchronous and cycling tasks in the same suite.}
\label{fig-test5}
\end{figure}

\paragraph{Repeating Asynchronous Tasks}

Repeating asynchronous tasks can be used, for example, to process
satellite data that arrives at irregular time intervals. Each new
dataset must have a unique ``asynchronous ID''. If it doesn't naturally
have such an ID a string representation of the data arrival time could
be used.  The graph VALIDITY section heading must contain ``ASYNCID:''
followed by a regular expression that matches the actual IDs.
Additionally, one task in the suite must be a designated ``daemon'' 
that waits indefinitely on incoming data and reports each new dataset
(and its ID) back to the suite by means of a special output message.
When the daemon task proxy receives a matching message it dynamically
registers a new output (containing the ID) that downstream tasks can
then trigger off. The downstream tasks likewise have prerequisites
containing the ID pattern (because they trigger off the aforementioned
outputs) and when these get satisfied during dependency negotiation the
actual ID is substituted into their own registered outputs.
Finally, each asynchronous repeating task proxy passes the ID to its
task execution environment as
\lstinline=$ASYNCID= to allow identification of the correct dataset by 
task scripts.
In this way a tree of tasks becomes dedicated to processing each 
new dataset, and multiple datasets can be processed in parallel if
they become available in quick succession. As Figure~\ref{fig-test3}
shows, a repeating asynchronous suite currently plots just like a
one-off asynchronous suite. But at run time the daemon task stays put,
while the others continually spawn successors to wait for new datasets
to come in. The {\em asynchronous.repeating} example suite demonstrates
how to do this in a real suite. {\em Note that other trigger types
(success, failure, start, suicide, and conditional) cannot currently be 
used in a repeating asynchronous graph section.}
\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = a suite of repeating asynchronous tasks
# for processing real time satellite datasets
[scheduling]
    [[dependencies]]
        [[[ASYNCID:satX-\d{6}]]]
            # match datasets satX-1424433 (e.g.)
            graph = "watcher:a => foo:a & bar:a => baz"
            daemon = watcher
[runtime]
    [[watcher]]
        [[[outputs]]]
            a = "New dataset <ASYNCID> ready for processing"
    [[foo,bar]]
        [[[outputs]]]
            a = "Products generated from dataset <ASYNCID>"
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.45\textwidth]{graphics/png/orig/test6.png}
    \end{center}
\end{minipage}
\caption[Repeating Asynchronous Tasks]{\scriptsize Repeating Asynchronous Tasks.}
\label{fig-test3}
\end{figure}

\subsubsection{Trigger Types}
\label{TriggerTypes}

\lstset{language=suiterc}

Trigger type, indicated by {\em:type} after the upstream task (or family)
name, determines what kind of event results in the downstream task (or
family) triggering.

\paragraph{Success Triggers}

The default, with no trigger type specified, is to trigger off the
upstream task succeeding:
\begin{lstlisting}
# SUITE.RC
# B triggers if A SUCCEEDS:
    graph = "A => B"
\end{lstlisting}
For consistency and completeness, however, the success trigger can be explicit:
\begin{lstlisting}
# SUITE.RC
# B triggers if A SUCCEEDS:
    graph = "A => B"
# or:
    graph = "A:succeed => B"
\end{lstlisting}

\paragraph{Failure Triggers}

To trigger off the upstream task reporting failure:
\begin{lstlisting}
# SUITE.RC
# B triggers if A FAILS:
    graph = "A:fail => B"
\end{lstlisting}
Section~\ref{SuicideTriggers} ({\em Suicide Triggers}) shows
one way of handling task B here if A does not fail.

\paragraph{Start Triggers}

To trigger off the upstream task starting to execute:
\begin{lstlisting}
# SUITE.RC
# B triggers if A STARTS EXECUTING:
    graph = "A:start => B"
\end{lstlisting}
This can be used to trigger tasks that monitor other tasks once they 
(the target tasks) start executing. Consider a long-running forecast model,
for instance, that generates a sequence of output files as it runs. A
postprocessing task could be launched with a start trigger on the model
(\lstinline@model:start => post@) to process the model output as it
becomes available. Note, however, that there are several alternative
ways of handling this scenario: both tasks could be triggered at the
same time (\lstinline@foo => model & post@), but depending on
external queue delays this could result in the monitoring task starting
to execute first; or a different postprocessing task could be
triggered off an internal output for each data file 
(\lstinline@model:out1 => post1@ etc.; see
Section~\ref{InternalTriggers}), but this may not be practical if the
number of output files is large or if it is difficult to add cylc
messaging calls to the model.

\paragraph{Finish Triggers}

To trigger off the upstream task succeeding or failing, i.e.\ finishing 
one way or the other:
\begin{lstlisting}
# SUITE.RC
# B triggers if A either SUCCEEDS or FAILS:
    graph = "A | A:fail => B"
# or
    graph = "A:finish => B"
\end{lstlisting}

\paragraph{Internal (Message) Triggers}
\label{InternalTriggers}

These allow triggering off off events that occur while a task runs. A
special event message must be registered in the suite definition, and 
deliberately sent by the task at the appropriate time.
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[6,18]]]
            # B triggers off internal output "upload1" of task A:
            graph = "A:upload1 => B"
[runtime]
    [[A]]
        [[[outputs]]]
            upload1 = "NWP products uploaded for [T]"
\end{lstlisting}
Task A must emit this message when the actual output has been completed - 
see {\em Reporting Internal Outputs Completed} (Section~\ref{RIOC}).

\paragraph{Job Submission Triggers}

It is also possible to trigger off a task submitting, or failing to submit:
\begin{lstlisting}
# SUITE.RC
# B triggers if A submits successfully:
    graph = "A:submit => B"
# D triggers if C fails to submit successfully:
    graph = "C:submit-fail => D"
\end{lstlisting}

A possible use case for submit-fail triggers: if a task goes into the
submit-failed state, possibly after several job submission retries,
another task that inherits the same runtime but sets a different job
submission method and/or host could be triggered to, in effect, run the
same job on a different platform.

\paragraph{Intercycle Triggers}
\label{IntercycleTriggers}

Typically most tasks in a suite will trigger off others in the same
cycle time, but some may depend on others with other cycle times. This
notably applies to warm-cycled forecast models, which depend
on their own previous instances (see below); but other kinds of
intercycle dependence are possible too.\footnote{In NWP forecast
analysis suites parts of the observation processing and data
assimilation subsystem will typically also depend on model background
fields generated by the previous forecast.} Here's how to express this
kind of relationship in cylc:
\begin{lstlisting}
# SUITE.RC
[dependencies]
    [[0,6,12,18]]
        # B triggers off A in the previous cycle
        graph = "A[T-6] => B"
\end{lstlisting}
Intercycle and trigger type (and internal output) notation can be combined:
\begin{lstlisting}
# SUITE.RC
    # B triggers if A in the previous cycle fails:
    graph = "A[T-6]:fail => B"
\end{lstlisting}

Tasks can also trigger off others in {\em the future} (with respect to
cycle time!):
\begin{lstlisting}
# SUITE.RC
[dependencies]
    [[0,6,12,18]]
        # B triggers off A in the next cycle
        graph = "A[T+6] => B"
\end{lstlisting}

\subparagraph{Bootstrapping Intercycle Triggers}

Tasks with intercycle triggers require an associated {\em cold-start}
task to bootstrap them into operation when the suite is cold-started,
because they depend on a previous cycle that does not exist at start
time. Otherwise the first such task will require manual triggering (and
that will only suffice if the real task does not have real
previous-cycle dependence in the first cycle).
Section~\ref{HandlingIntercycleDependenceAtStartUp}, {\em Handling
Intercycle Dependence At Start-Up}, explains how to use cold-start tasks
in cylc.
 
\paragraph{Conditional Triggers}

AND operators (\lstinline=&=) can appear on both sides of an arrow. They
provide a concise alternative to defining multiple triggers separately:
\begin{lstlisting}
# SUITE.RC
# 1/ this:
    graph = "A & B => C"
# is equivalent to:
    graph = """A => C
               B => C"""
# 2/ this:
    graph = "A => B & C"
# is equivalent to:
    graph = """A => B
               A => C"""
# 3/ and this:
    graph = "A & B => C & D"
# is equivalent to this:
    graph = """A => C
               B => C
               A => D
               B => D"""
\end{lstlisting}

OR operators (\lstinline=|=) which result in true conditional triggers,
can only appear on the left,\footnote{An OR
operator on the right doesn't make much sense: if ``B or C'' triggers
off A, what exactly should cylc do when A finishes?}
\begin{lstlisting}
# SUITE.RC
# C triggers when either A or B finishes:
    graph = "A | B => C"
\end{lstlisting}

Forecasting suites typically have simple conditional
triggering requirements, but any valid conditional expression can be
used, as shown in Figure~\ref{fig-conditional} 
(conditional triggers are plotted with open arrow heads).
\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
        graph = """
# D triggers if A or (B and C) succeed
A | B & C => D
# just to align the two graph sections
D => W
# Z triggers if (W or X) and Y succeed
(W|X) & Y => Z
                """
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{graphics/png/orig/conditional-triggers.png} 
    \end{center}
\end{minipage}
\caption[Conditional Triggers] {\scriptsize
Conditional triggers are plotted with open arrow heads.}
\label{fig-conditional} 
\end{figure}

\paragraph{Suicide Triggers}
\label{SuicideTriggers}

Suicide triggers take tasks out of the suite. This can be used for
automated failure recovery. The suite.rc listing and accompanying 
graph in Figure~\ref{fig-suicide} show how to define a chain of failure
recovery tasks
that trigger if they're needed but otherwise remove themselves from the
suite (you can run the {\em AutoRecover.async} example suite to see how
this works).  The dashed graph edges ending in solid dots indicate
suicide triggers, and the open arrowheads indicate conditional triggers
as usual.

\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = asynchronous automated recovery
description = """
Model task failure triggers diagnosis 
and recovery tasks, which take themselves
out of the suite if model succeeds. Model
post processing triggers off model OR 
recovery tasks.
              """
[scheduling]
    [[dependencies]]
        graph = """
pre => model
model:fail => diagnose => recover
model => !diagnose & !recover
model | recover => post
                """
[runtime]
    [[model]]
        # UNCOMMENT TO TEST FAILURE:
        # command scripting = /bin/false
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{graphics/png/orig/suicide.png} 
    \end{center}
\end{minipage}
\caption[Automated failure recovery via suicide triggers] {\scriptsize
Automated failure recovery via suicide triggers.}
\label{fig-suicide} 
\end{figure}

Note that multiple suicide triggers combine in the same way as other triggers, so this:
\begin{lstlisting}
foo => !baz
bar => !baz
\end{lstlisting}
is equivalent to this:
\begin{lstlisting}
foo & bar => !baz
\end{lstlisting}
i.e.\ both \lstinline=foo= and \lstinline=bar= must succeed for
\lstinline=baz= to be taken out of the suite. If you really want a task
to be taken out if any one of several events occurs then be careful to
write it that way:
\begin{lstlisting}
foo | bar => !baz
\end{lstlisting}

\paragraph{Family Triggers}
\label{FamilyTriggers}

Families defined by the namespace inheritance hierarchy
(Section~\ref{NIORP}) can be used in the graph trigger whole groups of
tasks at the same time (e.g.\ forecast model ensembles and groups of
tasks for processing different observation types at the same time) and 
for triggering downstream tasks off families as a whole. Higher level
families, i.e.  families of families, can also be used, and are reduced
to the lowest level member tasks. Note that tasks can also trigger off
individual family members if necessary. 

To trigger an entire task family at once:
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        graph = "foo => fam"
[runtime]
    [[fam]]    # a family (because others inherit from it)
    [[m1,m2]]  # family members (inherit from namespace fam)
        inherit = fam
\end{lstlisting}
This is equivalent to:
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        graph = "foo => m1 & m2"
[runtime]
    [[fam]]
    [[m1,m2]]
        inherit = fam
\end{lstlisting}

To trigger other tasks off families we have to specify whether 
to triggering off {\em all members} starting, succeeding, failing, 
or finishing, or off {\em any} members (doing the same). Legal family
triggers are thus:

\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        graph = """
      # all-member triggers:
    fam:start-all => one
    fam:succeed-all => one
    fam:fail-all => one
    fam:finish-all => one
      # any-member triggers:
    fam:start-any => one
    fam:succeed-any => one
    fam:fail-any => one
    fam:finish-any => one
                """
\end{lstlisting}

Here's how to trigger downstream processing after if one or more family
members succeed, but only after all members have finished (succeeded or
failed):

\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        graph = """
    fam:finish-all & fam:succeed-any => foo
                """
\end{lstlisting}


\subsubsection{Handling Intercycle Dependence At Start-Up}
\label{HandlingIntercycleDependenceAtStartUp}

In suites with intercycle dependence some kind of bootsrapping
process is required to get the suite going initially. In the example 
shown in {\em Intercycle Triggers} (Section~\ref{IntercycleTriggers}),
for instance, in the very first cycle there is no previous instance of 
task A to satisfy B's prerequisites. 
 
\paragraph{Cold-Start Tasks}

A {\em cold-start} task is a special one-off task used to satisfy the
initial previous-cycle dependence of another cotemporal task. In effect, 
the cold-start task masquerades as the previous-cycle trigger of its
associated cycling task. 

A cold-start task may invoke real processing to generate the files that
would normally be produced by the associated cycling task; or it
could be a dummy task that represents some external spinup process that
presumably generates in the same files but which has to be completed
before the suite is started. In the latter case the cold-start task can 
just report itself successfully completed after checking that the 
required files are present.

This kind of relationship can easily be expressed with a conditional
trigger:
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[special tasks]]
        cold-start = ColdFoo
    [[dependencies]]
        [[[0,6,12,18]]]
            graph = "ColdFoo | Bar[T-6] => Foo"
\end{lstlisting}
i.e.\ Foo[T] can trigger off {\em either} Bar[T-6] {\em or} ColdFoo[T].
At start-up ColdFoo will do the job, and thereafter Bar[T-6] will do it.

{\em Cold-start tasks can also be inserted into the suite at run time 
to cold-start just their associated cycling tasks, if a problem of 
some kind prevents continued normal cycling}.

\paragraph{Warm-Starting A Suite}

Cold-start tasks have to be declared as such in the suite.rc ``special
tasks'' section so that cylc knows they are one-off (non-spawning) tasks,
but also because they play a critical role in suite warm-starts. 
A suite that has previously been running and then shut down 
can be warm-started at a particular cycle time, an alternative to {\em
restarting} from a previous state (although restarting is preferred
because a warm start is likely to involve re-running some tasks).
A warm-start assumes the existence of a previous cycle (i.e.\ that any
files from the previous cycle required by the new cycle are in place
already) so cold-start tasks do not need to run {\em but} cylc itself
does not know the details of the previous cycle (it does in a restart,
but not in a warm-start) so it still has to solve the bootstrapping
problem to get the suite started. It does this by starting the suite
with designated cold start tasks in the {\em succeeded} state - in other
words finished cold start tasks stand in for the previous finished cycle,
rather than pretending to be a running previous cycle as they do in a
cold-start.

\subsubsection{Model Restart Dependencies}
\label{ModelRestartDependencies}

Warm cycled forecast models generate {\em restart files}, e.g.\ model
background fields, that are required to initialize the next forecast
(this is essentially the definition of ``warm cycling''). In fact
restart files will often be written for a whole series of subsequent
cycles in case the next cycle (or the next and the next-next, and so on)
cycle has to be omitted: 
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[special tasks]]
        sequential = A
    [[dependencies]]
        [[[0,6,12,18]]]
            # Model A cold-start and restart dependencies:
            graph = "ColdA | A[T-6] | A[T-12] | A[T-18] | A[T-24] => A"
\end{lstlisting}
In other words, task A can trigger off a cotemporal cold-start task,
{\em or} off its own previous instance, {\em or} off the instance before
that, and so on.
Restart dependencies are unusual because although A {\em could}
trigger off A[T-12] we don't actually want it to do so unless A[T-6]
fails and can't be fixed. {\em This is why Task A, above, is declared to be
`sequential'}.\footnote{A warm cycling model that only writes out
one set of restart files, for the very next cycle, does not need to be
declared sequential because this early triggering problem cannot arise.}
Sequential tasks do not spawn a successor until they have
succeeded (by default, tasks spawn as soon as they start running in
order to get maximum functional parallelism in a suite) which
means that A[T+6] will not be waiting around to trigger off an older
predecessor while A[T] is still running. If A[T] fails though, the
operator can force it, on removal, to spawn A[T+6], whose restart
dependencies will then automatically be satisfied by the older instance,
A[T-6]. 

Forcing a model to run sequentially means, of course, that its restart
dependencies cannot be violated anyway, so we might just ignore them.
This is certainly an option, but it should be noted that there are some 
benefits to having your suite reflect all of the real dependencies
between the tasks that it is managing, particularly for complex
multi-model operational suites in which the suite operator might not be
an expert on the models. Consider such a suite in which a failure in a
driving model (e.g.\ weather) precludes running one or more 
cycles of the downstream models (sea state, storm surge, river flow,
\dots). If the real restart dependencies of each model are known to the
suite, the operator can just do a recursive purge to remove the subtree
of all tasks that can never run due to the failure, and then cold-start
the failed driving model after a gap (skipping as few cycles as possible
until the new cold-start input data are available). After
that the downstream models will kick off automatically so long as the 
gap is spanned by their respective restart files, because their
restart dependencies will automatically be satisfied by the older
pre-gap instances in the suite. Managing this kind of scenario manually
in a complex suite can be quite difficult.

Finally, if a warm cycled model is declared to have explicit restart
outputs, and is not declared to be sequential, and you define appropriate 
labeled restart outputs which {\em must contain the word `restart'},
then the task will spawn as soon its last restart output is
completed so that successives instances of the task will be able to
overlap (i.e.\ run in parallel) if the opportunity arises. Whether or
not this is worth the effort depends on your needs.
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[special tasks]]
        explicit restart outputs = A
    [[dependencies]]
        [[[0,6,12,18]]]
            graph = "ColdA | A[T-18]:res18 | A[T-12]:res12| A[T-6]:res6 => A"
[runtime]
    [[A]]
        [[[outputs]]]
            r6  = restart files completed for [T+6]
            r12 = restart files completed for [T+12]
            r18 = restart files completed for [T+18]
\end{lstlisting}


\subsection{Runtime - Task Configuration}
\label{NIORP}

The \lstinline=[runtime]= section of a suite definition configures what
to execute (and where and how to execute it) when each task is ready to
run, in a {\em multiple inheritance hierarchy} of {\em
namespaces} culminating in individual tasks. This allows all common
configuration detail to be factored out and defined in one place.

Any namespace can configure any or all of the items defined in the
{\em Suite.rc Reference}, Appendix~\ref{SuiteRCReference}.

Namespaces that do not explicitly inherit from others automatically
inherit from the {\em root} namespace (below).

Nested namespaces define {\em task families} that can be used in the
graph as convenient shorthand for triggering all member tasks at once,
or for triggering other tasks off all members at once - see {\em Family
Triggers}, Section~\ref{FamilyTriggers}.  Nested namespaces can be
progressively expanded and collapsed in the dependency graph viewer, and
in the gcylc graph and tree views. Only the first parent of each
namespace (as for single-inheritance) is used for suite visualization
purposes.

\subsubsection{Namespace Names}

Namespace names may contain letters, digits, underscores, and hyphens.
%They may not contain colons, which would preclude use of suite 
%suite names in shell \lstinline=$PATH= variables. The 
%`.' character is the suite registration hierarchy delimiter (which
%separates suite groups and names, e.g.\
%my\_suites.test.foo). 

Note that {\em task names need not be hardwired into task
implementations} because task and suite identity can be extracted
portably from the task execution environment supplied by cylc 
(Section~\ref{TaskExecutionEnvironment}) - then to rename a task you  
can just change its name in the suite definition.

\subsubsection{Root - Runtime Defaults}

The root namespace, at the base of the inheritance hierarchy,
provides default configuration for all tasks in the suite. 
Most root items are unset by default, but some have default values
sufficient to allow test suites to be defined by dependency graph alone.
The {\em command scripting} item, for example, defaults to code that
prints a message then sleeps for between 1 and 15 seconds and
exits. Default values are documented with each item in
Appendix~\ref{SuiteRCReference}.  You can override the defaults or
provide your own defaults by explicitly configuring the root namespace. 
\subsubsection{Defining Multiple Namespaces At Once}
\label{MultiTaskDef}

If a namespace section heading is a comma-separated list of names
then the subsequent configuration applies to each list member. 
Particular tasks can be singled out at run time using the
\lstinline=$CYLC_TASK_NAME= variable.

As an example, consider a suite containing an ensemble of closely
related tasks that each invokes the same script but with a unique
argument that identifies the calling task name:

\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[ensemble]]
        command scripting = "run-model.sh $CYLC_TASK_NAME"
    [[m1, m2, m3]]
        inherit = ensemble
\end{lstlisting}

For large ensembles Jinja2 template processing can be used to
automatically generate the member names and associated dependencies
(see Section~\ref{Jinja2}).

\subsubsection{Runtime Inheritance - Single}

The following listing of the {\em inherit.single.one} example suite
illustrates basic runtime inheritance with single parents.

\lstset{language=suiterc}
\lstinputlisting{../examples/inherit/single/one/suite.rc}
\lstset{language=transcript}

\subsubsection{Runtime Inheritance - Multiple}

If a namespace inherits from multiple parents the linear order of
precedence (which namespace overrides which) is determined by the
so-called {\em C3 algorithm} used to find the linear {\em method
resolution order} for class hierarchies in Python and several other
object oriented programming languages. The result of this should be 
fairly obvious for typical use of multiple inheritance in cylc suites, 
but for detailed documentation of how the algorithm works refer to the
official Python documentation here: 
\lstinline=http://www.python.org/download/releases/2.3/mro/=.

The {\em inherit.multi.one} example suite, listed here, makes use of
multiple inheritance:

\lstset{language=suiterc}
\lstinputlisting{../examples/inherit/multi/one/suite.rc}
\lstset{language=transcript}

\lstinline=cylc get-config= provides an easy way to check the result of
inheritance in a suite. You can extract specific items, e.g.:
\begin{lstlisting}
% cylc get-config --item '[runtime][var_p2]command scripting' inherit.multi.one
echo ``RUN: run-var.sh''
\end{lstlisting}
or use the \lstinline=--sparse= option to print entire namespaces 
without obscuring the result with the dense runtime structure obtained 
from the root namespace: 
\begin{lstlisting}
% cylc get-config --sparse --item '[runtime]ops_s1' inherit.multi.one
command scripting = echo ``RUN: run-ops.sh''
inherit = ['OPS', 'SERIAL']
[directives]
   job_type = serial
\end{lstlisting}

\paragraph{Suite Visualization And Multiple Inheritance}

The first parent inherited by a namespace is also used as the
collapsible family group when visualizing the suite. If this is not what
you want, you can demote the first parent for visualization purposes,
without affecting the order of inheritance of runtime properties:
\begin{lstlisting}
[runtime]
    [[bar]]
        # ...
    [[foo]]
        # inherit properties from bar, but stay under root for visualization:
        inherit = None, bar
\end{lstlisting}


\subsubsection{How Runtime Inheritance Works}

The linear precedence order of ancestors is computed for each namespace
using the C3 algorithm. Then any runtime items that are explicitly
configured in the suite definition are ``inherited'' up the linearized
hierachy for each task, starting at the root namespace: if a particular
item is defined at multiple levels in the hiearchy, the level nearest
the final task namespace takes precedence.  Finally, root namespace
defaults are applied for every item that has not been configured in the 
inheritance process (this is more efficient than carrying the full dense
namespace structure through from root from the beginning).

\subsubsection{Task Execution Environment}
\label{TaskExecutionEnvironment}

The task execution environment contains suite and task identity
variables provided by cylc, and user-defined environment variables.
The environment is explicitly exported (by the task job script) prior to
executing task command scripting (see {\em Task Job Submission},
Section~\ref{TaskJobSubmission}).  

Suite and task identity are exported first, so that user-defined
variables can refer to them. Order of definition is preserved throughout 
so that variable assignment expressions can safely refer to previously 
defined variables.

Additionally, access to cylc itself is configured prior to the user-defined 
environment, so that variable assignment expressions can make use of 
cylc utility commands: 
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[foo]]
        [[[environment]]]
            REFERENCE_TIME = $( cylc util cycletime --offset-hours=6 )
\end{lstlisting}

\paragraph{User Environment Variables}

A task's user-defined environment results from its inherited
\lstinline=[[[environment]]]= sections:
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[root]]
        [[[environment]]]
            COLOR = red
            SHAPE = circle
    [[foo]]
        [[[environment]]]
            COLOR = blue  # root override
            TEXTURE = rough # new variable
\end{lstlisting}         
This results in a task {\em foo} with
\lstinline@SHAPE=circle@,
\lstinline@COLOR=blue@, and
\lstinline@TEXTURE=rough@ in its environment.

\paragraph{Overriding Environment Variables}

When you override inherited namespace items the original parent
item definition is {\em replaced} by the new definition. This applies to
all items including those in the environment sub-sections which,
strictly speaking, are not ``environment variables'' until they are
written, post inheritance processing, to the task job script that
executes the associated task. Consequently, if you override an
environment variable you cannot also access the original parent value:
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[foo]]
        [[[environment]]]
            COLOR = red
    [[bar]]
        inherit = foo
        [[[environment]]]
            tmp = $COLOR        # !! ERROR: $COLOR is undefined here
            COLOR = dark-$tmp   # !! as this overrides COLOR in foo.
\end{lstlisting}
The compressed variant of this, \lstinline@COLOR = dark-$COLOR@, is 
also in error for the same reason.  To achieve the desired result you
must use a different name for the parent variable:
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[foo]]
        [[[environment]]]
            FOO_COLOR = red
    [[bar]]
        inherit = foo
        [[[environment]]]
            COLOR = dark-$FOO_COLOR  # OK
\end{lstlisting}

\paragraph{Suite And Task Identity Variables}

The task identity variables provided to tasks by cylc are:
\lstset{language=bash}
\begin{lstlisting}
$CYLC_TASK_ID                    # X.2011051118 (e.g.)
$CYLC_TASK_NAME                  # X
$CYLC_TASK_CYCLE_TIME            # 2011051118
$CYLC_TASK_LOG_ROOT              # ~/cylc-run/foo.bar.baz/log/job/X.2011051118.1
$CYLC_TASK_NAMESPACE_HIERARCHY   # "root postproc X" (e.g.)
$CYLC_TASK_TRY_NUMBER            # increments with automatic retry-on-fail
$CYLC_TASK_WORK_DIR              # task work directory (see below)
$CYLC_SUITE_SHARE_DIR            # suite (or task!) shared directory (see below)
$CYLC_TASK_IS_COLDSTART          # 'True' for cold-start tasks, else 'False'
\end{lstlisting}
And the suite identity variables are:
\begin{lstlisting}
$CYLC_SUITE_DEF_PATH   # $HOME/mysuites/baz (e.g.)
$CYLC_SUITE_NAME       # foo.bar.baz (e.g.)
$CYLC_SUITE_REG_PATH   # name translate to path: foo/bar/baz
$CYLC_SUITE_HOST       # orca.niwa.co.nz (e.g.) 
$CYLC_SUITE_PORT       # 7766 (e.g.)
$CYLC_SUITE_OWNER      # oliverh (e.g.)
\end{lstlisting}
Some of these variables are also used by cylc task messaging commands in
order to target the right task proxy object in the right suite.

\paragraph{Suite Share And Task Work Directories}

A {\em suite share directory} is created automatically for use as a file
exchange area for tasks on same task host. It can be accessed via
\lstinline=$CYLC_SUITE_SHARE_DIR= and its location can be set in the
cylc site/user config files. 

A {\em task work directory} is also created automatically for each task, 
and can be accessed via the \lstinline=$CYLC_TASK_WORK_DIR= variable.
Task command scripting is executed from within the work directory (i.e.\
it is the task's {\em current working directory}). For non-detaching
tasks the work directory is automatically removed again if it is empty
when the task finishes. The main work directory location is set in the
cylc site/user config files, but the lowest-level sub-directory, which
name defaults to the task ID to give each task a unique workspace, can
be overridden under \lstinline=[runtime]= in suite definitions. This
enables groups of tasks that read and write files from their current
working directories to be given common work directories as file share
spaces.

\paragraph{Other Cylc-Defined Environment Variables}

Initial and final cycle times, if supplied via the suite.rc file or the
command line, are passed to task execution environments as:
\begin{lstlisting}
$CYLC_SUITE_INITIAL_CYCLE_TIME 
$CYLC_SUITE_FINAL_CYCLE_TIME 
\end{lstlisting}
Tasks can use these to determine whether or not they are running
in the first or final cycles.

\lstset{language=transcript}


\paragraph{Environment Variable Evaluation}

Variables in the task execution environment are not evaluated in the
shell in which the suite is running prior to submitting the task. They
are written in unevaluated form to the job script that is submitted by
cylc to run the task (Section~\ref{JobScripts}) and are therefore
evaluated when the task begins executing under the task owner account 
on the task host. Thus \lstinline=$HOME=, for instance, evaluates at
run time to the home directory of task owner on the task host. 

\subsubsection{How Tasks Get Access To The Suite Directory}

Tasks can use \lstinline=$CYLC_SUITE_DEF_PATH= to access suite files on
the task host, and the suite bin directory is automatically added
\lstinline=$PATH=.  If a remote suite definition directory is not
specified the local (suite host) path will be assumed with the local
home directory, if present, swapped for literal \lstinline=$HOME= for
evaluation on the task host.

\subsubsection{Remote Task Hosting}
\label{RunningTasksOnARemoteHost}

If a task declares an owner other than the suite owner and/or 
a host other than the suite host, cylc will use passwordless ssh to
execute the task on the \lstinline=owner@host= account by the configured
job submission method,
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[foo]]
        [[[remote]]]
            host = orca.niwa.co.nz
            owner = bob
        [[[job submission]]]
            method = pbs
\end{lstlisting}
\lstset{language=transcript}
For this to work,
\begin{myitemize}
    \item passwordless ssh must be configured between the suite and task
    host accounts.
    \item cylc must be installed on task hosts so that remote tasks can
    use cylc messaging and poll or kill commands.
    \begin{myitemize}
        \item Pyro and the suite passphrase are needed on tasks hosts,
            unless the ssh or polling task communication methods are
            used. Other cylc software dependencies such as graphviz and
        Jinja2 are not needed on task hosts.
    \end{myitemize}
    \item the suite definition directory, or some fraction of its
        content, can be installed on the task host, if needed.
\end{myitemize}

To learn how to give remote tasks access to cylc, see
Section~\ref{HowTasksGetAccessToCylc}.

Tasks running on the suite host under another user account are treated as
remote tasks.

Remote hosting, like all namespace settings, can be declared globally in
the root namespace, or per family, or for individual tasks.

\paragraph{Dynamic Host Selection}

Instead of hardwiring host names into the suite definition you can
specify a shell command that prints a hostname, or an environment
variable that holds a hostname, as the value of the host config item.
See Section~\ref{DynamicHostSelection}.

\paragraph{Remote Task Log Directories}

Task stdout and stderr streams are written to log files in a
suite-specific sub-directory of the {\em suite run directory}, as
explained in Section~\ref{WhitherStdoutAndStderr}. For remote tasks 
the same directory is used, but {\em on the task host}. 
Remote task log directories, like local ones, are created on the fly, if
necessary, during job submission.
 
\subsection{Visualization}
\label{viso}

The visualization section of a suite definition is used to configure
suite graphing, principally graph node (task) and edge (dependency
arrow) style attributes. Tasks can be grouped for the purpose of
applying common style attributes. See the suite.rc reference
(Appendix~\ref{SuiteRCReference}) for details.

\subsubsection{Collapsible Families In Suite Graphs}

\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[visualization]
    collapsed families = family1, family2 
\end{lstlisting}
\lstset{language=transcript}

Nested families from the namespace inheritance hierarchy, even if 
they are not used as family triggers in the graph, can be expanded or
collapsed in suite graphs and the gcylc suite views.

In the graph view ungraphed tasks, which includes the members of
collapsed families, are automatically plotted as rectangular nodes to
the right of the main graph if they are doing anything interesting
(submitted, running, or failed).

Note that family relationships can be defined purely for visualization
purposes - you can group tasks at root level in the inheritance
hierarchy prior to defining real properties at higher levels.

Figure~\ref{fig-namespaces} illustrates successive expansion of nested task
families in the {\em namespaces} example suite.

\begin{figure}
\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/png/orig/inherit-2.png}
    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/png/orig/inherit-3.png}
    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/png/orig/inherit-4.png}
    \end{center}
\end{minipage}

\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/png/orig/inherit-5.png}
    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/png/orig/inherit-6.png}
    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/png/orig/inherit-7.png}
    \end{center}
\end{minipage}
\caption[{\em namespaces} example suite graphs]{\scriptsize Graphs of the {\em
namespaces} example suite showing various states of expansion of the
nested namespace family hierarchy, from all families collapsed (top
left) through to all expanded (bottom right). This can also be done by 
right-clicking on tasks in the gcylc graph view.}
\label{fig-namespaces}
\end{figure} 


\subsection{Jinja2 Suite Templates}
\label{Jinja2}

Support for the Jinja2 template processor adds general variables,
mathematical expressions, loop control structures, and conditional
expressions to suite.rc files - which are automatically preprocessed to
generate the final suite definition seen by cylc.

The need for Jinja2 processing must be declared with a hash-bang
comment as the first line of the suite.rc file:
\begin{lstlisting}
#!Jinja2
# ...
\end{lstlisting}

Potential uses for this include automatic generation of repeated groups
of similar tasks and dependencies, and inclusion or exclusion of entire
suite sections according to the value of a single flag.  Consider a
large complicated operational suite and several related parallel test
suites with slightly different task content and structure (the parallel
suites, for instance, might take certain large input files from the
operation or the archive rather than downloading them again) - these can
now be maintained as a single master suite definition that reconfigures
itself according to the value of a flag variable indicating the intended use.

Template processing is the first thing done on parsing a suite
definition so Jinja2 expressions can appear anywhere in the file (inside
strings and namespace headings, for example).

Jinja2 is well documented at \url{http://jinja.pocoo.org/docs}, so here
we just provide an example suite that uses it. The meaning of the 
embedded Jinja2 code should be reasonably self-evident to anyone familiar
with standard programming techniques. 

\begin{figure}
    \begin{center}
        \includegraphics[width=10cm]{graphics/png/orig/jinja2-ensemble-graph.png} 
    \end{center}
    \caption[The Jinja2 ensemble example suite graph.]{\scriptsize
    The Jinja2 ensemble example suite graph.}
    \label{fig-jinja2-ensemble} 
\end{figure} 

The \lstinline=jinja2.ensemble= example, graphed in
Figure~\ref{fig-jinja2-ensemble}, shows an ensemble of similar tasks
generated using Jinja2:
\lstset{language=suiterc}
\begin{lstlisting}
#!jinja2
{% set N_MEMBERS = 5 %}
[scheduling]
    [[dependencies]]
        graph = """{# generate ensemble dependencies #}
            {% for I in range( 0, N_MEMBERS ) %}
               foo => mem_{{ I }} => post_{{ I }} => bar
            {% endfor %}"""
\end{lstlisting}
Here is the generated suite definition, after Jinja2 processing:
\lstset{language=suiterc}
\begin{lstlisting}
#!jinja2
[scheduling]
    [[dependencies]]
        graph = """
          foo => mem_0 => post_0 => bar
          foo => mem_1 => post_1 => bar
          foo => mem_2 => post_2 => bar
          foo => mem_3 => post_3 => bar
          foo => mem_4 => post_4 => bar
                """ 
\end{lstlisting}

And finally, the \lstinline=jinja2.cities= example uses variables,
includes or excludes special cleanup tasks according to the value of a
logical flag, and it automatically generates all dependencies and family
relationships for a group of tasks that is repeated for each city in the
suite. To add a new city and associated tasks and dependencies simply
add the city name to list at the top of the file. The suite is graphed,
with the New York City task family expanded, in
Figure~\ref{fig-jinja2-cities}.

\lstset{language=suiterc}
\lstinputlisting{../examples/jinja2/cities/suite.rc}
\lstset{language=transcript}

\begin{figure}
    \begin{center}
        \includegraphics[width=16cm]{graphics/png/orig/jinja2-suite-graph.png} 
    \end{center}
    \caption[Jinja2 cities example suite graph.]{\scriptsize
    The Jinja2 cities example suite graph, with the 
    New York City task family expanded.}
    \label{fig-jinja2-cities} 
\end{figure} 

\subsubsection{Accessing Environment Variables With Jinja2}

This functionality is not provided by Jinja2 by default, but cylc
automatically imports the user environment to the template in a 
dictionary structure called {\em environ}. A usage example:
\begin{lstlisting}
#!Jinja2
#...
[runtime]
    [[root]]
        [[[environment]]]
            SUITE_OWNER_HOME_DIR_ON_SUITE_HOST = {{environ['HOME']}}
\end{lstlisting}
This example is emphasizes that {\em the environment is read on the suite
host at the time the suite definition is parsed} - it is not, for
instance, read at task run time on the task host.

\subsubsection{Custom Jinja2 Filters}

Jinja2 variable values can be modified by ``filters'', using pipe
notation. For example, the built-in \lstinline=trim= filter strips
leading and trailing white space from a string:
\lstset{language=suiterc}
\begin{lstlisting}
{% set MyString = "   dog   " %}
{{ MyString | trim() }}  # "dog"
\end{lstlisting}
(See official Jinja2 documentation for available built-in filters.) 

Cylc also supports custom Jinja2 filters. A custom filter is a 
single Python function in a source file with the same name as the
function (plus ``.py'' extension) and stored in one of the following
locations:
\begin{myitemize}
    \item \lstinline=$CYLC_DIR/lib/Jinja2Filters/=
    \item \lstinline=[suite definition directory]/Jinja2Filters/=
    \item \lstinline=$HOME/.cylc/Jinja2Filters/=
\end{myitemize}

In the filter function argument list, the first argument is the variable
value to be ``filtered'', and subsequent arguments can be whatever is
needed. Currently there is one custom filter called ``pad'' in the
central cylc Jinja2 filter directory, for padding string values to some 
constant length with a fill character - useful for generating task names 
and related values in ensemble suites:

\lstset{language=suiterc}
\begin{lstlisting}
{% for i in range(0,100) %}  # 0, 1, ..., 99
    {% set j = i | pad(2,'0') %}
    A_{{j}}          # A_00, A_01, ..., A_99
{% endfor %}
\end{lstlisting}

\subsubsection{Associative Arrays In Jinja2}

Associative arrays ({\em dicts} in Python) can be very useful. Here's an
example, from \lstinline=$CYLC_DIR/examples/jinja2/dict=: 

\lstset{language=suiterc}
\begin{lstlisting}
#!Jinja2
{% set obs_types = ['airs', 'iasi'] %}
{% set resource = { 'airs':'ncpus=9', 'iasi':'ncpus=20' } %}

[scheduling]
    [[dependencies]]
        graph = "obs"
[runtime]
    [[obs]]
        [[[job submission]]]
            method = pbs
    {% for i in obs_types %}
    [[ {{i}} ]]
        inherit = obs
        [[[directives]]]
             -I = {{ resource[i] }}
     {% endfor %}
 \end{lstlisting}

Here's the result:
\lstset{language=transcript}
\begin{lstlisting}
% cylc get-config -i [runtime][airs]directives SUITE 
-I = ncpus=9
\end{lstlisting}

\subsubsection{Jinja2 Default Values And Template Inputs}

The values of Jinja2 variables can be passed in from the cylc command
line rather than hardwired in the suite definition. Here's an example,
from \lstinline=$CYLC_DIR/examples/jinja2/defaults=:

\lstset{language=suiterc}
\begin{lstlisting}
#!Jinja2

title = "Jinja2 example: use of defaults and external input"

description = """
The template variable FIRST_TASK must be given on the cylc command line
using --set or --set-file=FILE; two other variables, LAST_TASK and
N_MEMBERS can be set similarly, but if not they have default values.""" 

{% set LAST_TASK = LAST_TASK | default( 'baz' ) %}
{% set N_MEMBERS = N_MEMBERS | default( 3 ) | int %}

{# input of FIRST_TASK is required - no default #}

[scheduling]
    initial cycle time = 2010080800
    final cycle time   = 2010081600
    [[dependencies]]
        [[[0]]]
            graph = """{{ FIRST_TASK }} => ens
                 ens:succeed-all => {{ LAST_TASK }}"""
[runtime]
    [[ens]]
{% for I in range( 0, N_MEMBERS ) %}
    [[ mem_{{ I }} ]]
        inherit = ens
{% endfor %}
\end{lstlisting}

Here's the result:

\lstset{language=transcript}
\begin{lstlisting}
% cylc list SUITE
Jinja2 Template Error
'FIRST_TASK' is undefined
cylc-list foo  failed:  1

% cylc list --set FIRST_TASK=bob foo
bob
baz
mem_2
mem_1
mem_0

% cylc list --set FIRST_TASK=bob --set LAST_TASK=alice foo
bob
alice
mem_2
mem_1
mem_0

list --set FIRST_TASK=bob --set N_MEMBERS=10 foo
mem_9
mem_8
mem_7
mem_6
mem_5
mem_4
mem_3
mem_2
mem_1
mem_0
baz
bob
\end{lstlisting}

\lstset{language=suiterc}
Note also that 
\lstinline=cylc view --set FIRST_TASK=bob --jinja2 SUITE= will show the
suite with the Jinja2 variables as set.

{\em Warning:} suites started with template variables set on the command
line do not currently {\em restart} with the same settings - you have to 
set them again on the \lstinline=cylc restart= command line.

\subsection{Special Placeholder Variables}
\label{SPHV}

Several special variables are used as placeholders in cylc suite definitions:

\begin{myitemize}
    \item \lstinline=[T]= and \lstinline=[T+n]= \newline
        Where \lstinline=n= is an integer cycle time offset, the unit 
        of which (e.g. hours, days, months, or years) is determined by
        the cycling module in use. This is replaced, in internal task
        message strings under the runtime outputs config section, by the
        actual offset cycle time of the task. The syntax is the same as 
        for cycle time offsets in the suite graph.

    \item \lstinline=<ASYNCID>=\newline
        This is replaced with the actual ``asynchronous ID'' (e.g.
        satellite pass ID) for repeating asynchronous tasks.
\end{myitemize}

To use proper variables (c.f.\ programming languages) in suite
definitions, see the Jinja2 template processor (Section~\ref{Jinja2}).

\subsection{Omitting Tasks At Runtime}

It is sometimes convenient to omit certain tasks from the suite at
runtime without actually deleting their definitions from the suite.

Defining [runtime] properties for tasks that do not appear in the suite
graph results in verbose-mode validation warnings that the tasks are
disabled. They cannot be used because the suite graph is what defines
their dependencies and valid cycle times.  Nevertheless, it is legal to
leave these orphaned runtime sections in the suite definition because it
allows you to temporarily remove tasks from the suite by simply
commenting them out of the graph.

To omit a task from the suite at runtime but still leave it fully
defined and available for use (by insertion or \lstinline=cylc submit=)
use one or both of [scheduling][[special task]] lists, {\em include at
start-up} or {\em exclude at start-up} (documented in 
Sections~\ref{IASU} and~\ref{EASU}). Then the graph still defines the
validity of the tasks and their dependencies, but they are not actually
inserted into the suite at start-up. Other tasks that depend on the
omitted ones, if any, will have to wait on their insertion at a later
time or otherwise be triggered manually.

Finally, with Jinja2 (Section~\ref{Jinja2}) you can radically alter 
suite structure by including or excluding tasks from the [scheduling]
and [runtime] sections according to the value of a single logical flag
defined at the top of the suite.  

\subsection{Naked Dummy Tasks And Strict Validation}

A {\em naked dummy task} appears in the suite graph but has no
explicit runtime configuration section. Such tasks automatically
inherit the default ``dummy task'' configuration from the root
namespace. This is very useful because it allows functional suites to
be mocked up quickly for test and demonstration purposes by simply
defining the graph. It is somewhat dangerous, however, because there
is no way to distinguish an intentional naked dummy task from one
generated by typographic error: misspelling a task name in the graph
results in a new naked dummy task replacing the intended task in the
affected trigger expression; and misspelling a task name in a runtime
section heading results in the intended task becoming a dummy task
itself (by divorcing it from its intended runtime config section). 

To avoid this problem any dummy task used in a real suite should not be
naked - i.e.\ it should have an explicit entry in under the runtime
section of the suite definition, even if the section is empty. This
results in exactly the same dummy task behaviour, via implicit
inheritance from root, but it allows use of 
\lstinline=cylc validate --strict= 
to catch errors in task names by failing the suite if any naked dummy
tasks are detected. 

\section{Task Implementation}
\label{TaskImplementation}

Existing tasks (models, scripts, etc.) can be used by cylc without any
modification, with the following few exceptions:
\begin{myitemize}
    \item tasks that do not return error status on error
    \item tasks with internal outputs that others need to trigger off
    \item tasks that spawn internal processes and then detach and exit early
\end{myitemize}

\subsection{Inlined Tasks}

Simple tasks can be entirely implemented within the suite.rc file - 
task {\em command scripting} can be a multi-line string.

\subsection{Returning Proper Error Status}

Tasks should abort with non-zero exit status if a fatal error occurs 
(this is just standard coding practice anyway). This allows cylc's task
job scripts to automatically trap errors and send a 
\lstinline=cylc task failed= message back to the suite. The shell
\lstinline=set -e= option can be used in lieu of explicit error
checks for every command:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
set -e  # abort on error
mkdir /illegal/dir  # this will abort the script with error status
\end{lstlisting}

\subsection{Reporting Internal Outputs}
\label{RIOC}

If a task has {\em internal outputs} that others need to trigger off
then it must report completion of those outputs at the appropriate time.
Output messages must be unique within the suite or else downstream tasks
will trigger off whichever task happens to send the message first; they
must exactly match the corresponding outputs registered for the task
in the suite definition; and for cycling tasks they must contain the
cycle time in order to distinguish between the same outputs of the same
task at other cycle times.

The ``outputs'' example is a self-contained suite that illustrates this:
\lstset{language=suiterc}
\lstinputlisting{../examples/outputs/suite.rc}
Note the use of \lstinline=[T]= as a placeholder for cycle time in
messages registered under \lstinline=[[[outputs]]]= these strings are
held inside cylc for comparison with incoming task messages; they are
never interpreted by the shell and may not contain shell environment
variables. The actual messaging calls made by running tasks, on the
other hand, can make use of variables in the task runtime environment.

\subsection{Other Task Messages}

General (non-output) messages can also be sent to report progress,
warnings, and so on, e.g.:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
# a warning message (this will be logged by the suite):
cylc task message -p WARNING "oops, something's fishy here"
# information (this will also be logged by the suite):
cylc task message "Hello from task foo"
\end{lstlisting}

Explanatory messages can be sent before aborting on error:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
set -e  # abort on error
if ! mkdir /illegal/dir; then
    # (use inline error checking to avoid triggering the above 'set -e')
    cylc task message -p CRITICAL "Failed to create directory /illegal/dir"
    exit 1 # now abort non-zero exit status to trigger the task failed message
fi
\end{lstlisting}
Or equivalently, with different syntax:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
set -e
mkdir /illegal/dir || {  # inline error checking using OR operator
    cylc task message -p CRITICAL "Failed to create directory /illegal/dir"
    exit 1
}
\end{lstlisting}
But not this:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
set -e
mkdir /illegal/dir  # aborted via 'set -e'
if [[ $? != 0 ]]; then  # so this will never be reached.
    cylc task message -p CRITICAL "Failed to create directory /illegal/dir"
    exit 1
fi
\end{lstlisting}
If critical errors are not reported in this way task failures will still
be detected and logged by cylc, but you may have to examine task logs to
determine what the problem was.

\subsection{Detaching Tasks}
\label{DetachingTasks}

If a task spawns another job internally and then detaches and exits
without seeing the spawned process through, you must arrange for the
detached process to send its own completion messages, because the
cylc-generated job script cannot know when it is finished. 

First check that you can't ``reconnect'' the detaching process. If
it is a background shell process, for instance, just run it in the
foreground instead. For loadleveler jobs the \lstinline=-s= option
prevents \lstinline=llsubmit= from returning until the job has
completed. For Sun Grid Engine, \lstinline=qsub -sync yes= has the same
effect.  Section~\ref{CommandTemplate} shows how to override job
submission command template to achieve this.

If the detaching process cannot be reconnected, disable cylc's automatic
completion messaging:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[foo]]
        manual completion = False # this is a detching task
\end{lstlisting}

The cylc messaging commands are called like this:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
# ...
if $SUCCESS; then
    # release my task lock and report success
    cylc task succeeded
    exit 0
else
    # release my task lock and report failed
    cylc task failed "Input file X not found"
    exit 1
fi
\end{lstlisting}
They read environment variables that identify the calling task and the
target suite, so the task execution environment must be propagated to
the deatched process.  

One way to handle this is to write a {\em task wrapper} that modifies a
copy of the detaching native job scripts, on the fly, to insert
completion messaging in the appropriate places. An advantage of this
method is that you don't need to permanently modify the model or its
associated native scripting for cylc. Another is that you can configure
the native job setup for a single test case (running it without cylc)
and then have your custom wrapper modify the standalone test case on the
fly with suite, task, and cycle-specific parameters as required.

To make this easier, for tasks that declare manual completion 
messaging cylc makes non user-defined environment scripting
available in a variable \lstinline=$CYLC_SUITE_ENVIRONMENT=,
the value of which can be inserted at the appropriate point in the
task scripts (just prior to calling the cylc messaging
commands as above).\footnote{Note that \lstinline=$CYLC_SUITE_ENVIRONMENT= is 
a string containing embedded newline characters and it has
to be handled accordingly. In the bash shell, for instance, it should
be echoed in quotes to avoid concatenation to a single line.}

\subsubsection{Detaching Tasks And Polling}

Another reason to avoid detaching tasks if possible is that they cannot
be polled or killed because there is no way for cylc to determine the
job ID of the detached process. Attempted polling of a detaching task
will just result in cylc logging a warning message.

\subsubsection{A Custom Detaching Task Wrapper Example}

The {\em detaching} example suite contains a script 
\lstinline=model.sh= that runs a pseudo model as follows:
\lstset{language=bash}
\lstinputlisting{../examples/detaching/native/model.sh}
this is in turn executed by a script \lstinline=run-model.sh= that
detaches immediately after job submission (i.e.\ it exits before the
model executable actually runs):
\lstinputlisting{../examples/detaching/native/run-model.sh}
{\em Note that your {\bf at} scheduler daemon must be up 
if you want to test this suite.}

Here's a cylc suite to run this unruly model:
\lstset{language=suiterc}
\lstinputlisting{../examples/detaching/suite.rc}
\lstset{language=bash}
The suite invokes the task by means of the custom wrapper 
\lstinline=model-wrapper.sh= which modifies, on the fly, 
a temporary copy of the model's native job scripts as described above:
\lstinputlisting{../examples/detaching/bin/model-wrapper.sh}
\lstset{language=transcript}
If you run this suite, or submit the model task alone with
\lstinline=cylc submit=, you'll find that the usual job submission 
log files for task stdout and stderr end before the task is finished. 
To see the ``model'' output and the final task completion message
(success or failure), examine the log files generated by the 
job submitted internally to the {\em at} scheduler (their 
location is determined by the \lstinline=$PREFIX= variable in the
suite.rc file). 

It should not be difficult to adapt this example to real tasks
with detaching internal job submission.  You will probably also need to
replace other parameters, such as model input and output filenames, with
suite- and cycle-appropriate values, but exactly the same technique can
be used: identify which job script needs to be modified and use text
processing tools (such as the single line {\em perl} search-and-replace 
expressions above) to do the job.

%\pagebreak

\section{Task Job Submission, Poll and Kill}
\label{TaskJobSubmission}

{\em Task Implementation} (Section~\ref{TaskImplementation}) describes
what requirements a command, script, or program, must fulfill in order
to function as a cylc task. This section explains how tasks are submitted 
by cylc when they are ready to run, and how to define new task job
submission methods.

\subsection{Job Poll And Kill Support}

For most job submission methods cylc now supports polling for real task
status, and job kill, from the gcylc GUI and command line
(\lstinline=cylc poll= and \lstinline=cylc kill=).
In addition to on-demand polling, submitted and running tasks are polled
automatically on suite restart (Section~\ref{RestartingSuites})
and on job submission and execution timeouts, and one-way polling can be
used as regular health check for submitted tasks, and to track tasks on
hosts that do not allow return routing for task messaging
(Section~\ref{RunningSuites}).

\subsubsection{Exceptions}

Task poll and kill support has not yet been added to the {\em SGE} and
{\em slurm} job submission methods. It will be added in an upcoming
release.

\subsection{Task Job Scripts}
\label{JobScripts}

When a task is ready to run cylc generates a temporary {\em task job
script} to configure the task's execution environment and call its
command scripting. The job script is the embodiment of all suite.rc
runtime settings for the task.  It is submitted to run by the {\em job
submission method} configured for the task. Different tasks can have
different job submission methods. Like other runtime properties,
you can set a suite default job submission method and override it for
specific tasks or families:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
   [[root]] # suite defaults
        [[[job submission]]]
            method = loadleveler
   [[foo]] # just task foo
        [[[job submission]]]
            method = at 
\end{lstlisting}

The actual command line used to submit the job script is written to
stdout by cylc:
\lstset{language=transcript}
\begin{lstlisting}
% cylc submit --dry-run examples.QuickStart.c Model.2011080506
> JOB SCRIPT: ~/cylc-run/examples.QuickStart.c/log/job/Model.2011080506.1
> THIS IS A DRY RUN. HERE'S HOW I WOULD SUBMIT THE TASK:
~/cylc-run/examples.QuickStart.c/log/job/Model.2011080506.1 </dev/null 
    1> ~/cylc-run/examples.QuickStart.c/log/job/Model.2011080506.1.out 
    2> ~/cylc-run/examples.QuickStart.c/log/job/Model.2011080506.1.err &
\end{lstlisting}
Job scripts can also be generated and printed directly to stdout with the 
\lstinline=cylc jobscript= command.  Take a look at one to see exactly
how cylc wraps and runs your tasks.

\subsection{Supported Job Submission Methods}
\label{AvailableMethods}

Cylc supports a number of commonly used job submission methods, and 
Section~\ref{DefiningNewJobSubmissionMethods} shows how to add 
support for other user-defined job submission methods.

\subsubsection{background}

Runs tasks directly in a background shell.

\subsubsection{at}

Submits tasks to the rudimentary Unix \lstinline=at= scheduler. The
\lstinline=atd= daemon must be running.

\subsubsection{loadleveler}

Submits tasks to loadleveler by the \lstinline=llsubmit= command.
Loadleveler directives can be provided in the suite.rc file: 

\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[__NAME__]]
        [[[directives]]] 
            foo = bar
            baz = qux
\end{lstlisting}
These are written to the top of the task job script like this: 
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
# DIRECTIVES
# @ foo = bar
# @ baz = qux
# @ queue
\end{lstlisting}

\subsubsection{pbs}

Submits tasks to PBS (or Torque) by the \lstinline=qsub= command.  PBS
directives can be provided in the suite.rc file: 
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[__NAME__]]
        [[[directives]]] 
            -q = foo
            -l = 'nodes=1,walltime=00:01:00'
\end{lstlisting}
These are written to the top of the task job script like this: 
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
# DIRECTIVES
#PBS -q foo
#PBS -l nodes=1,walltime=00:01:00
\end{lstlisting}

\subsubsection{sge}

Submits tasks to Sun/Oracle Grid Engine by the \lstinline=qsub= command.
SGE directives can be provided in the suite.rc file: 
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[__NAME__]]
        [[[directives]]] 
            -cwd = ' '
            -q = foo
            -l = 'h_data=1024M,h_rt=24:00:00'
\end{lstlisting}
These are written to the top of the task job script like this: 
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
# DIRECTIVES
#$ -cwd
#$ -q foo
#$ -l h_data=1024M,h_rt=24:00:00
\end{lstlisting}

\subsubsection{slurm}

Submits tasks to Simple Linux Utility for Resource Management by the
\lstinline=sbatch= command.  SLURM directives can be provided in the
suite.rc file (note that since not all SLURM commands have a short form,
cylc requires the long form directives):
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[__NAME__]]
        [[[directives]]]
            --nodes = 5
            --time = 1:00:00
            --account = QXZ5W2
\end{lstlisting}
These are written to the top of the task job script like this:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
#SBATCH --nodes=5
#SBATCH --time=1:00:00
#SBATCH --account=QXZ5W2
\end{lstlisting}

\subsubsection{Default Directives Provided}

For job submission methods that use job file directives (PBS,
Loadlevler, etc.) default directives are provided to set the job name
and stdout and stderr file paths.

\subsubsection{Cylc Quirks (PBS,SGE,...) }

As shown in the example above, multiple entries for the same PBS or SGE
directive option must be comma-separated on the same line, in the
suite.rc file.  Otherwise, repeating the option on another line will
override the previous entry, not add to it.  Also, the right-hand side
must be quoted to hide the comma from the suite.rc parser (commas indicate
list values, whereas directives are treated as singular).

As also shown in the example above, to get a naked option flag such as
\lstinline=-cwd= in SGE you must give a quoted blank space as the
directive value in the suite.rc file.

\subsection{Task stdout And stderr Logs}
\label{WhitherStdoutAndStderr}

When a task is ready to run cylc generates a filename root to be used
for the task job script and log files. The filename containing the task
name, cycle time (or integer tag), and a submit number that increments
if the same task is re-triggered multiple times:
\lstset{language=bash}
\begin{lstlisting}
# task job script:
~/cylc-run/examples.QuickStart.c/log/job/Model.2011080506.1
# task stdout:
~/cylc-run/examples.QuickStart.c/log/job/Model.2011080506.1.out 
# task stderr:
~/cylc-run/examples.QuickStart.c/log/job/Model.2011080506.1.err
\end{lstlisting}

How the stdout and stderr streams are directed into these files depends
on the job submission method. The \lstinline=background= method just uses
appropriate output redirection on the command line, as shown above. The
\lstinline=loadleveler= method writes appropriate directives to the job
script that is submitted to loadleveler.

Cylc obviously has no control over the stdout and stderr output from
tasks that do their own internal output management (e.g.\ tasks 
that submit internal jobs and direct the associated output to other
files). For less internally complex tasks, however, the files referred
to here will be complete task job logs.

\subsection{Overriding The Job Submission Command}
\label{CommandTemplate}

\lstset{language=suiterc}
To change the form of the actual command used to submit a job you do not
need to define a new job submission method; just override the
\lstinline=command template= in the relevant job submission sections of
your suite.rc file:
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[root]]
        [[[job submission]]]
            method = loadleveler
            # Use '-s' to stop llsubmit returning until all job steps have completed:
            command template = llsubmit -s %s 
\end{lstlisting}
As explained in the suite.rc reference (Appendix~\ref{SuiteRCReference}), 
the template's first \%s will be substituted by the job file path and,
where applicable a second and third \%s will be substituted by the paths
to the job stdout and stderr files.

\subsection{Defining New Job Submission Methods}
\label{DefiningNewJobSubmissionMethods}

Defining a new job submission method requires a little Python
programming. You can derive (in the sense of object oriented programming
inheritance) new methods from one of the existing ones, or directly from
cylc's job submission base class,
\lstset{language=transcript}
\begin{lstlisting}
$CYLC_DIR/lib/cylc/job_submission/job_submit.py
\end{lstlisting}
using the existing job submission methods as examples.
\lstset{language=Python}

\subsubsection{An Example}

The following user-defined job submission class, called {\em qsub},
overrides the built-in {\em pbs} class to change the directive 
prefix from \lstinline=#PBS= to \lstinline=#QSUB=:

\begin{lstlisting}
#!/usr/bin/env python

# to import from outside of the cylc source tree:
from cylc.job_submission.pbs import pbs
# OR, from $CYLC_DIR/lib/cylc/job_submission
# from pbs import pbs

class qsub( pbs ):
    """
This is a user-defined job submission method that overrides the '#PBS'
directive prefix of the built-in pbs method.
    """
    def set_directives( self ):
        pbs.set_directives( self )
        # override the '#PBS' directive prefix
        self.directive_prefix = "#QSUB"
\end{lstlisting}

To check that this works correctly save the new source file to
\lstinline=qsub.py= in one of the allowed locations (see just below),
use it in a suite definition:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.rc
# $HOME/test/suite.rc
[scheduling]
    [[dependencies]]
        graph = "a"
[runtime]
    [[root]]
        [[[job submission]]]
            method = qsub
        [[[directives]]]
            -I = bar=baz
            -l = 'nodes=1,walltime=00:01:00'
            -cwd = ' '
\end{lstlisting}
and generate a job script to see the resulting directives:
\lstset{language=bash}
\begin{lstlisting}
$ cylc db reg test $HOME/test
$ cylc jobscript test a | grep QSUB 
#QSUB -e /home/oliverh/cylc-run/pbs/log/job/a.1.1.err
#QSUB -l nodes=1,walltime=00:01:00
#QSUB -o /home/oliverh/cylc-run/pbs/log/job/a.1.1.out
#QSUB -N a.1
#QSUB -I bar=baz
#QSUB -cwd  
\end{lstlisting}

\subsubsection{Where To Put New Job Submission Modules}

You new job submission class code should be saved to a file with 
the same name as the class (plus ``.py'' extension). It can reside 
in any of the following locations, depending on how generally useful
the new method is and whether or not you have write-access to the cylc
source tree:
\begin{myitemize}
    \item a \lstinline=python= sub-directory of your suite definition
        directory.
    \item any directory in, or added to, your \lstinline=PYTHONPATH= 
        environment variable.
    \item in the \lstinline=lib/cylc/job_submission= directory of
        the cylc source tree.
\end{myitemize}
Note that the form of the import statement at the top of the new 
user-defined Python module differs depending on whether or not 
the file is installed in the cylc source tree (see the comment
at the top of the example file above).


%\pagebreak


\section{Running Suites}
\label{RunningSuites}

To learn how to control running suites please also see the Quick Start
Guide (Section~\ref{QuickStartGuide}), command documentation 
(Section~\ref{CommandReference}), and experiment with cylc's example
suites and your own dummy task test suites.

\subsection{How Tasks Interact With Running Suites}

Cylc has three ways of tracking the progress of tasks, configured per
task host in the site/user config files (Section~\ref{SiteUserConfig}).
All three methods can be used on different task hosts within the same
suite if necessary.
\begin{myenumerate}
\item {\bf task-to-suite messaging:} cylc job scripts encapsulate task
scripting in a wrapper that automatically invokes messaging commands to
report progress back to the suite. The messaging commands can be
configured to work in two different ways:
    \begin{myenumerate}
        \item {\bf Pyro:} direct messaging via network sockets using  
        Pyro (Python Remote Objects). 
        \item {\bf ssh:} for tasks hosts that block access to the
        network ports required by Pyro, cylc can use passwordless ssh to
        re-invoke task messaging commands on the suite host (where
        ultimately Pyro is still used to connect to the server process).
    \end{myenumerate}
\item {\bf polling:} for task hosts that do not allow return routing to
the suite host for Pyro or ssh, cylc can poll tasks at configurable
intervals, using passwordless ssh. 
\end{myenumerate}

The Pyro communication method is the default because it is the most
direct and efficient; the ssh method inserts an extra step in the
process (command re-invocation on the suite host); and task polling is
the least efficient because results are checked at predetermined
intervals, not when task events actually occur.  

\subsubsection{Task Polling}

Be careful to avoid spamming task hosts with polling commands. Each poll
opens (and then closes) a new ssh connection. Polling subprocesses are
batched by cylc, and the number invoked at once can be configured in the
suite definition:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[cylc]
    [[poll and kill command submission]]
        batch size = 5  # default 10
        delay between batches = 10 # seconds, default 0
\end{lstlisting}

Polling intervals are configurable here because they should be
appropriate to the expected task run length. For instance, a task that
typically takes an hour to run might be polled every 10 minutes
initially, and then every minute toward the end of its run. Interval
values are used in turn until the last value, which is used repeatedly
until finished:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[foo]]
        # poll every minute in the 'submitted' state:
        submission polling intervals = 1.0
        # poll one minute after foo starts running, then every 10
        # minutes for 50 minutes, then every minute until finished:
        execution polling intervals = 1.0, 5*10.0, 1.0 
\end{lstlisting}
A list of intervals with optional multipliers can be used for both
submission and execution polling, although a single value is probably
sufficient for submission polling. If these items are not configured
default values from site/user config will be used for the polling
task communication method; polling is not done by default under the
other task communications methods (but it can still be used if you
like).

Polling is also done automatically once on job submission and execution
timeouts, to see if the timed-out task has failed or not; and on suite
restarts, to see what happened to any tasks that were orphaned when the
suite went down.

\subsection{Alternatives To Polling When Routing Is Blocked}

If Pyro and ssh ports are blocked but you don't want to use polling from
the suite host,
\begin{myitemize}
\item it has been suggested that network {\em port forwarding} may
provide a solution;
\item you may be able to persuade system administrators to provide
network routing to one or more dedicated cylc servers;
\item it is possible to run cylc itself on HPC login nodes, but
depending on what software is installed there this may preclude use
of the gcylc GUI and suite visualization tools.
\end{myitemize}

\subsection{Task Host Communications Configuration}

Here are the site/user config items relevant to task tracking:

\lstset{language=suiterc}
\begin{lstlisting}
#SITE/USER CONFIG

# Task messaging settings affect task-to-suite communications.
[task messaging]
    # If a message send fails, retry after this delay:
    retry interval in seconds = float( min=1, default=5 )
    # If send fails after this many tries, give up trying:
    maximum number of tries = integer( min=1, default=7 )

    # This timeout is the same as --pyro-timeout for user commands. If
    # set to None (no timeout) message send to non-responsive suite
    # (e.g. suspended with Ctrl-Z) could hang indefinitely.
    connection timeout in seconds = float( min=1, default=30 )

# Pyro is required for communications between cylc clients and servers
# (i.e. between suite-connecting commands and guis, and running suite
# server processes).
[pyro]

    # Each suite listens on a dedicated network port.
    # Servers bind on the first port available from the base port up:
# SITE ONLY
    base port = integer( default=7766 )

    # This sets the maximum number of suites that can run at once.
# SITE ONLY
    maximum number of ports = integer( default=100 )

    # Port numbers are recorded in this directory, by suite name.
    ports directory = string( default="$HOME/.cylc/ports/" )

[hosts]
    # The default task host is the suite host, i.e. localhost:
    # Add task host sections if local defaults are not sufficient.
    [[HOST]]
       # Method of communication of task progress back to the suite:
        #   1) pyro - direct client-server RPC via network ports
        #   2) ssh  - re-invoke pyro messaging commands on suite server
        #   3) poll - the suite polls for status of passive tasks
        # Pyro RPC is still required in all cases *on the suite host*
        # for cylc clients (commands etc.) to communicate with suites. 
        task communication method = option( "pyro", "ssh", "poll", default="pyro" ) 
        # The "poll" method sets a default interval here to ensure no
        # tasks are accidentally left unpolled. You should override this
        # with run-length appropriate intervals under task [runtime] -
        # which will also result in routine polling to check task health
        # under the pyro or ssh communications methods.
        default polling interval in minutes = float( min=0.1, default=1.0 ) 
\end{lstlisting}

\subsection{How Commands Interact With Running Suites}

User-invoked commands that connect to running suites can also choose
between direct communication across network sockets (Pyro) and
re-invocation of commands on the suite host using passwordless ssh
(there is a \lstinline=--use-ssh= command option for this purpose).

The gcylc GUI requires direct Pyro connections to its target suite. If
that is not possible, run gcylc on the suite host.


\subsection{Connection Authentication}

All Pyro connections to a running suite (task messaging and
user-invoked commands) must authenticate with an arbitary single line of
text in a file called \lstinline=passphrase=, which will be found and
used automatically if installed properly - see below.  A secure MD5
checksum, not the raw passphrase, is passed across the network. A random
passphrase is generated in the suite definition directory when a suite
is registered, but you can create your own if you wish.

For ssh task messaging and user command re-invocation, on the other
hand, the suite passphrase is only required on the suite host account
but ssh keys must be installed for passwordless connections instead. 

\subsubsection{Suite Pyro Passphrase Locations}
\label{passphrases}

Suite passphrases currently have to be installed manually to all task
host accounts that use the Pyro communication method (see above); and
also to accounts used to run commands that interact directly with the
suite via Pyro.

Legal passphrase locations, in order of preference, are:
\begin{myenumerate}
    \item \lstinline=$CYLC_SUITE_DEF_PATH/passphrase=
    \item \lstinline=$HOME/.cylc/SUITE_HOST/SUITE_OWNER/SUITE_NAME/passphrase=
    \item \lstinline=$HOME/.cylc/SUITE_HOST/SUITE_NAME/passphrase=
    \item \lstinline=$HOME/.cylc/SUITE_NAME/passphrase=
\end{myenumerate}
Remote tasks know the location of the remote suite definition directory
(if one exists) through their execution environment. Local (suite host)
user command invocations can find the suite definition directory in the
suite name database. Remote user command invocations, however,
cannot interrogate the database on the command host because the suite
will not be registered there (cylc cannot assume that the command host
shares a common filesystem with the suite host). Consequently remote
command host accounts must have the suite passphrase installed in one of
the secondary locations under \lstinline=$HOME/.cylc/=.


\subsection{How Tasks Get Access To Cylc}
\label{HowTasksGetAccessToCylc}

Running tasks need access to cylc via \lstinline=$PATH=, principally for
the task messaging commands.  To allow this, the first thing a task job
script does is set \lstinline=CYLC_VERSION= to the cylc version number,
then it sources \lstinline=/etc/profile= and \lstinline=$HOME/.profile=
to configure \lstinline=$PATH=. If you need to run several suites at
once under different incompatible versions of cylc, use
\lstinline=$HOME/.profile= to set \lstinline=$PATH= according to the
value of \lstinline=$CYLC_VERSION=. For example,
\lstset{language=bash}
\begin{lstlisting}
# $HOME/.profile

if [[ $- == *i* ]]; then
    # interactive shell: use the latest cylc
    export PATH=/path/to/cylc-latest/bin:$PATH
elif [[ "$CYLC_VERSION" = 5.3.3-dev ]]; then
    # non-interactive shell, called by a suite running a new
    # development version of cylc that I'm testing:
    export PATH=/path/to/cylc-5.3.3-test/bin:$PATH
else
    # non-interactive shell, called by a suite running our 
    # current production release of cylc.
    export PATH=/path/to/cylc/bin:$PATH
fi
\end{lstlisting}
{\em Note that \lstinline=$CYLC_VERSION= has only been used in this way
since cylc-5.3.}

It is also possible to configure \lstinline=$PATH= manually in the 
{\em initial scripting} section at the top of task job scripts,
but the use of \lstinline=.profile= is recommended because it works 
for all suites and it also allows selection of different cylc versions
forfor other commands invoked by cylc, namely task poll and kill
commands and event handlers (for these, cylc explicitly sets the version
variable and sources profile scripts at the start of the subprocess
command string).


\subsection{Restarting Suites}
\label{RestartingSuites}

A restarted suite (see \lstinline=cylc restart --help=) is initialized
from a previous recorded suite state dump so that it can carry on from
wherever it got to before being shut down or killed.

Tasks that were recorded in the submitted or running states are now
automatically polled on restart, to see if they are still submitted 
(e.g. waiting in a PBS batch queue or similar), still running, or if they 
finished (succeeded or failed) while the suite was down.

Tasks recorded in the failed state at shutdown are not automatically
resubmitted on restarting the suite, in case the underlying problem has
not been addressed yet. 

\subsection{Task States}

As a suite runs its task proxies transition through the following states:

\begin{myitemize}
    \item {\bf waiting} - task prerequisites not satisfied yet
    (clock-triggered tasks also wait on their trigger time).

    \item {\bf held} - task waiting, but will not be submitted even if ready
    to run because it (and possibly the suite as a whole) has been put
    on hold. Cylc automatically holds tasks that spawn past the final
    cycle time, if one is set. 

    \item {\bf runahead} - task waiting, but will not be submitted 
    even if ready to run because it has exceeded the {\em suite
    runahead limit}, i.e.\ it has got too far ahead of the slowest tasks
    and will be held back until they have caught up sufficiently. The
    size of the runahead limit is configurable - see
    Section~\ref{RunaheadLimit}.

    \item {\bf queued} - task ready to run (prerequisites satisfied) but
    is temporarily held in an {\em internal cylc queue} (see
    Section~\ref{InternalQueues}). Note this is not the same as the
    `submitted' state below.

    \item {\bf submitting} - task ready to run (prerequisites satisfied)
    and handed off the cylc background thread that handles job submission.

    \item {\bf submitted} - task successfully submitted but has not
    started running yet (e.g.\ waiting in a PBS batch queue).

    \item {\bf submit-failed} - task job submission failed {\em or} the 
    submitted job was killed before it could start executing.

    \item {\bf running} - task currently executing (a {\em task started}
    message was received, or the task polled as running).

    \item {\bf succeeded} - task finished successsfully (a {\em task
    succeeded} message was received, or the task polled as succeeeded).
    Succeeded task proxies are removed from the suite once they are no
    longer needed to satisfy the prerequisites of others.

    \item {\bf failed} - task aborted due to some error condition (a
    {\em task failed} message was received, or the task polled as failed). 
    Failed task proxies are not automatically removed from the suite 
    unless by deliberate use of suicide triggers.

    \item {\bf retrying} - task job submission or execution failed, but 
    a retry was configured (submission or execution retries, respectively) 
    so the task is currently waiting for the configured retry delay 
    before resubmitting. See Section~\ref{TaskRetries}. Tasks only enter
    the `submit-failed' or `failed' states if all configured retries 
    are exhausted. 

\end{myitemize}

Note that greyed-out ``base graph nodes'' in the gcylc graph view do not 
represent task states; they are displayed to fill out the graph
structure where corresponding task proxies do not currently exist
in the live task pool.

For manual task state reset purposes {\bf ready} is a pseudo-state that means
{\em waiting} with all prerequisites satisfied.


\subsection{Remote Control - Passphrases and Network Ports}

Connecting to a running suite requires knowing the {\em network port} it
is listening on, and the {\em suite passphrase} to authenticate with once
a connection is made to the port.

Suites write their port number to \lstinline=$HOME/.cylc/ports/<SUITE>=
at start-up, and suite-connecting commands read this file to get the
number.\footnote{If you accidentally delete a port file while a suite
is running, use \lstinline=cylc scan= to determine the port number
then use it on the command line (\lstinline=--port=) or rewrite the port
file manually.} An exception to this is the messaging commands called by
tasks. Running tasks know the port number from the execution environment
provided by the suite (via the task job script).

So, to connect to a suite running on another account you must install
the suite passphrase (Section~\ref{passphrases}), and configure
passwordless ssh so that the port number can be retrieved from the
remote port file. Then use the \lstinline=--owner= and
\lstinline=--host= command options to connect:
\begin{lstlisting}
$ cylc monitor --owner=USER --host=HOST SUITE
\end{lstlisting}
If you know the port number of the target suite, give it on the command
line to prevent the port-retrieving ssh connection being attempted:
\begin{lstlisting}
$ cylc monitor --owner=USER --host=HOST --port=PORT SUITE
\end{lstlisting}

Possession of a suite passphrase gives full control over the suite, and
ssh access to the port file also implies full access to the suite host
account, so it is recommended that this only be used to interact with
your own suites running on other hosts. We plan to implement
finer-grained authentication in the future to allow suite owners to
grant read-only access to others.


\subsection{Ensemble Suites, Job Submission, and Network Timeouts}
\label{MaxSimJobSub}

\subsubsection{Parallel Submission Of Jobs Ready At The Same Time}

Cylc now handles task job submission in a dedicated worker thread so
that submission of many remote tasks at once does not impact cylc's
performance or responsiveness.

Further, for maximum efficiency, job submissions are batched inside the
worker thread: batch members are submitted in parallel, and all members
must complete (the job submission process, that is, not the submitted
task) before the next batch is handled. There is a configurable delay
between batches to avoid swamping the host system in the event that
hundreds of tasks become ready at the same time:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[cylc]
    [[job submission]]
        batch size = 50 # default 10
        delay between batches = 10 # seconds, default 0
\end{lstlisting}
Here a 120 task ensemble, for example, would be submitted in two batches
of 50 followed by one of 20, with a 10 second delay between batches.

\subsubsection{Network Connection Timeouts}

A connection timeout can be set in site/user config files (see 
Section~\ref{SiteAndUserConfiguration}) so that messaging commands
cannot hang indefinitely if the suite is not responding (thie can be
caused by suspending a suite with Ctrl-Z) thereby preventing the task
from completing. The same can be done on the command line for other
suite-connecting user commands, with the \lstinline=--pyro-timeout= option.

\subsection{Internal Queues And The Runahead Limit}

Some cylc suites have the potential to generate too much activity at
once by virtue of the fact that each task cycles independently
constrained only by dependence on other tasks or by clock triggers. 
Quick-running tasks at the top of the dependency tree with no
prerequisites and no clock-triggers (or when running far behind the
clock) will spawn rapidly into the future if not constrained somehow.
There are two issues to be aware of here: over-burdening task host
resources by submitting too many tasks at once, and over-burdening cylc
itself by letting the task pool become too big (when fast tasks spawn
ahead of the pack cylc has to keep them around in the {\em succeeded}
state until other tasks, which may depend on them, have caught up). 

\subsubsection{The Suite Runahead Limit}
\label{RunaheadLimit}

The runahead limit prevents the fastest tasks in a suite from getting too
far ahead of the slowest ones. Cylc's cycle-interleaving abilities make 
for generally efficient scheduling, but there is no great advantage in
letting a few fast data retrieval tasks, say, get far ahead of the
slower tasks because it is typically the tasks at the bottom of the
dependency tree, which necessarily run last, that generate the final
products.

\lstset{language=suiterc}
\begin{lstlisting}
#SUITE.RC
[scheduling]
    runahead limit = 48 # hours
\end{lstlisting}

A cycling task spawns its successor when it enters the submitted state
or, for sequential tasks, when it finishes. If a newly spawned task's
cycle time is ahead of the oldest non-finished (succeeded or failed)
task by more than the runahead limit it is put into the special
{\em runahead} held state until other tasks catch up sufficiently; i.e.\
the runahead limit constrains the number of cycles that can run at once.

The default runahead limit is normally set to twice the minimum cycling
interval in the suite. For a suite with 1- and 24-hourly cylcing tasks
the default limit will be 2 hours, so that two of the hourly cycles can 
run at once in between the 24-hourly cycles.  If there are any future
triggers present (\lstinline@graph = "foo[T+24] => bar"@) that extend
beyond the default limit, it is adjusted up to equal the future
offset plus one minimum cycling interval.

A manually set runahead limit should not stall the suite even if set to
less than the minimum cycling interval, unless it does not extend out
past any future triggers.

Succeeded and failed tasks are ignored when applying the runahead limit
(but tasks that can't run because they depend on a failed task are not
ignored, of course). 

\subsubsection{Internal Queues}
\label{InternalQueues}

Large suites could potentially swamp the task host hardware or external
batch queueing system, depending on the chosen job submission method, by
submitting too many tasks at once. Cylc's internal queues prevent this 
by limiting the number of tasks, within defined groups, that are active
(submitted or running) at once.

A queue is defined by a name; a {\em limit}, which is the maximum
number of active tasks allowed for the queue; and a list of member tasks,
which are assigned by name to the queue.

Queue configuration is done under the [scheduling] section of the
suite.rc file, not as part of the runtime namespace hierarchy, because
like dependencies queues constrain {\em when} a task runs rather than
{\em what} runs after it is submitted. When runtime family relationships
and queues do coincide you can assign task family members en masse to
queues by using the family name, as shown in the example suite listing
below.

By default every task is assigned to a {\em default} queue, which by
default has a zero limit (interpreted by cylc as no limit). To use a
single queue for the whole suite just set the default queue
limit:
\lstset{language=suiterc}
\begin{lstlisting}
#SUITE.RC
[scheduling]
    [[ queues]]
        # limit the entire suite to 5 active tasks at once
        [[[default]]]
            limit = 5
\end{lstlisting}
To use other queues just name each one, set the limit, and assign member
tasks:
\begin{lstlisting}
#SUITE.RC
[scheduling]
    [[ queues]]
        [[[q_foo]]]
            limit = 5
            members = foo, bar, baz
\end{lstlisting}
Any tasks not assigned to a particular queue will remain in the default
queue. The {\em queues} example suite illustrates how queues work by 
running two task trees side by side (as seen in the graph GUI) each
limited to 2 and 3 tasks respectively:
\lstset{language=suiterc}
\lstinputlisting{../examples/queues/suite.rc}
Note assignment of runtime task family members to queues using the
family name.

\subsection{Automatic Task Retry On Failure}
\label{TaskRetries}

See also Section~\ref{RefRetries} in the {\em Suite.rc Reference}.

Tasks can be configured with a list of ``retry delay'' periods, in
minutes, such that if a task fails it will go into a temporary
{\em retrying} state and then automatically resubmit itself after
the next specified delay period expires. A usage example is shown in the
suite listed below under {\em Suite And Task Event Handling},
Section~\ref{EventHandling}.

\subsection{Suite And Task Event Handling}
\label{EventHandling}

See also Sections~\ref{SuiteEventHandling} and~\ref{TaskEventHandling}
in the {\em Suite.rc Reference}.

Cylc can call nominated event handlers when certain suite or task events
occur. This is intended to facilitate centralized alerting and automated
handling of critical events. Event handlers can send an email or an SMS,
call a pager, and so on; or intervene in the operation of their own
suite using cylc commands.  \lstinline=cylc [hook] email-suite= and
\lstinline=cylc [hook] email-task= are example event handlers packaged
with cylc.

Event handlers can be located in the suite bin directory, otherwise 
it is up to you to ensure their location is in \lstinline=$PATH=
(in the shell in which cylc runs, on the suite host).

Task event handlers are passed the following arguments by cylc:

\begin{lstlisting}
<task-event-handler> EVENT SUITE TASKID MESSAGE
\end{lstlisting}
where EVENT is one of the following:

\begin{myitemize}
    \item `submitted' - the job submit command was successful
    \item `submission failed' - the job submit command failed
    \item `submission timeout' - task job submission timed out
    \item `submission retry' - task job submission failed, but will retry after a configured delay
    \item `started' - the task reported commencement of execution
    \item `succeeded' - the task reported successful completion
    \item `warning' - the task reported a warning message
    \item `failed' - the task failed
    \item `retry' - the task failed but will retry
    \item `execution timeout' - task execution timed out
\end{myitemize}
MESSAGE, if provided, describes what has happened, and TASKID identifies
the task (\lstinline=NAME.CYCLE= for cycling tasks).

The retry event occurs if a task fails and has any remaining retries
configured (see Section~\ref{TaskRetries}). 
The event handler will be called as soon as the task fails, not after
the retry delay period when it is resubmitted.

{\em Note that event handlers are called by cylc itself, not by the 
running tasks} so if you wish to pass them additional information via
the environment you must use [cylc] $\rightarrow$ [[environment]],
not task runtime environments.

Here is an example suite that tests the {\em retry} and {\em failed} events.
The handler in this case simply echoes its command line arguments to
suite stdout.

\lstset{language=suiterc}
\begin{lstlisting}
[scheduling]
    initial cycle time = 2010080800
    final cycle time = 2010081000
    [[dependencies]]
        [[[0]]]
            graph = "foo => bar"
[runtime]
    [[foo]]
        retry delays = 0, 0.5
        command scripting = """
echo TRY NUMBER: $CYLC_TASK_TRY_NUMBER
sleep 10
# retry twice and succeed on the final try,
# but fail definitively in the final cycle.
if (( CYLC_TASK_TRY_NUMBER <= 2 )) || \
    (( CYLC_TASK_CYCLE_TIME == CYLC_SUITE_FINAL_CYCLE_TIME )); then
    echo ABORTING
    /bin/false
fi"""
        [[[event hooks]]]
            retry handler = "echo !!!!!EVENT!!!!! "
            failed handler = "echo !!!!!EVENT!!!!! "
\end{lstlisting}

\subsection{Reloading The Suite Definition At Runtime}

The \lstinline=cylc reload= command reloads the suite definition at run
time. This allows:
 (a) changing task config such as command scripting or environment;
 (b) adding tasks to, or removing them from, the suite definition,
at run time - without shutting the suite down and restarting it. (It is
easy to shut down and restart cylc suites, but reloading may be useful
if you don't want to wait for long-running tasks to finish first).

Note that {\em defined tasks} can be already be added to or removed from
a running suite with the 'cylc insert' and 'cylc remove' commands; the
reload command allows addition and removal of {\em task definitions}.
If a new task is definition is added (and used in the graph) you will
still need to manually insert an instance of it (with a particular cycle
time) into the running suite. If a task definition (and its use in
the graph) is deleted, existing task proxies of the of the deleted type
will run their course after the reload but new instances will not be
spawned. Changes to a task definition will only take effect when the
next task instance is spawned (existing instances will not be affected).


\subsection{Handling Job Preemption}
\label{PreemptionHPC}

Some HPC facilities allow job preemption: the resource manager can kill
or suspend running low priority jobs in order to make way for high
priority jobs. The preempted jobs may then be automatically restarted
by the resource manager, from the same point (if suspended) or requeued
to run again from the start (if killed). If a running cylc task gets
suspended or hard-killed 
(\lstinline=kill -9 <PID>= is not a trappable signal so cylc cannot detect
task failure in this case) and then later restarted, it will just appear
to cylc as if it takes longer than normal to run. If the job is
soft-killed the signal will be trapped by the task job script and a
failure message sent, resulting in cylc putting the task into the failed
state.  When the preempted task restarts and sends its started message
cylc would normally treat this as an error condition (a dead task is not
supposed to be sending messages) - a warning will be logged and the task
will remain in the failed state. However, if you know that preemption is
possible on your system you can tell cylc that affected tasks should be
resurrected from the dead, to carry on as normal if progress messages
start coming in again after a failure: 

\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
# ...
[runtime]
    [[on_HPC]]
        enable resurrection = True
    [[TaskFoo]]
        inherit = on_HPC
# ...
\end{lstlisting}

To test this in any suite, manually kill a running task then, after cylc
registers the task failed, resubmit the killed job manually by
cutting-and-pasting the original job submission command from the suite
stdout stream.

\subsection{Runtime Settings Broadcast and Communication Between Tasks}

The \lstinline=cylc broadcast= command overrides \lstinline=[runtime]=
settings in a running suite. This can 
be used to communicate information to downstream tasks by broadcasting 
environment variables (communication of information from one task to
another normally takes place via the filesystem, i.e.\ the input/output
file relationships embodied in inter-task dependencies). Variables (and
any other runtime settings) may be broadcast to all subsequent tasks, 
or targetted specifically at a specific task, all subsequent tasks with a 
given name, or all tasks with a given cycle time; see broadcast command
help for details.

Broadcast settings targetted at a specific task ID or cycle time expire
and are forgotten as the suite moves on. Untargetted variables and those
targetted at a task name persist throughout the suite run, even across
restarts, unless manually cleared using the broadcast command - and so
should be used sparingly. 

\subsection{The Meaning And Use Of Initial Cycle Time}

When a suite is started with the \lstinline=cylc run= command (cold or
warm start) the cycle time at which it starts can be given on the
command line or hardwired into the suite.rc file:
\begin{lstlisting}
cylc run foo 2012080806
\end{lstlisting}
or,
\begin{lstlisting}
# SUITE.RC
[scheduling]
    initial cycle time = 2010080806
\end{lstlisting}
An initial cycle time given on the command line will override one in the
suite.rc file.

\subsubsection[CYLC\_SUITE\_INITIAL\_CYCLE\_TIME]{The Environment Variable CYLC\_SUITE\_INITIAL\_CYCLE\_TIME}

In the case of {\em cold starts only} the initial cycle time will also
be passed through to task execution environments as
\lstinline=$CYLC_SUITE_INITIAL_CYCLE_TIME=.  The intended use of this
variable is to allow tasks to determine whether they are running in the
initial cold-start cycle (when different behaviour may be required) or
in a normal mid-run cycle. {\em This is not done for warm starts}
because a warm start is really an implicit restart - it does not
reference a particular previous suite state but it does assume that a 
previous cycle (for each task) has been run and completed entirely.
It follows that in a warm start tasks are really in a normal mid-run
cycle, and because no actual previous state is referenced
\lstinline=$CYLC_SUITE_INITIAL_CYCLE_TIME= gets the value
\lstinline=None=. After a cold-start, however, the value of the 
environment variable does persist across restarts because the original
cold-start cycle time is stored in suite state dump files. 


\subsection{The Simulation And Dummy Run Modes} 
\label{SimulationMode}

Since cylc-4.6.0 any cylc suite can run in {\em live}, {\em simulation},
or {\em dummy} mode.  Prior to that release simulation mode was a
hybrid mode that replaced real tasks with local dummy tasks. This
allowed local simulation testing of any suite, to get the scheduling
right without running real tasks, but running dummy tasks locally does
not add much value over a pure simulation (in which no tasks are
submitted at all) because all job submission configuration has to be
ignored and most task job script sections have to be cut out to avoid
any code that could potentially be specific to the intended task host.
So at 4.6.0 we replaced this with a pure simulation mode (task proxies
go through the {\em running} state automatically within cylc, and no
dummy tasks are submitted to run) and a new dummy mode in which only the
real task command scripting  is dummied out - each dummy task is
submitted exactly as the task it represents on the correct host and in
the same execution environment. A successful dummy run confirms not only
that the scheduling works correctly but also tests real job submission,
communication from remote task hosts, and the real task job scripts (in
which errors such as use of undefined variables will cause a task to
fail).

The run mode, which defaults to {\em live}, is set on the command line
(for run and restart):
\lstset{language=transcript}
\begin{lstlisting}
% cylc run --mode=dummy SUITE
\end{lstlisting}
but you can configure the suite to force a particular run mode,
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[cylc]
    force run mode = simulation
\end{lstlisting}
This can be used, for example, for demo suites that necessarily run out
of their original context; or to temporarily prevent accidental
execution of expensive real tasks during suite development.

Dummy task command scripting just prints a message and sleeps for ten
seconds by default, but you can override this behaviour for particular
tasks or task groups if you like. Here's how to make a task sleep for
twenty seconds and then fail in dummy mode:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[foo]]
        command scripting = "run-real-task.sh"
        dummy mode command scripting = """
echo "hello from dummy task $CYLC_TASK_ID"
sleep 20
echo "ABORTING"
/bin/false"""
\end{lstlisting}

Finally, in simulation mode each task takes between 1 and 15 seconds to
``run'' by default, but you can also alter this for particular tasks or
groups of tasks:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[foo]]
        run time range = 20,31 # (between 20 and 30 seconds)
        command scripting = "echo ABORTING; /bin/false" # fail in dummy mode
\end{lstlisting}
Note that to get a failed simulation or dummy mode task to succeed on
re-triggering, just change the suite.rc file appropriately and reload
the suite definition at run time with \lstinline=cylc reload SUITE=
before re-triggering the task.

Dummy mode is equivalent to commenting out each task's command scripting
to expose the default scripting. 

\subsubsection{The Non-live-mode Accelerated Clock}

In simulation and dummy mode cylc uses an accelerated clock with
configurable rate and offset relative to the suite's initial cycle time.
This affects the trigger time of any clock-triggered tasks in the suite,
and the length of time between cycles if simulating ``caught up''
operation (without this a six-hour cycling suite, for instance, would
wait six hours between cycles when simulating caught-up operation, even
though the simulated or dummy tasks run very quickly). By configuring
the initial clock offset you can quickly simulate how suites catch up
and transition from delayed to real time operation.

See Section~\ref{ClockConfig} for accelerated clock configuration
settings.

\subsubsection{Restarting Suites With A Different Run Mode?}

The run mode is recorded in the suite state dump file. Cylc will not let
you {\em restart} a non-live mode suite in live mode, or vice versa -
any attempt to do the former would certainly be a mistake (because the
simulation mode dummy tasks do not generate any of the real outputs
depended on by downstream live tasks), and the latter, while feasible,
would corrupt the live state dump by turning it over to simulation mode.
The easiest way to test a live suite in simulation mode, if you don't
want to obliterate the current state dump by doing a cold or warm start
(as opposed to a restart from the previous state) is to take a quick
copy of the suite and run the copy in simulation mode. However, if you
really want to run a live suite forward in simulation mode without
copying it, do this:
\begin{myenumerate}
    \item Back up the live mode suite state dump file.
    \item Edit the mode line in the state dump and restart in simulation mode.
    \item Later, restart the live suite from the restored live state dump back up.
\end{myenumerate}

%{\em Finally, it should be noted that cylc previously had the ability to
%create an instant safe simulation mode clone of the current state of any
%suite, running or stopped, but this ``practice mode'' has been disabled
%since the upgrade to cylc-3, pending testing; it could easily be
%restored if needed.}

%\subsubsection{Practice Mode}
%
%Practice mode allows quick and easy testing of potentially complex
%suite interventions, with complete safety.
%
%\begin{lstlisting}
%cylc restart --practice examples:CUG_1
%\end{lstlisting}
%
%This will start a simulation mode clone of an existing suite from the
%current state of that suite (which may be paused, still running, or
%halted), but using different state and log files so that the original
%suite will not be corrupted by the clone.
%
%{\em At start-up in practice mode, failed tasks are not reset to waiting}
%because the whole point of practice mode is to ``practice'' how to
%recover from failures.
%
%Note that other cylc commands for monitoring or interacting with the
%suite must also use the \lstinline=--practice= option in order to
%target the practice suite and not the real one. Be sure to set
%\lstinline=cylc lock= on the original suite first, to avoid
%accidentally messing with it (even if you do screw up, however, cylc's
%automatic pre-intervention state dumps will save you!).
%
%
%\subsubsection{Roll Your Own Practice Mode}
%
%A less automated way to ``practice'' on a copy of an existing suite
%that starts up from the current (or previous) state of that suite, 
%\lstinline=cylc run --practice= is this:
%
%\begin{myitemize}
%    \item register your suite again under a different name. This allows
%        you to run a simulation mode copy of the same suite without
%        interfering with the original suite (it also allows you to run
%        a copy of the live mode suite without interference, but only if
%        the real suite tasks are configured to use the registered
%        suite name in all important input and output filenames and/or
%        directory paths - see {\em Command Reference} Section~\ref{register}).
%
%    \item start-up the newly registered suite in simulation mode using:
%        \begin{lstlisting}
%cylc restart --simulation-mode SUITE PATH
%        \end{lstlisting}
%        where PATH is a state dump file from the original suite. The
%        absolute path is required here because the default state
%        dump location depends on the registered suite name (so that
%        different suites don't interfere with each other's state
%        dumps).
%
%\end{myitemize}
%

        

%\pagebreak

%\section{Network Issues}
%
%Cylc can control tasks on a distributed system (multiple hosts).  If you
%do have a distributed suite, in any or all of these ways, be aware of
%the following issues: 
%
%\begin{myitemize}
%
%    \item In addition to the cylc host, cylc must be installed on all
%        task hosts; and the remote task scripts must
%        themselves be installed on their host machines.  Refer to {\em
%        Running Tasks On A Remote Host}
%        (Section~\ref{RunningTasksOnARemoteHost}) for more on this.
%
%    \item Pyro must be installed on every host used by the suite.
%         Ideally all relevant machines should have the same version of
%         Pyro, but you can easily check for Pyro version
%         compatibility by attempting to run one of the cylc example
%         system.
%        
%\end{myitemize}
%
%Other notes relevant to the last point above: the \lstinline=--host=
%cylc command option defaults to Python \lstinline=socket.getfqdn()=,
%which retrieves the fully qualified domain name of the local host
%if possible.  But \lstinline=/etc/hosts= may cause this to return
%just the hostname, which locally may resolve to the local-only IP
%address that is not accessible on the network.  Short of reconfiguring
%the hosts file, you may be able to workaround these problems by:
%
%\begin{myitemize}
%
%    \item and, get cylc to configure the Pyro daemon 
%        with \lstinline@Pyro.config.PYRO_DNS_URI = True@ 
%        
%    \item and, use the \lstinline=--host= cylc command option where
%        required.
%
%\end{myitemize}


\subsection{Automated Reference Test Suites}
\label{AutoRefTests}

Reference tests are finite-duration suite runs that abort with non-zero
exit status if any of the following conditions occur (by default):

\begin{myitemize}
    \item cylc fails
    \item any task fails
    \item the suite times out (e.g.\ a task dies without reporting failure)
    \item a nominated shutdown event handler exits with error status
\end{myitemize}

The default shutdown event handler for reference tests is   
\lstinline=cylc hook check-triggering= which compares task triggering
information (what triggers off what at run time) in the test run suite
log to that from an earlier reference run, disregarding the timing and
order of events - which can vary according to the external queueing
conditions, runahead limit, and so on.

To prepare a reference log for a suite, run it with the
\lstinline=--reference-log= option, and manually verify the
correctness of the reference run.

To reference test a suite, just run it (in dummy mode for the most
comprehensive test without running real tasks) with the
\lstinline=--reference-test= option.

A battery of reference tests are (or will soon be) used to automatically
test cylc before posting a new release version.  They can also be used
at cylc upgrade time to check that changes in cylc will not break
your own complex suites - the triggering check will catch any bug that
causes a task to run when it shouldn't, for instance; and even in a
dummy mode reference test the full task job script (sans real command
scripting) has to execute successfully on the proper task host by the
proper job submission method.

The reference test can be configured with the following settings:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[cylc]
    [[reference test]]
        suite shutdown event handler = cylc check-triggering
        required run mode = dummy
        allow task failures = False
        live mode suite timeout = 5 # minutes
        dummy mode suite timeout = 2
        simulation mode suite timeout = 2
\end{lstlisting}

\subsubsection{Roll-your-own Reference Tests}

If the default reference test is not sufficient for your needs, firstly
note that you can override the default shutdown event handler, and
secondly that the \lstinline=--reference-test= option is merely a short
cut to the following suite.rc settings which can also be set manually if
you wish:

\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[cylc]
    abort if any task fails = True
    [[event hooks]]
        shutdown handler = cylc check-triggering
        timeout = 5
        abort if shutdown handler fails = True
        abort on timeout = True
\end{lstlisting}

\section{Other Topics In Brief}

The following topics have yet to be documented in detail.

\begin{myitemize}
    \item The difference between cold-, warm-, raw-, and re-starting, a suite:
        see \lstinline=cylc run help=.

    \item Intervening in suites, e.g.\ stopping, removing, inserting tasks: 
        see \lstinline=cylc control help=.

    \item Interrogating suites and tasks: 
        see \lstinline=cylc info help=, \lstinline=cylc show help=,
        and \lstinline=cylc discovery help=.

    \item Understanding suite evolution, particularly in delayed/catchup
        operation: the {\em Quick Start Guide} helps here, along with
        running the example suites.

    \item Automatic state dump backups, named pre-intervention state dumps:
        these are used to resetart a suite from a previous state of operation.
        They are mentioned in the {\em Quick Start Guide}. Watch the
        suite log after intervening in a suite, to get the filename.

    \item Recursive purge: this is a powerful intervention but you need
        to understand how it works before using it. See 
        \lstinline=cylc purge help= for details.

    \item Handling spin-up processes via temporary tasks and adding
        prerequisites on-the-fly: see \lstinline=cylc depend --help=,
        and note that when you insert a task into a running suite (a)
        the initial cycle time can be in the past; and (b) you can give
        a final cycle time after which the task will be eliminated from
        the suite. 

    % \item How to recover from certain kinds of task failure.
    %\item instant simulation mode clones of a running suite
    %\item fuzzy prerequisites - triggering of the {\em most recent available},
    %    within limits, instance of an upstream task

    \item Sub-suites: to run another suite inside a task, just invoke the 
        subsuite, with appropriate start and end cycles (probably a
        single cycle), in the host task's command scripting:

\lstset{language=suiterc}
\begin{lstlisting}
[runtime]
    [[foo]]
        command scripting = \
        "cylc run SUITE $CYLC_TASK_CYCLE_TIME --until=$CYLC_TASK_CYCLE_TIME"
\end{lstlisting}

    \item Dependence between suites: cylc will eventually support this
        directly, via special graph syntax. For the moment dependence on
        other suites can be implemented easily, within task scripting,
        using the command \lstinline=cylc suite-state=, which has a
        polling mode and reads the other suite's run database.

\end{myitemize}
\lstset{language=transcript}

\section{Suite Storage, Discovery, Revision Control, and Deployment}

Small groups of cylc users can of course share suites by manual copying,
and generic revision control tools can be used on cylc suites as for any
collection of files. Beyond this cylc does not have a built-in solution
for suite storage and discovery, revision control, and deployment, on a
network. That is not cylc's core purpose, and large sites may have
preferred revision control systems and suite meta-data requirements that
are difficult to anticipate. We can, however, recommend the use of {\em
Rose} to do all of this very easily and elegantly with cylc suites.

\subsection{Rose}

{\bf Rose} is {\em a framework for managing and running suites of
scientific applications}, developed at the UK Met Office for use with
cylc. It is available under the open source GPL license.

\begin{myitemize}
    \item Rose documentation: http://metomi.github.io/rose/doc/rose.html
    \item Rose source repository: https://github.com/metomi/rose
\end{myitemize}


\section{Suite Design Principles}
\label{SuiteDesignPrinciples}

%Simplicity, flexibility, efficiency, and portability of cylc suites.

\subsection{Make Fine-Grained Suites} 
\label{Granularity}

A suite can contain a small number of large, internally complex tasks; a
large number of small, simple tasks; or anything in between. Cylc can
easily handle a large number of tasks, however, so there are definite
advantages to fine-graining:

\begin{myitemize}
    \item a more modular and transparent suite.

    \item better functional parallelism (multiple tasks running
        at the same time).

    \item faster debugging and failure recovery: rerun just the tasks(s)
        that failed. 

    \item code reuse: similar tasks can often call the same script or
        command with differing task-specific input parameters
        (consider tasks that move files around, for example).

\end{myitemize}

\subsection{Make Tasks Rerunnable}

It should be possible to rerun a task by simply resubmitting it for the
same cycle time. In other words, failure at any point during execution
of a task should not render a rerun impossible by corrupting the state
of some internal-use file, or whatever. It is difficult to overstate the
usefulness of being able to rerun the same task multiple times,
either outside of the suite with \lstinline=cylc submit=, or by
retriggering it within the running suite, when debugging a problem.

\subsection{Make Models Rerunnable} 

If a warm-cycled model simply overwrites its restart files in each
run, the only cycle that can subsequently run is the next one. This
is dangerous because if, accidentally or otherwise, the task runs for the
wrong cycle time, its restart files will be corrupted such that the
correct cycle can no longer run (probably necessitating a cold-start).
Instead, consider organising restart files by cycle time, through a file
or directory naming convention, and keep them in a simple rolling
archive (cylc's filename templating and housekeeping
utilities can easily do this for you). Then, given availability of 
external inputs, you can easily rerun the task for any cycle still
in the restart archive.

\subsection{Limit Previous-Instance Dependence} 
\label{LimitPID}

Cylc does not require that successive instances of the same task run 
sequentially. In order to task advantage of this and achieve maximum
functional parallelism whenever the opportunity arises (usually when 
catching up from a delay) you should ensure that tasks that in
principle do not depend on their own previous instances (the vast
majority of tasks in most suites, in fact) do not do so in practice. In
other words, they should be able to run as soon as their prerequisites
are satisfied regardless of whether or not their predecessors have
finished yet.  This generally just means ensuring that all file I/O
contains the generating task's cycle time in the file or directory name
so that there is no interference between successive instances. If this
is difficult to achieve in particular cases, however, you can declare
the offending tasks to be {\em sequential}.

% MAYBE SHOULD INCLUDE THE FOLLOWING HERE:
%Warm-cycled forecast models {\em do} depend on their own previous
%instances (through their ``model background'' restart prerequisites).
%These can be made to run sequentially (i.e.\ with maximal previous
%instance dependence) but you can have cylc suite launch the next model,
%assuming other prerequisites are satisfied, as soon as the previous one
%has completed its restart prerequisites (minimal previous instance
%dependence, maximal throughput).

\subsection{Put Task Cycle Time In All Output File Paths}
\label{PutCycleTimeinIO}

Having all filenames, or perhaps the names of their containing
directories, stamped with the cycle time of the generating task greatly
aids in managing suite disk usage, both for archiving and cleanup. It
also enables the aforementioned task rerunnability recommendation by avoiding
overwrite of important files from one cycle to the next. Cylc has 
powerful utilities for cycle time offset filename templating and
housekeeping.

\subsubsection{Use Cylc Cycle Time Filename Templating}

The command line utility program \lstinline=cylc [util] cycletime=
computes offsets (in hours, days, months, and years) from a given or
current (in the environment) cycle time, and optionally inserts the
resulting computed cycle time, or components of it, into a given 
template string containing ``YYYY'' as a placeholder for the year
value, ``MM'' for month, and so on. This can be used in the suite.rc
environment or command scripting sections, or in task implementation
scripting, to generate filenames containing the current cycle time (or
some offset from it) for use by tasks.

See \lstinline=cylc [util] cycletime --help= for examples.

\subsection{How To Manage Input/Output File Dependencies}
\label{HandlingDependencies}

Dependencies between tasks usually, though not always, take the form of
files generated by one task that are used by other tasks. It is possible
to manage these files across a suite without hard wiring I/O locations 
and therefore compromising suite flexibility and portability.

\begin{myitemize}

\item {\bf Use A Common I/O Workspace}

For small suites you may be able to have all tasks read and write from a
common workspace, thereby avoiding the need to move common files around.
You should be able to define the workspace location once in the suite.rc
file rather than hard wiring it into the task implementations.

\item {\bf Add Connector Tasks To The Suite} 

Tasks can be added to a suite to move files from A's output directory to
B's input directory, and so on.  Many connector tasks may be able
to call the same file transfer script or command, with task-dependendent
input parameters defined in the suite.rc file. 

\item {\bf Dynamic Configuration Of I/O Paths}

Whether or not your suite uses a single common workspace, passing common
I/O paths to tasks via variables defined once in the suite.rc file should
allow you to avoid using connector tasks at all, except where it is
necessary to transfer files between machines, or similar. 

\end{myitemize}

\subsection{Use Generic Task Scripts}

If your suite contains multiple logically distinct tasks that actually
have similar functionality (e.g.\ for moving files around, or for 
generating similar products from the output of several similar models)
have the corresponding cylc tasks all call the same command, script, or
executable - just provide different input parameters
via the task command scripting and/or execution environment, in the
suite.rc file.


\subsection{Make Suites Portable}

If every task in a suite is configured to put its output under
\lstinline=$HOME= (i.e.\ the environment variable, literally, not the
explicit path to your home directory; and similarly for temporary
directories, etc.) then other users will be able to copy the suite and
run it immediately, after merely ensuring that any external input files
are in the right place.

For the ultimate in portability, construct suites in which all task I/O
paths are dynamically configured to be user and suite (registration)
specific, e.g.
\begin{lstlisting}
$HOME/output/$CYLC_SUITE_REG_PATH
\end{lstlisting}
(these variables are automatically exported to the task execution
environment by cylc - see {\em Task Execution Environment},
Section~\ref{TaskExecutionEnvironment}). Then you can run multiple
instances of the suite
at once (even under the same user account) without changing anything,
and they will not interfere with each other.

{\em You can test changes to a portable suite safely by making a quick copy
of it in a temporary directory, then modifying and running the test copy 
without fear of corrupting the output directories, suite logs, and 
suite state, of the original.} 


\subsection{Make Tasks As Self-Contained As Possible}

Where possible, no task should rely on the action of another task,
except for the prerequisites embodied in the suite dependency graph that
it has no choice but to depend on. If this rule is followed, your suite
will be as flexible as possible in terms of being able to run single
tasks, or subsets of the suite, whilst debugging or developing new
features.\footnote{The \lstinline=cylc submit= command runs a single
task exactly as its suite would, in terms of both job submission method
and execution environment.}  For example, every task should create its
own output directories if they do not already exist, instead of assuming
their existence due to the action of some other task; then you will be
able to run single tasks without having to manually create output
directories first. 

\begin{lstlisting}
# manual task scripting:
  # 1/ create $OUTDIR if it doesn't already exist:
  mkdir -p $OUTDIR
  # 2/ create the parent directory of $OUTFILE if it doesn't exist:
  mkdir -p $( dirname $OUTFILE )

# OR using the cylc checkvars utility:
  # 1/ check vars are defined, and create directories if necessary:
  cylc util checkvars -c OUTDIR1 OUTDIR2 #...
  # 2/ check vars are defined, and create parent dirs if necessary:
  cylc util checkvars -p OUTFILE1 OUTFILE2 #...
\end{lstlisting}

\subsection{Make Suites As Self-Contained As Possible}

The only compulsory content of a cylc suite definition directory is the
\lstinline=suite.rc= file. However, you can store whatever you
like in a suite definition directory;\footnote{If you copy a suite using
cylc commands or GUI the entire suite definition directory
will be copied.} other files there will be ignored by cylc but
suite tasks can access them via the
\lstinline=$CYLC_SUITE_DEF_PATH= variable that cylc automatically 
exports into the task execution environment. Disk space is cheap - if
all programs, ancillary files, control files (etc.) required by the
suite are stored in the suite
definition directory instead of having the suite reference external
build directories (etc.), you can turn the directory into a revision
control repository and be virtually assured of the ability to exactly
reproduce earlier versions, regardless of suite complexity.

\subsection{Orderly Product Generation?}
\label{OrderlyProductGeneration}

Correct scheduling is not equivalent to ``orderly generation of products
by cycle time''.  Under cylc, a product generation task will trigger as
soon as its prerequisites are satisfied (i.e.\ when its input files are
ready, generally) regardless of whether other tasks with the same cycle
time have finished or have yet to run. If your product delivery or
presentation system demands that all products for one cycle time are
uploaded (or whatever) before any from the next cycle, then be aware
that this may be quite inefficient if your suite is ever faced with
catching up from a significant delay or running over historical data.

If you must, however, you can introduce artificial dependencies into
your suite to ensure that the final products never arrive out of
sequence.  One way of doing this would be to have a final ``product
upload'' task that depends on completion of all the real product
generation tasks at the same cycle time, and then declare it to be
sequential.

\subsection{Clock-triggered Tasks Wait On External Data}

All tasks in a cylc suite know their own private cycle time, but most
don't care about the wall clock time - they just run when their
prerequisites are satisfied. The exception to this is {\em
clock-triggered} tasks, which wait on a wall clock time expressed as an
offset from their own cycle time, in addition to any other
prerequisites. The usual purpose of these tasks is to retrieve real time
data from the external world, triggering at roughly the expected time of
availability of the data. Triggering the task at the right time is up to
cylc, but the task itself should go into a check-and-wait loop in case
the data is delayed; only on successful detection or retrieval should
the task report success and then exit (or perhaps report failure and
then exit if the data has not arrived by some cutoff time). 

\subsection{Do Not Treat Real Time Operation As Special} 

Cylc suites, without modification, can handle real time and delayed
operation equally well.

In real time operation clock-triggered tasks constrain the
behaviour of the whole suite, or at least of all tasks 
downstream of them in the dependency graph.

In delayed operation (whether due to an actual delay in an operational
suite or because you're running an historical trial) clock-triggered
tasks will not constrain the suite at all, and cylc's cycle interleaving 
abilities come to the fore, because their trigger times have already
passed.  But if a clock-triggered task happens to catch up to the wall
clock, it will automatically wait again. In this way a cylc suite
naturally and seamlessly transitions between delayed and real time
operation as required.

\pagebreak

\appendix

\input{suiterc.tex}

\pagebreak

\section{Site/User Config File Reference}
\label{SiteUserConfig}

Cylc now has a site config file for settings that apply to all suites:
    \lstinline=$CYLC_DIR/conf/site/site.rc=.
Prior to cylc-5.0 some of these settings (e.g. log directory locations)
had to be put in suite definitions, and some (e.g. preferred editors) 
were specified in the user's environment.
Many of the site settings can be overridden by users: 
    \lstinline=$HOME/.cylc/user.rc=

See also \lstinline=cylc get-global-config --help=, which can be used to
write an initial site or user config file with internal documentation
and all items commented out.

As a temporary measure the self-documenting {\em configspec} for 
site/user config files is included verbatim here.  

\lstinputlisting{../conf/siterc/cfgspec}

\pagebreak


\section{Command Reference}
\label{CommandReference}

%This section is auto-generated from the self-documenting command set.
  
\lstset{language=usage}
\input{commands.tex}
\lstset{language=transcript}

\section{The Cylc Lockserver}
\label{TheCylcLockserver}

Each cylc user can optionally run his/her own lockserver to prevent
accidental invocation of multiple instances of the same suite or task at
the same time. The suite and task locks brokered by the lockserver are
analogous to traditional lock files, but they work across a network,
even for distributed suites containing tasks that start executing on one
host and finish on another.  

Accidental invocation of multiple instances 
of the same suite or task at the same time can have serious consequences, 
so use of the lockserver should be considered for important operational
suites, but it may be considered an unnecessary complication for general 
less critical usage, so it is currently disabled by default. 

To enable the lockserver:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
use lockserver = True
\end{lstlisting}

The suite will now abort at start-up if it cannot connect to the
lockserver.  To start your lockserver daemon, 
\lstset{language=transcript}
\begin{lstlisting}
% cylc lockserver start
\end{lstlisting}

To check that it is running,
\begin{lstlisting}
% cylc lockserver status
\end{lstlisting}

For detailed usage information,
\begin{lstlisting}
% cylc lockserver --help 
\end{lstlisting}

There is a command line client interface, 
\begin{lstlisting}
% cylc lockclient --help 
\end{lstlisting}
for interrogating the lockserver and managing locks manually (e.g.\
releasing locks if a suite was killed before it could clean up after
itself).

To watch suite locks being acquired and released as a suite runs, 
\begin{lstlisting}
% watch cylc lockclient --print
\end{lstlisting}

\section{The gcylc Graph View}
\label{TheGraphBasedcontrolGUI}

The graph view in the gcylc GUI has the advantage that it shows
the structure of a suite very clearly as it evolves. It works remarkably
well even for very large suites (up to several hundred tasks or more)
{\em but} because the graphviz engine does a new global layout every
time the graph changes the layout is often not very stable. This may not
be a solvable problem even in principle as it seems likely that making
continual incremental changes to an existing graph without redoing the
global layout would inevitably result in a horrible mess.

The following features of the graph view, however, help mitigate the the
jumping layout problem:

\begin{myitemize}
    \item The disconnect button can be used to temporarily prevent the
        graph from changing as the content of the suite changes (and
        in real time operation suites evolve quite slowly anyway)
    \item In most suites, toggling the greyed-out ``base graph'' nodes
        (which do not represent current live task proxies and are just
        present to fill out the graph structure) will dramatically
        reduce the size of the graph (but it will split into several
        disconnected sub-trees).
    \item Right-click on a task and choose the ``Focus'' option to restrict
        the graph display to that task's cycle time. Anything interesting
        happening in other cycles will show up as disconnected
        rectangular nodes to the right of the graph (and you can click
        on those to intantly refocus to their cycles).
    \item Task filtering is the ultimate quick route to temporarily focusing
        on just the tasks you're interested in (but this will 
        destroy the graph structure, to state the obvious).
\end{myitemize}

\section{Cylc Project README File}

\lstinputlisting{../README}

\section{Cylc Project INSTALL File}
\label{INSTALL}

\lstinputlisting{../INSTALL}

\section{Cylc Development History - Major Changes}

\begin{myitemize}

    \item {\bf pre-cylc-3} - early versions focused on the new
    scheduling algorithm. A suite was a collection of ``task definition
    files'' that encoded the prerequisites and outputs of each task, 
    exposing cylc's self-organising nature. Tasks could be transferred
    from one suite to another by simply copying their taskdef files over
    and checking prerequisite and output consistency. Global suite
    structure was not easy to discern until run time (although cylc-2
    could generate resolved run time dependency graphs).

    \item {\bf cylc-3} - a new suite design interface: dependency graph
    and task runtime properties defined in a single structured,
    validated, configuration file - the suite.rc file; graphical user
    interface; suite graphing.

    \item {\bf cylc-4} - refined and organized the suite.rc file
    structure; task runtime properties defined by an efficient
    inheritance hierarchy; support for the Jinja2 template processor in
    suite definitions.

    \item {\bf cylc-5} - multi-threading for continuous network request
    handling and job submission; more task states to distinguish job
    submission from execution; dependence between suites via new suite
    run databases; polling and killing of real task jobs; polling as
    task communications option.

\end{myitemize}

\section{Pyro} 
\label{Pyro}

Pyro (Python Remote Objects) is a widely used open source objected
oriented Remote Procedure Call technology developed by Irmen de Jong.

Earlier versions of cylc used the Pyro Nameserver to marshal
communication between client programs (tasks, commands, viewers, etc.)
and their target suites. This worked well, but in principle it
provided a route for one suite or user on the subnet to bring down 
all running suites by killing the nameserver. Consequently cylc now
uses Pyro simply as a lightweight object oriented wrapper for
direct network socket communication between client programs and their
target suites - all suites are thus entirely isolated from one another. 


\section{GNU GENERAL PUBLIC LICENSE v3.0}
\input{gpl-3.0}



%\subsection{Understanding Suite Evolution}
%\label{UnderstandingSuiteEvolution}
%
%On a cold-start all tasks (including one-off tasks) in the system will be
%instantiated at the initial cycle time, or at the next subsequent valid
%cycle time for the task. Any tasks that have no prerequisites (and, if
%they are contact tasks, have reached their trigger time) will submit to
%run immediately. Any cycling (i.e.\ non one-off) tasks that have no
%prerequisites (and, if they are contact tasks, have reached their
%trigger time) will rapidly spawn ahead until stopped by the suite's
%runahead limit (observe task X in the User Guide example suite).
%Thereafter, each task will, of its own accord, submit to run as soon as
%its prerequisites have been satisfied by other tasks already running or
%succeeded in the suite (and trigger time etc.).  Each task spawns a
%successor at a point in its life-cycle that depends on its type: tied
%tasks spawn has soon as their restart prerequisites have been completed,
%and free tasks spawn at the instant they start running.  Once a task
%exists it is free to run as soon as its prerequisites are satisfied,
%thus successive instances of a free task can run entirely in parallel,
%and successive instances of a tied task can overlap if the opportunity
%arises (other prerequisites allowing).

%\subsection{Automatic State Dumps}
%\label{AutomaticStateDumps}
%
%Cylc updates its configured state dump file (e.g.\
%\lstinline=$HOME/cylc-state/state=) every time the state of a task
%changes. Previous states are maintained in a rolling archive 
%(length specified in the {\em suite.rc} file):
%
%\begin{lstlisting}
%nwp_oper> ls .cylc/state/SUITE/
%state       # current state
%state-1     # most recent previous state
%state-2     # next most recent previous state
%...
%state-N     # oldest state dump; will be deleted at next update
%\end{lstlisting}
%
%In addition, immediately prior to any system intervention a special
%uniquely named state dump file is created and logged, e.g.:
%
%\begin{lstlisting}
%2010/03/30 14:54:29 WARNING main - pre-purge state dump: state.2010:3:30:14:54:29
%\end{lstlisting}
%
%If you accidentally intervene wrongly in a suite, just shut it down
%and restart from the pre-intervention state dump:
%
%\begin{lstlisting}
%cylc restart SUITE state.2010:3:30:14:54:29
%\end{lstlisting}

%\subsection{Suite Log Files}
%\label{SuiteLogFiles}
%
%Earlier versions of cylc created a main suite log file and a
%task-specific log for every task. However, because when all logged
%events were made to percolate up to the main log the task-specific logs
%became superfluous. Instead, cylc provides facilities for filtering the
%main log for task-specific messages (or you can just use
%\lstinline=grep= for this purpose).
%
%\begin{lstlisting}
%$ tail $HOME/cylc-logs/intro/log
%2010/03/28 00:33:50 INFO main.F - [2010010312] disconnected (spent; general)
%2010/03/28 00:33:52 INFO main.C - [2010010400] storm surge fields ready for 2010010400
%2010/03/28 00:33:52 INFO main.A - [2010010412] surface wind fields ready for 2010010412
%2010/03/28 00:33:52 INFO main.C - [2010010400] C.2010010400 completed
%2010/03/28 00:33:52 INFO main.C - [2010010400] C.2010010400 succeeded
%2010/03/28 00:33:52 INFO main.A - [2010010412] surface pressure field ready for 2010010412
%2010/03/28 00:33:52 INFO main.A - [2010010412] level forecast fields ready for 2010010412
%2010/03/28 00:33:53 INFO main.A - [2010010412] A.2010010412 completed
%2010/03/28 00:33:53 INFO main.A - [2010010412] A.2010010412 succeeded
%2010/03/28 00:33:53 CRITICAL main - ALL RUNNING TASKS SUCCEEDED
%\end{lstlisting}
%
%Each entry shows the time of logging, the name and cycle time of the
%reporting task (in square brackets), and the logged message.
%
%In simulation mode, the logged time is the simulation mode accelerated clock time, not 
%real time.
%
%Existing log files are automatically rotated at start-up and,
%individually, when they reach a size of 10 MB.  This maximum file 
%size should be configurable, but it is currently hardwired in
%\lstinline=$CYLC_DIR/src/pimp_my_logger.py=.

%\subsection{Diagnosing A Stalled Suite}
%\label{DiagnosingAStalledSuite}
%
%In certain situations a suite may appear to be ``stuck'', i.e.\ no
%tasks are running and nothing appears to be happening. There are several 
%possible reasons for this (it does not necessarily indicate a problem!):
%
%\begin{myitemize}
%    \item In {\em normal real time operation}, when all running tasks
%        have finished for the most recent cycle, nothing will happen
%        until the one or more contact tasks in the suite trigger at the
%        start of the next cycle. \lstinline=cylc show= tells if a
%        contact task has yet to reach its trigger time.
%
%    \item if every task in the suite has one or more unsatisfied
%        prerequisites, the suite will be stalled. This could happen,
%        for example, if you start a suite that contains tied (forecast
%        model) tasks without the corresponding one-off cold-start tasks to
%        satisfy their initial restart prerequisites.
%
%\end{myitemize}
%
%Operational suites should have automated means of alerting the
%operators to any failure that occurs
%
%\begin{myitemize}
%    \item If the system operator, perhaps in a post-task-failure
%    intervention, kills some tasks that are required to satisfy the
%    prerequisites of other tasks that still exist in the system, then 
%    the suite will eventually stall as a result of these tasks being
%    unable to run. Solution: insert tasks (possibly one-off cold-start
%    tasks) to get the suite running again.
%
%    \ldots This could also happen if you purge enough
%    cycles that the difference between the pre- and post-purge tasks
%    is greater than the runahead limit. Solution: ensure your runahead
%    limit is large enough to span these gaps.
%
%    \item If a failed task has not yet been removed or reset by the
%    system operator it will eventually stall the suite. Solution:
%    Fix, or otherwise deal with, failed tasks as quickly as possible.
%
%    \item If through a suite design error error, a task exists that
%        cannot get its prerequisites satisfied by any other task in the
%        suite, that task will never run and will eventually cause the
%        suite to stall.  Solution: test the suite in simulation mode to 
%        check that all prerequisites and outputs, suite-wide, are 
%        compatible.
%
%    \item If a misconfigured external task does not report an output
%        that it is supposed to (i.e.\ as registered in its task proxy
%        definition file), then its task proxy will not record that 
%        output as complete and cylc will set it to the 'failed' state
%        when it finishes without completing a registered output. A
%        failed task will eventually stall the suite, as explained above, 
%        if it is not fixed and re-run, or removed from the suite.
%        Solution: ensure all external tasks report their outputs
%        correctly.
%
%\end{myitemize}
%
%
%\subsection{Failure Recovery Scenarios}
%\label{FailureRecoveryScenarios}
%
%\begin{myitemize}
%    \item {\em One forecast cycle runs into the next, after a delay in
%        operations}. This is never a problem for cylc; every task runs
%        as soon as it can run, regardless of forecast cycle, and any
%        task that can't run before its predecessor has finished will
%        wait.
%
%    \item {\em A delayed parallel trial or case study catches up to real
%        time operation}. This is no problem for cylc; any cylc suite
%        will seamlessly transition in and out of ``normal real time
%        operation'' (distinct cycles triggered by the wall clock) as needed.
%
%    \item {\em An external task fails, but can be fixed}. For example, a
%        forecast model aborts trying to read a corrupted data file that
%        can be regenerated correctly. The failed task will be noted by
%        cylc, and its downstream dependants will not be able to run,
%        but other tasks will carry on as normal while you address the
%        problem. When fixed, use `cylc reset' to get the failed task to
%        run again, after which it and its downstream dependants will
%        catch up to the rest of the suite as quickly as possible.
%
%    \item {\em An important external task fails, but cannot be fixed.}
%        In this case, if the task has a lot of downstream dependants,
%        you will presumably need omit one or more cycles of the affected
%        tasks, and cold-start their part of the suite at the earliest
%        possible subsequent cycle.  To do this, insert the relevant cold
%        start task, or task group, at the later cycle, then purge the
%        failed task and everything that depends on it (and on them, and
%        so on) down to the cold-start time.  Other downstream forecast
%        models will be able to pick up immediately so long their most
%        recent previous instance (i.e.\ just before the gap) wrote out
%        sufficient restart outputs to bridge the gap (otherwise they,
%        or perhaps the entire suite, will need to be cold-started). 
%
%    \item {\em HELP, I attempted a drastic intervention in a complex
%        suite, using the horrifying purge command, and this time I
%        really screwed the pooch!} Before any operation that alters the
%        system state, cylc automatically writes out a special state dump
%        file and reports the filename in the main log. Shut the suite
%        down and restart it from its pre-intervention state (just
%        cut-and-paste the state dump filename from the main log file -
%        the file path is not required because the file will be in the
%        configured suite state dump directory).  Then {\em retry your
%        intervention in practice mode} before doing it for real!
%
%\end{myitemize}
%
%\subsection{Dead Suite Cleanup}
%%\label{Deadcleanup}
%
%\subsubsection{Normal Shutdown}
%
%Cylc waits for any currently running tasks to finish before shutting
%down cleanly. There will be nothing to clean up. 
%
%\subsubsection{Shutdown NOW or Controlled Abort}
%
%If a critical error of some kind, or use of \lstinline=cylc stop --now=,
%results in an immediate suite shutdown while there are still external
%tasks running, any subsequent cylc messaging calls made by the
%still-running tasks will fail because the parent suite no longer
%exists. Depending on the exact circumstances this may result in some 
%orphaned processes that need to be killed manually.
%
%\subsubsection{Uncontrolled Suite Abort}
%
%(To Do, check: currently no ill effects - maybe sockets (ports) remain
%tied up until they time out?).
%
%
%
%\pagebreak


